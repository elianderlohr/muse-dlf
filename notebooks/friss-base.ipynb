{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Waumhn_ldoGu"
   },
   "source": [
    "## FRISS with MFC\n",
    "\n",
    "Implementation of the FRISS using the Media Frames Corpus (MFC) from Card et al. (2015). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.64.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FRISS_srl.pkl',\n",
       " 'training_metrics.csv',\n",
       " 'README.md',\n",
       " 'notebooks',\n",
       " 'grid_search_metrics.csv',\n",
       " '.git',\n",
       " 'assets',\n",
       " 'test.csv',\n",
       " 'friss',\n",
       " 'models',\n",
       " '.ipynb_checkpoints',\n",
       " 'data',\n",
       " '.gitignore',\n",
       " 'frameaxis']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = \"data/mfc/immigration_labeled.json\"\n",
    "codes_path = \"data/mfc/codes.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from path \n",
    "import json\n",
    "\n",
    "with open(labels_path) as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "with open(codes_path) as f:\n",
    "    codes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# articles list\n",
    "articles_list = []\n",
    "\n",
    "# Iterate through the data to fill the DataFrame\n",
    "for article_id, article_data in labels.items():\n",
    "    annotations_data = article_data['annotations']\n",
    "\n",
    "    irrelevant_dict = annotations_data['irrelevant']\n",
    "\n",
    "    text = article_data['text']\n",
    "    irrelevant = article_data['irrelevant']\n",
    "\n",
    "    # if primary_frame is none set to 15.0\n",
    "    if article_data['primary_frame'] is not None:\n",
    "        primary_frame = str(article_data['primary_frame']).split(\".\")[0] + \".0\"\n",
    "    else:\n",
    "        primary_frame = \"15.0\"\n",
    "\n",
    "    # get primary frame from code\n",
    "    primary_frame = str(codes[primary_frame])\n",
    "\n",
    "    # split text into sentences using nltk library\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # iterate through sentences\n",
    "    for sentence in sentences:\n",
    "        article = {\n",
    "            'article_id': article_id,\n",
    "            'irrelevant': irrelevant,\n",
    "            'text': sentence,\n",
    "            'document_frame': primary_frame\n",
    "        }\n",
    "\n",
    "        articles_list.append(article)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "df = pd.DataFrame(articles_list, columns=['article_id', 'irrelevant', 'text', 'document_frame'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>irrelevant</th>\n",
       "      <th>text</th>\n",
       "      <th>document_frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IMM-10005\\n\\nPRIMARY\\n\\nImmigrants without HOP...</td>\n",
       "      <td>Quality of Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It mounted as students went around the room te...</td>\n",
       "      <td>Quality of Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Georgia Tech.</td>\n",
       "      <td>Quality of Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>University of Georgia.</td>\n",
       "      <td>Quality of Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"All I could say was, 'I'm planning to see if ...</td>\n",
       "      <td>Quality of Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74463</th>\n",
       "      <td>Immigration1.0-9998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sue Brown, spokeswoman for the INS, said it's ...</td>\n",
       "      <td>Crime and Punishment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74464</th>\n",
       "      <td>Immigration1.0-9998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"They love it,\" she said.</td>\n",
       "      <td>Crime and Punishment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74465</th>\n",
       "      <td>Immigration1.0-9998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"They use these units to interview the people,...</td>\n",
       "      <td>Crime and Punishment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74466</th>\n",
       "      <td>Immigration1.0-9998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"We do about 15 interviews a day,\" Brown said.</td>\n",
       "      <td>Crime and Punishment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74467</th>\n",
       "      <td>Immigration1.0-9998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"We put a hold on about a third of them.\"</td>\n",
       "      <td>Crime and Punishment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74468 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 article_id  irrelevant  \\\n",
       "0      Immigration1.0-10005         0.0   \n",
       "1      Immigration1.0-10005         0.0   \n",
       "2      Immigration1.0-10005         0.0   \n",
       "3      Immigration1.0-10005         0.0   \n",
       "4      Immigration1.0-10005         0.0   \n",
       "...                     ...         ...   \n",
       "74463   Immigration1.0-9998         0.0   \n",
       "74464   Immigration1.0-9998         0.0   \n",
       "74465   Immigration1.0-9998         0.0   \n",
       "74466   Immigration1.0-9998         0.0   \n",
       "74467   Immigration1.0-9998         0.0   \n",
       "\n",
       "                                                    text        document_frame  \n",
       "0      IMM-10005\\n\\nPRIMARY\\n\\nImmigrants without HOP...       Quality of Life  \n",
       "1      It mounted as students went around the room te...       Quality of Life  \n",
       "2                                          Georgia Tech.       Quality of Life  \n",
       "3                                 University of Georgia.       Quality of Life  \n",
       "4      \"All I could say was, 'I'm planning to see if ...       Quality of Life  \n",
       "...                                                  ...                   ...  \n",
       "74463  Sue Brown, spokeswoman for the INS, said it's ...  Crime and Punishment  \n",
       "74464                          \"They love it,\" she said.  Crime and Punishment  \n",
       "74465  \"They use these units to interview the people,...  Crime and Punishment  \n",
       "74466     \"We do about 15 interviews a day,\" Brown said.  Crime and Punishment  \n",
       "74467          \"We put a hold on about a third of them.\"  Crime and Punishment  \n",
       "\n",
       "[74468 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1296
    },
    "executionInfo": {
     "elapsed": 3772,
     "status": "ok",
     "timestamp": 1696624002536,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "DG_Xix7gdoGy",
    "outputId": "d6fad26e-e6f7-4c20-f4bb-c7b01d51eb33",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[df[\"irrelevant\"] == False][[\"article_id\", \"text\", \"document_frame\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>text</th>\n",
       "      <th>document_frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>IMM-10005\\n\\nPRIMARY\\n\\nImmigrants without HOP...</td>\n",
       "      <td>Quality of Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>It mounted as students went around the room te...</td>\n",
       "      <td>Quality of Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>Georgia Tech.</td>\n",
       "      <td>Quality of Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>University of Georgia.</td>\n",
       "      <td>Quality of Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>\"All I could say was, 'I'm planning to see if ...</td>\n",
       "      <td>Quality of Life</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             article_id                                               text  \\\n",
       "0  Immigration1.0-10005  IMM-10005\\n\\nPRIMARY\\n\\nImmigrants without HOP...   \n",
       "1  Immigration1.0-10005  It mounted as students went around the room te...   \n",
       "2  Immigration1.0-10005                                      Georgia Tech.   \n",
       "3  Immigration1.0-10005                             University of Georgia.   \n",
       "4  Immigration1.0-10005  \"All I could say was, 'I'm planning to see if ...   \n",
       "\n",
       "    document_frame  \n",
       "0  Quality of Life  \n",
       "1  Quality of Life  \n",
       "2  Quality of Life  \n",
       "3  Quality of Life  \n",
       "4  Quality of Life  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create for each code a col and fill with 1 if code is in code col\n",
    "df = pd.concat([df, pd.get_dummies(df['document_frame'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>text</th>\n",
       "      <th>document_frame</th>\n",
       "      <th>Capacity and Resources</th>\n",
       "      <th>Crime and Punishment</th>\n",
       "      <th>Cultural Identity</th>\n",
       "      <th>Economic</th>\n",
       "      <th>External Regulation and Reputation</th>\n",
       "      <th>Fairness and Equality</th>\n",
       "      <th>Health and Safety</th>\n",
       "      <th>Legality, Constitutionality, Jurisdiction</th>\n",
       "      <th>Morality</th>\n",
       "      <th>Other</th>\n",
       "      <th>Policy Prescription and Evaluation</th>\n",
       "      <th>Political</th>\n",
       "      <th>Public Sentiment</th>\n",
       "      <th>Quality of Life</th>\n",
       "      <th>Security and Defense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>IMM-10005\\n\\nPRIMARY\\n\\nImmigrants without HOP...</td>\n",
       "      <td>Quality of Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>It mounted as students went around the room te...</td>\n",
       "      <td>Quality of Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>Georgia Tech.</td>\n",
       "      <td>Quality of Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>University of Georgia.</td>\n",
       "      <td>Quality of Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>\"All I could say was, 'I'm planning to see if ...</td>\n",
       "      <td>Quality of Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             article_id                                               text  \\\n",
       "0  Immigration1.0-10005  IMM-10005\\n\\nPRIMARY\\n\\nImmigrants without HOP...   \n",
       "1  Immigration1.0-10005  It mounted as students went around the room te...   \n",
       "2  Immigration1.0-10005                                      Georgia Tech.   \n",
       "3  Immigration1.0-10005                             University of Georgia.   \n",
       "4  Immigration1.0-10005  \"All I could say was, 'I'm planning to see if ...   \n",
       "\n",
       "    document_frame  Capacity and Resources  Crime and Punishment  \\\n",
       "0  Quality of Life                       0                     0   \n",
       "1  Quality of Life                       0                     0   \n",
       "2  Quality of Life                       0                     0   \n",
       "3  Quality of Life                       0                     0   \n",
       "4  Quality of Life                       0                     0   \n",
       "\n",
       "   Cultural Identity  Economic  External Regulation and Reputation  \\\n",
       "0                  0         0                                   0   \n",
       "1                  0         0                                   0   \n",
       "2                  0         0                                   0   \n",
       "3                  0         0                                   0   \n",
       "4                  0         0                                   0   \n",
       "\n",
       "   Fairness and Equality  Health and Safety  \\\n",
       "0                      0                  0   \n",
       "1                      0                  0   \n",
       "2                      0                  0   \n",
       "3                      0                  0   \n",
       "4                      0                  0   \n",
       "\n",
       "   Legality, Constitutionality, Jurisdiction  Morality  Other  \\\n",
       "0                                          0         0      0   \n",
       "1                                          0         0      0   \n",
       "2                                          0         0      0   \n",
       "3                                          0         0      0   \n",
       "4                                          0         0      0   \n",
       "\n",
       "   Policy Prescription and Evaluation  Political  Public Sentiment  \\\n",
       "0                                   0          0                 0   \n",
       "1                                   0          0                 0   \n",
       "2                                   0          0                 0   \n",
       "3                                   0          0                 0   \n",
       "4                                   0          0                 0   \n",
       "\n",
       "   Quality of Life  Security and Defense  \n",
       "0                1                     0  \n",
       "1                1                     0  \n",
       "2                1                     0  \n",
       "3                1                     0  \n",
       "4                1                     0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67480, 18)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='transformers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract SRL Embeddings from articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycuda\n",
      "  Downloading pycuda-2023.1.tar.gz (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytools>=2011.2\n",
      "  Downloading pytools-2023.1.1-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mako\n",
      "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting appdirs>=1.4.0\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.9/dist-packages (from pytools>=2011.2->pycuda) (4.4.0)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from pytools>=2011.2->pycuda) (2.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from mako->pycuda) (2.1.2)\n",
      "Building wheels for collected packages: pycuda\n",
      "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycuda: filename=pycuda-2023.1-cp39-cp39-linux_x86_64.whl size=661891 sha256=69a923028332aad6d3854e0166b5d026014c5b4f0f6f7b1b4f7c7a11b168ea13\n",
      "  Stored in directory: /root/.cache/pip/wheels/58/65/0d/8abe9d4a5fad9c5ba54c7e6b57bbc927a99517f3383a428f9d\n",
      "Successfully built pycuda\n",
      "Installing collected packages: appdirs, pytools, mako, pycuda\n",
      "Successfully installed appdirs-1.4.4 mako-1.3.0 pycuda-2023.1 pytools-2023.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting allennlp\n",
      "  Downloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.2/730.2 kB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting allennlp-models\n",
      "  Downloading allennlp_models-2.10.1-py3-none-any.whl (464 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.5/464.5 kB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wandb<0.13.0,>=0.10.0\n",
      "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.1.2)\n",
      "Collecting base58>=2.1.1\n",
      "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting lmdb>=1.2.1\n",
      "  Downloading lmdb-1.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (298 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.5/298.5 kB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.23.4)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.9.2)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (3.19.6)\n",
      "Requirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (3.8.0)\n",
      "Collecting termcolor==1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.9/dist-packages (from allennlp) (2.28.2)\n",
      "Collecting spacy<3.4,>=2.1.0\n",
      "  Downloading spacy-3.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (5.8.1)\n",
      "Collecting tensorboardX>=1.2\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.12.0)\n",
      "Collecting more-itertools>=8.12.0\n",
      "  Downloading more_itertools-10.1.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.9/dist-packages (from allennlp) (3.7)\n",
      "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.4.2)\n",
      "Requirement already satisfied: torchvision<0.14.0,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.13.1+cu116)\n",
      "Requirement already satisfied: torch<1.13.0,>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.12.1+cu116)\n",
      "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.9/dist-packages (from allennlp) (7.2.1)\n",
      "Requirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.9/dist-packages (from allennlp) (4.64.1)\n",
      "Collecting fairscale==0.4.6\n",
      "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cached-path<1.2.0,>=1.1.3\n",
      "  Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: sentencepiece>=0.1.96 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.1.97)\n",
      "Collecting jsonnet>=0.10.0\n",
      "  Downloading jsonnet-0.20.0.tar.gz (594 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.3.5.1)\n",
      "Collecting transformers<4.21,>=4.1\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock<3.8,>=3.3\n",
      "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ftfy\n",
      "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting word2number>=1.1\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py-rouge==1.1\n",
      "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (from allennlp-models) (2.4.0)\n",
      "Collecting conllu==4.4.2\n",
      "  Downloading conllu-4.4.2-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: boto3<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (1.24.90)\n",
      "Collecting rich<13.0,>=12.1\n",
      "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.0.16\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-storage<3.0,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.13.0-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.0.16->allennlp) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.0.16->allennlp) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.0.16->allennlp) (23.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk>=3.6.5->allennlp) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk>=3.6.5->allennlp) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk>=3.6.5->allennlp) (8.1.3)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (2.0.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (2.0.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (1.1.0)\n",
      "Collecting attrs>=19.2.0\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.28->allennlp) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.28->allennlp) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.28->allennlp) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.28->allennlp) (2.8)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.1->allennlp) (3.1.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (66.1.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.5)\n",
      "Collecting thinc<8.1.0,>=8.0.14\n",
      "  Downloading thinc-8.0.17-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (668 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m668.8/668.8 kB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp39-cp39-manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (6.3.0)\n",
      "Collecting protobuf<4.0.0,>=3.12.0\n",
      "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (9.2.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<4.21,>=4.1->allennlp) (0.12.1)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.0.11)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.14.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.14.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (3.1.30)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.9.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.3.2)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (0.70.13)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (2023.1.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (10.0.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (1.5.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (3.8.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (3.2.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (0.18.0)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12\n",
      "  Downloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.90 in /usr/local/lib/python3.9/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.27.90)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (6.0.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (4.0.10)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.14.0-py3-none-any.whl (122 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.2/122.2 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Collecting google-auth<3.0dev,>=2.23.3\n",
      "  Downloading google_auth-2.23.4-py2.py3-none-any.whl (183 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.3/183.3 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-resumable-media>=2.6.0\n",
      "  Downloading google_resumable_media-2.6.0-py2.py3-none-any.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->allennlp-models) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->allennlp-models) (2.8.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (5.0.0)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.9/230.9 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.2.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.4.8)\n",
      "Building wheels for collected packages: fairscale, termcolor, jsonnet, word2number\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307222 sha256=d66bb0af7e9437fb7d3d408d5b980d8c253876a3053c3d0598e6d65665dc3d44\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/7b/f9/6c7a350821ca240450550801e17996aee846b71caaccda7a32\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4833 sha256=9d527996c0338b0bd5f8f57992ad8a33c7d0ae1bade098bb7f782428970e7391\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/00/51/e04e70a050b271a6aac779726204a324ada2b39b99334175c3\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jsonnet: filename=jsonnet-0.20.0-cp39-cp39-linux_x86_64.whl size=6619539 sha256=f5a1329fd287c0d7d421960ef6af1744a3ba17a3135b0ccecb815151c85e8578\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/1a/fe/4d7df1823604150f77a6877f88fc6236d7c56d92a4d15e8b8c\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5566 sha256=87ee16d68e11bf674c6fd6c144564c570ab483728c7d241f7045633f366879f7\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/1d/b2/768b65901d249c6eb2a8d9c30392506555a4e94055ba4e0aa0\n",
      "Successfully built fairscale termcolor jsonnet word2number\n",
      "Installing collected packages: word2number, wcwidth, termcolor, py-rouge, lmdb, jsonnet, commonmark, sacremoses, rich, pydantic, protobuf, more-itertools, google-crc32c, ftfy, filelock, conllu, base58, attrs, thinc, tensorboardX, huggingface-hub, googleapis-common-protos, google-resumable-media, google-auth, fairscale, wandb, transformers, spacy, google-api-core, google-cloud-core, google-cloud-storage, cached-path, allennlp, allennlp-models\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.6\n",
      "    Uninstalling wcwidth-0.2.6:\n",
      "      Successfully uninstalled wcwidth-0.2.6\n",
      "  Attempting uninstall: termcolor\n",
      "    Found existing installation: termcolor 2.2.0\n",
      "    Uninstalling termcolor-2.2.0:\n",
      "      Successfully uninstalled termcolor-2.2.0\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 13.2.0\n",
      "    Uninstalling rich-13.2.0:\n",
      "      Successfully uninstalled rich-13.2.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.9.2\n",
      "    Uninstalling pydantic-1.9.2:\n",
      "      Successfully uninstalled pydantic-1.9.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.9.0\n",
      "    Uninstalling filelock-3.9.0:\n",
      "      Successfully uninstalled filelock-3.9.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 18.2.0\n",
      "    Uninstalling attrs-18.2.0:\n",
      "      Successfully uninstalled attrs-18.2.0\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.1.7\n",
      "    Uninstalling thinc-8.1.7:\n",
      "      Successfully uninstalled thinc-8.1.7\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.12.0\n",
      "    Uninstalling huggingface-hub-0.12.0:\n",
      "      Successfully uninstalled huggingface-hub-0.12.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.16.0\n",
      "    Uninstalling google-auth-2.16.0:\n",
      "      Successfully uninstalled google-auth-2.16.0\n",
      "  Attempting uninstall: wandb\n",
      "    Found existing installation: wandb 0.13.4\n",
      "    Uninstalling wandb-0.13.4:\n",
      "      Successfully uninstalled wandb-0.13.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.21.3\n",
      "    Uninstalling transformers-4.21.3:\n",
      "      Successfully uninstalled transformers-4.21.3\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.4.1\n",
      "    Uninstalling spacy-3.4.1:\n",
      "      Successfully uninstalled spacy-3.4.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed allennlp-2.10.1 allennlp-models-2.10.1 attrs-23.1.0 base58-2.1.1 cached-path-1.1.6 commonmark-0.9.1 conllu-4.4.2 fairscale-0.4.6 filelock-3.7.1 ftfy-6.1.3 google-api-core-2.14.0 google-auth-2.23.4 google-cloud-core-2.3.3 google-cloud-storage-2.13.0 google-crc32c-1.5.0 google-resumable-media-2.6.0 googleapis-common-protos-1.61.0 huggingface-hub-0.10.1 jsonnet-0.20.0 lmdb-1.4.1 more-itertools-10.1.0 protobuf-3.20.3 py-rouge-1.1 pydantic-1.8.2 rich-12.6.0 sacremoses-0.1.1 spacy-3.3.3 tensorboardX-2.6.2.2 termcolor-1.1.0 thinc-8.0.17 transformers-4.20.1 wandb-0.12.21 wcwidth-0.2.12 word2number-1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pycuda\n",
    "!pip install allennlp allennlp-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "# get name / id of cuda device\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "cuda.init()\n",
    "device = cuda.Device(0)\n",
    "print(device.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def batched_extract_srl_components(batched_sentences, predictor):\n",
    "    # Convert each sentence into the required format for the predictor\n",
    "    batched_sentences = [{'sentence': sentence} for sentence in batched_sentences]\n",
    "\n",
    "    # Prepare the batched input for the predictor\n",
    "    batched_srl = predictor.predict_batch_json(batched_sentences)\n",
    "\n",
    "    # Extract SRL components from the batched predictions\n",
    "    results = []\n",
    "    for index, srl in enumerate(batched_srl):\n",
    "        sentence_results = []\n",
    "        for verb_entry in srl['verbs']:\n",
    "            arg_components = {'ARG0': [], 'ARG1': []}\n",
    "            for i, tag in enumerate(verb_entry['tags']):\n",
    "                if 'ARG0' in tag:\n",
    "                    arg_components['ARG0'].append(srl['words'][i])\n",
    "                elif 'ARG1' in tag:\n",
    "                    arg_components['ARG1'].append(srl['words'][i])\n",
    "\n",
    "            if arg_components['ARG0'] or arg_components['ARG1']:\n",
    "                sentence_results.append({\n",
    "                    'predicate': verb_entry['verb'],\n",
    "                    'ARG0': ' '.join(arg_components['ARG0']),\n",
    "                    'ARG1': ' '.join(arg_components['ARG1'])\n",
    "                })\n",
    "\n",
    "        if sentence_results:\n",
    "            # add empty dict if predicate, arg0 or arg1 is empty\n",
    "            if not sentence_results[0]['predicate']:\n",
    "                results.append({'predicate': '', 'ARG0': '', 'ARG1': ''})\n",
    "            elif not sentence_results[0]['ARG0']:\n",
    "                results.append({'predicate': '', 'ARG0': '', 'ARG1': ''})\n",
    "            elif not sentence_results[0]['ARG1']:\n",
    "                results.append({'predicate': '', 'ARG0': '', 'ARG1': ''})\n",
    "            else:\n",
    "                results.append(sentence_results)    \n",
    "        else:\n",
    "            results.append([{'predicate': '', 'ARG0': '', 'ARG1': ''}])\n",
    "\n",
    "    return results\n",
    "\n",
    "def optimized_extract_srl(X, predictor, batch_size=32):\n",
    "    all_results = []\n",
    "\n",
    "    # Process sentences in batches\n",
    "    for i in tqdm(range(0, len(X), batch_size), desc=\"Processing Batches\"):\n",
    "        batched_sentences = X[i:i+batch_size]\n",
    "\n",
    "        batch_results = batched_extract_srl_components(batched_sentences, predictor)\n",
    "\n",
    "        all_results.extend(batch_results)\n",
    "\n",
    "    return pd.Series(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_X_srl(X, recalculate=False, pickle_path=\"../notebooks/classifier/X_srl_filtered.pkl\"):\n",
    "    \"\"\"\n",
    "    Returns the X_srl either by loading from a pickled file or recalculating.\n",
    "    \"\"\"\n",
    "    if recalculate or not os.path.exists(pickle_path):\n",
    "        print(\"Recalculate SRL\")\n",
    "        # Load predictor\n",
    "        predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\", cuda_device=0)\n",
    "\n",
    "        # make sentences max 480 chars long\n",
    "        X = X.apply(lambda x: x[:480])\n",
    "\n",
    "        X_srl = optimized_extract_srl(X, predictor, batch_size=32)\n",
    "        with open(pickle_path, 'wb') as f:\n",
    "            pickle.dump(X_srl, f)\n",
    "    else:\n",
    "        print(\"Load SRL from Pickle\")\n",
    "        with tqdm(total=os.path.getsize(pickle_path)) as pbar:\n",
    "            with open(pickle_path, 'rb') as f:\n",
    "                X_srl = pickle.load(f)\n",
    "                pbar.update(os.path.getsize(pickle_path))\n",
    "                \n",
    "    return X_srl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_X_srl(df[\"text\"], recalculate=False, pickle_path=\"../notebooks/FRISS_srl.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def free_gpu():\n",
    "    print(torch.cuda.mem_get_info())\n",
    "    print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/distributed/distributed_c10d.py:181: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def list_gpu_tensors():\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                if obj.is_cuda:\n",
    "                    obj = obj.cpu()\n",
    "                    obj = obj.to(\"cpu\")\n",
    "                    print(type(obj), obj.size())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "list_gpu_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "class ArticleDataset(Dataset):\n",
    "    def __init__(self, X, X_srl, tokenizer, labels=None, max_sentences_per_article=32, max_sentence_length=32, max_args_per_sentence=10, max_arg_length=16):\n",
    "        self.X = X  # DataFrame where each row has multiple sentences\n",
    "        self.X_srl = X_srl  # DataFrame where each row has multiple dictionaries for SRL\n",
    "        self.labels = labels  # DataFrame where each row has a list of lists of integers\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sentences_per_article = max_sentences_per_article\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.max_args_per_sentence = max_args_per_sentence\n",
    "        self.max_arg_length = max_arg_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentences = self.X.iloc[idx]\n",
    "        srl_data = self.X_srl.iloc[idx]\n",
    "        labels = self.labels.iloc[idx]\n",
    "\n",
    "        # Tokenize sentences\n",
    "        sentence_ids = [self.tokenizer.encode(sentence, add_special_tokens=True, max_length=self.max_sentence_length, truncation=True, padding='max_length') for sentence in sentences]\n",
    "        \n",
    "        sentence_ids += [[0] * self.max_sentence_length] * (self.max_sentences_per_article - len(sentence_ids))\n",
    "        sentence_ids = sentence_ids[:self.max_sentences_per_article]\n",
    "        \n",
    "        # Process SRL data\n",
    "        predicates, arg0s, arg1s = [], [], []\n",
    "        for srl_items in srl_data:\n",
    "\n",
    "            sentence_predicates, sentence_arg0s, sentence_arg1s = [], [], []\n",
    "\n",
    "            # if srl_items is not list pack it into a list\n",
    "            if not isinstance(srl_items, list):\n",
    "                srl_items = [srl_items]\n",
    "\n",
    "            for item in srl_items:\n",
    "                p = self.tokenizer.encode(item[\"predicate\"], add_special_tokens=True, max_length=self.max_arg_length, truncation=True, padding='max_length')\n",
    "                a0 = self.tokenizer.encode(item[\"ARG0\"], add_special_tokens=True, max_length=self.max_arg_length, truncation=True, padding='max_length')\n",
    "                a1 = self.tokenizer.encode(item[\"ARG1\"], add_special_tokens=True, max_length=self.max_arg_length, truncation=True, padding='max_length')\n",
    "                \n",
    "                sentence_predicates.append(p)\n",
    "                sentence_arg0s.append(a0)\n",
    "                sentence_arg1s.append(a1)\n",
    "\n",
    "            # Padding\n",
    "            for _ in range(self.max_args_per_sentence - len(srl_items)):\n",
    "                sentence_predicates.append([0] * self.max_arg_length)\n",
    "                sentence_arg0s.append([0] * self.max_arg_length)\n",
    "                sentence_arg1s.append([0] * self.max_arg_length)\n",
    "\n",
    "            # pad to max_args_per_sentence\n",
    "            sentence_predicates = sentence_predicates[:self.max_args_per_sentence]\n",
    "            sentence_arg0s = sentence_arg0s[:self.max_args_per_sentence]\n",
    "            sentence_arg1s = sentence_arg1s[:self.max_args_per_sentence]\n",
    "\n",
    "            predicates.append(sentence_predicates)\n",
    "            arg0s.append(sentence_arg0s)\n",
    "            arg1s.append(sentence_arg1s)\n",
    "\n",
    "        # Truncate or pad SRL items to max_sentences_per_article\n",
    "        srl_padding = [[0] * self.max_arg_length] * self.max_args_per_sentence\n",
    "        predicates = (predicates + [srl_padding] * self.max_sentences_per_article)[:self.max_sentences_per_article]\n",
    "        arg0s = (arg0s + [srl_padding] * self.max_sentences_per_article)[:self.max_sentences_per_article]\n",
    "        arg1s = (arg1s + [srl_padding] * self.max_sentences_per_article)[:self.max_sentences_per_article]\n",
    "\n",
    "        data = {\n",
    "            'sentence_ids': torch.tensor(sentence_ids, dtype=torch.long),\n",
    "            'predicate_ids': torch.tensor(predicates, dtype=torch.long),\n",
    "            'arg0_ids': torch.tensor(arg0s, dtype=torch.long),\n",
    "            'arg1_ids': torch.tensor(arg1s, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels[0], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Extract individual lists from the batch\n",
    "    sentence_ids = [item['sentence_ids'] for item in batch]\n",
    "    predicate_ids = [item['predicate_ids'] for item in batch]\n",
    "    arg0_ids = [item['arg0_ids'] for item in batch]\n",
    "    arg1_ids = [item['arg1_ids'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Pad each list\n",
    "    sentence_ids = torch.nn.utils.rnn.pad_sequence(sentence_ids, batch_first=True, padding_value=0)\n",
    "    predicate_ids = torch.nn.utils.rnn.pad_sequence(predicate_ids, batch_first=True, padding_value=0)\n",
    "    arg0_ids = torch.nn.utils.rnn.pad_sequence(arg0_ids, batch_first=True, padding_value=0)\n",
    "    arg1_ids = torch.nn.utils.rnn.pad_sequence(arg1_ids, batch_first=True, padding_value=0)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Conditionally extract and add labels\n",
    "    output_dict = {\n",
    "        'sentence_ids': sentence_ids,\n",
    "        'predicate_ids': predicate_ids,\n",
    "        'arg0_ids': arg0_ids,\n",
    "        'arg1_ids': arg1_ids,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "def preprocess_df(df, recalculate_srl=False, pickle_path=\"../notebooks/FRISS_srl.pkl\"):\n",
    "    # reset index of df\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Get X_srl\n",
    "    X_srl = get_X_srl(df[\"text\"], recalculate=recalculate_srl, pickle_path=pickle_path)\n",
    "\n",
    "    # reset index of X_srl\n",
    "    X_srl = X_srl.reset_index(drop=True)\n",
    "\n",
    "    # Columns to be one-hot encoded in y_subset\n",
    "    y_cols = ['Capacity and Resources', 'Crime and Punishment', 'Cultural Identity', \n",
    "            'Economic', 'External Regulation and Reputation', 'Fairness and Equality', \n",
    "            'Health and Safety', 'Legality, Constitutionality, Jurisdiction', \n",
    "            'Morality', 'Other', 'Policy Prescription and Evaluation', 'Political', \n",
    "            'Public Sentiment', 'Quality of Life', 'Security and Defense']\n",
    "\n",
    "    # Creating y_subset\n",
    "    y_subset = df.groupby('article_id')[y_cols].apply(lambda x: x.values.tolist()).reset_index(name='encoded_values')\n",
    "    y_subset = y_subset['encoded_values']\n",
    "\n",
    "    # Aggregating 'text' column in df into a list of strings for each article_id\n",
    "    X_subset = df.groupby('article_id')['text'].apply(list).reset_index(name='text')\n",
    "    X_subset = X_subset['text']\n",
    "\n",
    "    # Assuming X_srl follows the same index order as df\n",
    "    X_srl_subset = X_srl.groupby(df['article_id']).apply(lambda x: x.values.tolist()).reset_index(name='srl_values')\n",
    "    X_srl_subset = X_srl_subset['srl_values']\n",
    "\n",
    "    return X_subset, X_srl_subset, y_subset\n",
    "\n",
    "def get_datasets_dataloaders(df, tokenizer, recalculate_srl=False, pickle_path=\"../notebooks/FRISS_srl.pkl\", batch_size=16, max_sentences_per_article=32, max_sentence_length=32, max_arg_length=16):\n",
    "    \n",
    "    X_subset, X_srl_subset, y_subset = preprocess_df(df, recalculate_srl=recalculate_srl, pickle_path=pickle_path)\n",
    "\n",
    "    # Len\n",
    "    print(\"X:\", len(X_subset))\n",
    "    print(\"X_srl:\", len(X_srl_subset))\n",
    "    print(\"y:\", len(y_subset))\n",
    "\n",
    "    print(\"CREATING DATASETS\")\n",
    "    test_size = 0.1\n",
    "    \n",
    "    # Assuming X, X_srl, and y are already defined and have the same number of samples\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_subset, y_subset, test_size=test_size, random_state=42)\n",
    "    \n",
    "    print(\"TRAIN TEST SPLIT DONE\")\n",
    "    \n",
    "    X_srl_train, X_srl_test, _, _ = train_test_split(X_srl_subset, y_subset, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Create the dataset\n",
    "    train_dataset = ArticleDataset(X_train, X_srl_train, tokenizer, y_train, max_sentences_per_article, max_sentence_length, max_arg_length)\n",
    "    test_dataset = ArticleDataset(X_test, X_srl_test, tokenizer, y_test, max_sentences_per_article, max_sentence_length, max_arg_length)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "    \n",
    "    print(\"CREATION DONE\")\n",
    "    return train_dataset, test_dataset , train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_dataloader(article, tokenizer, batch_size=1):\n",
    "    X = pd.Series([article])\n",
    "    y = None  # No labels for this single article\n",
    "    \n",
    "    predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\", cuda_device=0)\n",
    "    # Directly use the optimized_extract_srl function since we don't need to cache for single articles\n",
    "    X_srl = optimized_extract_srl(X, predictor)\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = ArticleDataset(X, X_srl, tokenizer, y)\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_dataloader(X, tokenizer, batch_size=4):\n",
    "    y = None\n",
    "    \n",
    "    predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\", cuda_device=0)\n",
    "    # Directly use the optimized_extract_srl function since we don't need to cache for single articles\n",
    "    X_srl = optimized_extract_srl(X, predictor)\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = ArticleDataset(X, X_srl, tokenizer, y)\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Model\n",
    "The Model consist out of various Layers.\n",
    "\n",
    "1. SRL_Embedding\n",
    "2. Autoencoder\n",
    "3. FRISSLoss\n",
    "4. Unsupervised\n",
    "5. Supervised\n",
    "6. FRISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SRL_Embeddings\n",
    "\n",
    "The layer takes tensors of token IDs with the shape [batch_size, max_num_sentences, max_num_tokens] for the sentence, predicates, arg0 and arg1 and returns for each sentence an embedding with shape [batch_size, embedding_dim] for the sentence, predicate, arg0 and arg1. \n",
    "\n",
    "The single embedding for the sentence is extracted by taking the [CLS] token embedding. For the predicate, arg0 and arg1 by taking the mean over all word embeddings in this list of tokens. \n",
    "\n",
    "> Possible improvements: Better way of extracting the single embedding for predicate, arg0 and arg1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shapes:  torch.Size([2, 12, 8]) torch.Size([2, 12, 9, 8]) torch.Size([2, 12, 9, 8]) torch.Size([2, 12, 9, 8])\n",
      "Outputs shapes:  torch.Size([2, 12, 768]) torch.Size([2, 12, 9, 768]) torch.Size([2, 12, 9, 768]) torch.Size([2, 12, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class SRL_Embeddings(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"bert-base-uncased\"):\n",
    "        super(SRL_Embeddings, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "        self.embedding_dim = 768  # for bert-base-uncased\n",
    "\n",
    "    def forward(self, sentence_ids, predicate_ids, arg0_ids, arg1_ids):\n",
    "        with torch.no_grad():\n",
    "            # Sentence embeddings\n",
    "            sentence_embeddings = self.bert_model(sentence_ids.view(-1, sentence_ids.size(-1)))[0]\n",
    "            sentence_embeddings = sentence_embeddings.view(sentence_ids.size(0), sentence_ids.size(1), -1, self.embedding_dim)\n",
    "            sentence_embeddings = sentence_embeddings.mean(dim=2)\n",
    "\n",
    "            # Predicate embeddings\n",
    "            predicate_embeddings = self.bert_model(predicate_ids.view(-1, predicate_ids.size(-1)))[0]\n",
    "            predicate_embeddings = predicate_embeddings.view(predicate_ids.size(0), predicate_ids.size(1), predicate_ids.size(2), -1, self.embedding_dim)\n",
    "            predicate_embeddings = predicate_embeddings.mean(dim=3)\n",
    "\n",
    "            # ARG0 embeddings\n",
    "            arg0_embeddings = self.bert_model(arg0_ids.view(-1, arg0_ids.size(-1)))[0]\n",
    "            arg0_embeddings = arg0_embeddings.view(arg0_ids.size(0), arg0_ids.size(1), arg0_ids.size(2), -1, self.embedding_dim)\n",
    "            arg0_embeddings = arg0_embeddings.mean(dim=3)\n",
    "\n",
    "            # ARG1 embeddings\n",
    "            arg1_embeddings = self.bert_model(arg1_ids.view(-1, arg1_ids.size(-1)))[0]\n",
    "            arg1_embeddings = arg1_embeddings.view(arg1_ids.size(0), arg1_ids.size(1), arg1_ids.size(2), -1, self.embedding_dim)\n",
    "            arg1_embeddings = arg1_embeddings.mean(dim=3)\n",
    "\n",
    "        return sentence_embeddings, predicate_embeddings, arg0_embeddings, arg1_embeddings\n",
    "    \n",
    "# Generate dummy data for the SRL_Embeddings\n",
    "batch_size = 2\n",
    "num_sentences = 12\n",
    "sentence_length = 8\n",
    "num_args = 9\n",
    "predicate_length = 8\n",
    "arg0_length = 8\n",
    "arg1_length = 8\n",
    "\n",
    "# Dummy data for sentences, predicates, arg0, and arg1\n",
    "sentence_ids = torch.randint(0, 10000, (batch_size, num_sentences, sentence_length))\n",
    "predicate_ids = torch.randint(0, 10000, (batch_size, num_sentences, num_args, predicate_length))\n",
    "arg0_ids = torch.randint(0, 10000, (batch_size, num_sentences, num_args, arg0_length))\n",
    "arg1_ids = torch.randint(0, 10000, (batch_size, num_sentences, num_args, arg1_length))\n",
    "\n",
    "srl_embeddings = SRL_Embeddings()\n",
    "\n",
    "sentence_embeddings, predicate_embeddings, arg0_embeddings, arg1_embeddings = srl_embeddings(sentence_ids, predicate_ids, arg0_ids, arg1_ids)\n",
    "\n",
    "print(\"Inputs shapes: \", sentence_ids.shape, predicate_ids.shape, arg0_ids.shape, arg1_ids.shape)\n",
    "print(\"Outputs shapes: \", sentence_embeddings.shape, predicate_embeddings.shape, arg0_embeddings.shape, arg1_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autoencoder\n",
    "\n",
    "The layer takes tensors for `v` (size: [batch_size, embedding_dim]), `v_sentence` (size: [batch_size, embedding_dim]), `tau` (type: _float_), and `identifier` (type: _str_). Where `v` is the embedding of either predicate, arg0 or arg1 identified by the `identifier` parameter. The `v_sentence` is the sentence embedding and `tau` defined the tau for annealing the gumpel softmax.\n",
    "\n",
    "The forward function returns `vhat` (size: [batch_size, embedding_dim]), `dz` (size: [batch_size, embedding_dim]), `gz` (size: [batch_size, embedding_dim]) and `F` (size: [K, embedding_dim]).\n",
    "\n",
    "- `vhat`: Reconstructed embedding of SRL\n",
    "- `dz`: Descriptor weights\n",
    "- `gz`: Gumbel softmax from logits\n",
    "- `F`: Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shapes:\n",
      "p -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n",
      "a0 -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n",
      "a1 -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n",
      "NaN values:\n",
      "p -> vhat: False, d: False, g: False, F: False\n",
      "a0 -> vhat: False, d: False, g: False, F: False\n",
      "a1 -> vhat: False, d: False, g: False, F: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CombinedAutoencoder(nn.Module):\n",
    "    def __init__(self, D_w, D_h, K, dropout_prob=0.3):\n",
    "        super(CombinedAutoencoder, self).__init__()\n",
    "        \n",
    "        self.D_h = D_h\n",
    "        self.K = K\n",
    "        \n",
    "        # Shared feed-forward layer for all views\n",
    "        self.feed_forward_shared = nn.Linear(2 * D_w, D_h)\n",
    "        \n",
    "        # Unique feed-forward layers for each view\n",
    "        self.feed_forward_unique = nn.ModuleDict({\n",
    "            'a0': nn.Linear(D_h, K),\n",
    "            'p': nn.Linear(D_h, K),\n",
    "            'a1': nn.Linear(D_h, K),\n",
    "        })\n",
    "\n",
    "        # Initializing F matrices for each view\n",
    "        self.F_matrices = nn.ParameterDict({\n",
    "            'a0': nn.Parameter(torch.randn(K, D_w)),\n",
    "            'p': nn.Parameter(torch.randn(K, D_w)),\n",
    "            'a1': nn.Parameter(torch.randn(K, D_w)),\n",
    "        })\n",
    "\n",
    "        # Additional layers and parameters\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.batch_norm = nn.BatchNorm1d(D_h)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.activation2 = nn.Sigmoid()\n",
    "    \n",
    "    # try softmax\n",
    "    def gumbel_softmax(self, logits, tau: float = 1, hard: bool = False, eps: float = 1e-10, threshold = 0.5, dim: int = 1):\n",
    "        gumbels = (\n",
    "            -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "        )  # ~Gumbel(0,1)\n",
    "\n",
    "        #gumbels = (torch.log(logits) + (gumbels/tau))  # ~Gumbel(logits,tau)\n",
    "        gumbels = (logits + gumbels) / tau\n",
    "        y_soft = gumbels.softmax(dim)\n",
    "\n",
    "        if hard:\n",
    "            indices = (y_soft > threshold).nonzero(as_tuple=True)\n",
    "            y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format)\n",
    "            y_hard[indices[0], indices[1]] = 1.0\n",
    "            ret = y_hard - y_soft.detach() + y_soft\n",
    "        else:\n",
    "            ret = y_soft\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, v_p, v_a0, v_a1, v_sentence, tau):\n",
    "        h_p, h_a0, h_a1 = self.process_through_shared(v_p, v_a0, v_a1, v_sentence)\n",
    "\n",
    "        logits_p = self.feed_forward_unique['p'](h_p)\n",
    "        logits_a0 = self.feed_forward_unique['a0'](h_a0)\n",
    "        logits_a1 = self.feed_forward_unique['a1'](h_a1)\n",
    "        \n",
    "        dz_p = torch.softmax(logits_p, dim=1)\n",
    "        dz_a0 = torch.softmax(logits_a0, dim=1)\n",
    "        dz_a1 = torch.softmax(logits_a1, dim=1)\n",
    "        \n",
    "        gz_p = self.gumbel_softmax(dz_p, tau=tau, hard=False)\n",
    "        gz_a0 = self.gumbel_softmax(dz_a0, tau=tau, hard=False)\n",
    "        gz_a1 = self.gumbel_softmax(dz_a1, tau=tau, hard=False)\n",
    "\n",
    "        vhat_p = torch.matmul(gz_p, self.F_matrices['p'])\n",
    "        vhat_a0 = torch.matmul(gz_a0, self.F_matrices['a0'])\n",
    "        vhat_a1 = torch.matmul(gz_a1, self.F_matrices['a1'])\n",
    "\n",
    "        return {\n",
    "            \"p\": {\"vhat\": vhat_p, \"d\": dz_p, \"g\": gz_p, \"F\": self.F_matrices['p']},\n",
    "            \"a0\": {\"vhat\": vhat_a0, \"d\": dz_a0, \"g\": gz_a0, \"F\": self.F_matrices['a0']},\n",
    "            \"a1\": {\"vhat\": vhat_a1, \"d\": dz_a1, \"g\": gz_a1, \"F\": self.F_matrices['a1']}\n",
    "        }\n",
    "        \n",
    "    def process_through_shared(self, v_p, v_a0, v_a1, v_sentence):\n",
    "        concatenated_p = torch.cat((v_p, v_sentence), dim=-1)\n",
    "        concatenated_a0 = torch.cat((v_a0, v_sentence), dim=-1)\n",
    "        concatenated_a1 = torch.cat((v_a1, v_sentence), dim=-1)\n",
    "        \n",
    "        # Concatenate them along the batch dimension for a single pass through the shared layer\n",
    "        stacked_embeddings = torch.cat([concatenated_p, concatenated_a0, concatenated_a1], dim=0)\n",
    "        \n",
    "        #h_shared = self.dropout(stacked_embeddings)\n",
    "        h_shared = self.feed_forward_shared(stacked_embeddings)\n",
    "        \n",
    "        # Splitting them back to individual embeddings\n",
    "        batch_size = v_p.shape[0]\n",
    "        h_shared = h_shared.view(3, batch_size, self.D_h)\n",
    "        \n",
    "        h_p, h_a0, h_a1 = h_shared[0], h_shared[1], h_shared[2]\n",
    "        return h_p, h_a0, h_a1\n",
    "\n",
    "# Mock Data Preparation\n",
    "D_h = 768\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "K = 20\n",
    "tau = 0.9\n",
    "\n",
    "# Generating mock embeddings for article, predicate, ARG0, ARG1, and their corresponding sentence embeddings\n",
    "article_embedding = torch.randn(batch_size, embedding_dim)\n",
    "v_p = torch.randn(batch_size, embedding_dim)\n",
    "v_a0 = torch.randn(batch_size, embedding_dim)\n",
    "v_a1 = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "# Testing CombinedAutoencoder\n",
    "autoencoder = CombinedAutoencoder(embedding_dim, D_h, K)\n",
    "outputs = autoencoder(v_p, v_a0, v_a1, article_embedding, tau)\n",
    "\n",
    "# Check shapes of the outputs\n",
    "print(\"Output shapes:\")\n",
    "for key, value in outputs.items():\n",
    "    print(f\"{key} -> vhat: {value['vhat'].shape}, d: {value['d'].shape}, g: {value['g'].shape}, F: {value['F'].shape}\")\n",
    "\n",
    "# check if tensor have nan values\n",
    "def check_nan(tensor):\n",
    "    # if tensor has any nan values, return True\n",
    "    if torch.isnan(tensor).any():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Check if any of the outputs have NaN values\n",
    "print(\"NaN values:\")\n",
    "for key, value in outputs.items():\n",
    "    print(f\"{key} -> vhat: {check_nan(value['vhat'])}, d: {check_nan(value['d'])}, g: {check_nan(value['g'])}, F: {check_nan(value['F'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FRISSLoss\n",
    "\n",
    "The layer calculates the unsupervised loss for predicate, arg0 and arg1. \n",
    "\n",
    "The forward function takes as input 3 dicts with the parameters `v`, `v_hat`, `g` and `F`. Where `v` is the embedding of the predicate, arg0 or arg1. The `v_hat` (size: [batch_size, embedding_dim]) is the reconstructed embedding for the predicate, arg0 and arg1. The `g` is the gumbel softmax result (size: [batch_size, embedding_dim]). The `F` (size: [K, embedding_dim]) which is the descriptor dictionary.\n",
    "\n",
    "The layer returns the loss for each batch. So the output is [batch_size]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRiSSLoss output: tensor([822901.6250, 822901.3750])\n"
     ]
    }
   ],
   "source": [
    "class FRISSLoss(nn.Module):\n",
    "    def __init__(self, lambda_orthogonality, M, t):\n",
    "        super(FRISSLoss, self).__init__()\n",
    "        \n",
    "        self.lambda_orthogonality = lambda_orthogonality\n",
    "        self.M = M\n",
    "        self.t = t\n",
    "        self.triplet_loss = nn.TripletMarginLoss(margin=M)\n",
    "\n",
    "    def contrastive_loss(self, v, vhat, negatives):\n",
    "        batch_size = vhat.size(0)\n",
    "        N = negatives.size(0)\n",
    "        loss = torch.zeros(batch_size, device=v.device)\n",
    "\n",
    "        # Calculate true distance between reconstructed and real embeddings\n",
    "        true_distance = self.l2(vhat, v)\n",
    "\n",
    "        for i in range(N):  # loop over each element in \"negatives\"\n",
    "            \n",
    "            # Tranform negative from [embedding dim] to [batch size, embedding_dim] \n",
    "            negative = negatives[i, :].expand(v.size(0), -1)\n",
    "\n",
    "            # Calculate negative distance for current negative embedding\n",
    "            negative_distance = self.l2(vhat, negative)\n",
    "\n",
    "            # Compute loss based on the provided logic: l2(vhat, v) + 1 + l2(vhat, negative) and clamp to 0 if below 0\n",
    "            current_loss = 1 + true_distance - negative_distance\n",
    "            loss += torch.clamp(current_loss, min=0.0)\n",
    "\n",
    "        # Normalize the total loss by N\n",
    "        return loss / N\n",
    "\n",
    "    \n",
    "    def l2(self, u, v):\n",
    "        return torch.sqrt(torch.sum((u - v) ** 2, dim=1))\n",
    "    \n",
    "    def focal_triplet_loss_WRONG(self, v, vhat_z, g, F):\n",
    "        losses = []\n",
    "        for i in range(F.size(0)):  # Iterate over each negative example\n",
    "            # For each negative, compute the loss against the anchor and positive\n",
    "            loss = self.triplet_loss(vhat_z, v, F[i].unsqueeze(0).expand(v.size(0), -1))\n",
    "            losses.append(loss)\n",
    "\n",
    "        loss_tensor = torch.stack(losses) \n",
    "        loss = loss_tensor.mean(dim=0).mean()\n",
    "        return loss\n",
    "    \n",
    "    def focal_triplet_loss(self, v, vhat_z, g, F):\n",
    "        _, indices = torch.topk(g, self.t, largest=False, dim=1)\n",
    "\n",
    "        F_t = torch.stack([F[indices[i]] for i in range(g.size(0))])\n",
    "        \n",
    "        g_tz = torch.stack([g[i, indices[i]] for i in range(g.size(0))])\n",
    "                    \n",
    "        g_t = g_tz / g_tz.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # if division by zero set all nan values to 0\n",
    "        g_t[torch.isnan(g_t)] = 0\n",
    "        \n",
    "        m_t = self.M * ((1 - g_t)**2)\n",
    "\n",
    "        # Initializing loss\n",
    "        loss = torch.zeros_like(v[:, 0])\n",
    "        \n",
    "        # Iteratively adding to the loss for each negative embedding\n",
    "        for i in range(self.t):\n",
    "            current_v_t = F_t[:, i]\n",
    "            current_m_t = m_t[:, i]\n",
    "            \n",
    "            current_loss = current_m_t + self.l2(vhat_z, v) - self.l2(vhat_z, current_v_t)\n",
    "            \n",
    "            loss += torch.max(torch.zeros_like(current_loss), current_loss)\n",
    "             \n",
    "        # Normalizing\n",
    "        loss = loss / self.t\n",
    "        return loss\n",
    "\n",
    "    def orthogonality_term(self, F, reg=1e-4):\n",
    "        gram_matrix = torch.mm(F, F.T)  # Compute the Gram matrix F * F^T\n",
    "        identity_matrix = torch.eye(gram_matrix.size(0), device=gram_matrix.device)  # Create an identity matrix\n",
    "        ortho_loss = (gram_matrix - identity_matrix).abs().sum()\n",
    "        return ortho_loss\n",
    "\n",
    "\n",
    "    def forward(self, p, a0, a1, p_negatives, a0_negatives, a1_negatives):\n",
    "        # Extract components from dictionary for predicate p\n",
    "        v_p, vhat_p, d_p, g_p, F_p = p[\"v\"], p[\"vhat\"], p[\"d\"], p[\"g\"], p[\"F\"]\n",
    "        \n",
    "        # Extract components from dictionary for ARG0\n",
    "        v_a0, vhat_a0, d_a0, g_a0, F_a0 = a0[\"v\"], a0[\"vhat\"], a0[\"d\"], a0[\"g\"], a0[\"F\"]\n",
    "\n",
    "        # Extract components from dictionary for ARG1\n",
    "        v_a1, vhat_a1, d_a1, g_a1, F_a1 = a1[\"v\"], a1[\"vhat\"], a1[\"d\"], a1[\"g\"], a1[\"F\"]\n",
    "        \n",
    "         # Calculate losses for predicate\n",
    "        Ju_p = self.contrastive_loss(v_p, vhat_p, p_negatives)        \n",
    "        Jt_p = self.focal_triplet_loss(v_p, vhat_p, g_p, F_p)\n",
    "        \n",
    "        Jz_p = Ju_p + Jt_p + self.lambda_orthogonality * self.orthogonality_term(F_p) ** 2\n",
    "        #print(Ju_p, Jt_p, self.orthogonality_term(F_p))\n",
    "        # Calculate losses for ARG0\n",
    "        Ju_a0 = self.contrastive_loss(v_a0, vhat_a0, a0_negatives)\n",
    "        Jt_a0 = self.focal_triplet_loss(v_a0, vhat_a0, g_a0, F_a0)\n",
    "        Jz_a0 = Ju_a0 + Jt_a0 + self.lambda_orthogonality * self.orthogonality_term(F_a0) ** 2\n",
    "        \n",
    "        # Calculate losses for ARG1\n",
    "        Ju_a1 = self.contrastive_loss(v_a1, vhat_a1, a1_negatives)\n",
    "        Jt_a1 = self.focal_triplet_loss(v_a1, vhat_a1, g_a1, F_a1)\n",
    "        Jz_a1 = Ju_a1 + Jt_a1 + self.lambda_orthogonality * self.orthogonality_term(F_a1) ** 2\n",
    "        \n",
    "        if torch.isnan(Jz_p).any():\n",
    "            print(\"Jz_p has nan\")\n",
    "            \n",
    "        if torch.isnan(Jz_a0).any():\n",
    "            print(\"Jz_a0 has nan\")\n",
    "            \n",
    "        if torch.isnan(Jz_a1).any():\n",
    "            print(\"Jz_a1 has nan\")\n",
    "        \n",
    "        # Aggregate the losses\n",
    "        loss = Jz_p + Jz_a0 + Jz_a1\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "# Mock Data Preparation\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "K = 15  # Number of frames/descriptors\n",
    "\n",
    "# Generating mock embeddings for article, predicate, ARG0, ARG1 and their reconstructions\n",
    "article_embedding = torch.randn(batch_size, embedding_dim)\n",
    "v_p = torch.randn(batch_size, embedding_dim)\n",
    "vhat_p = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "v_a0 = torch.randn(batch_size, embedding_dim)\n",
    "vhat_a0 = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "v_a1 = torch.randn(batch_size, embedding_dim)\n",
    "vhat_a1 = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "# Generating mock descriptor weights and descriptor matrices for predicate, ARG0, ARG1\n",
    "d_p = torch.randn(batch_size, K)\n",
    "d_a0 = torch.randn(batch_size, K)\n",
    "d_a1 = torch.randn(batch_size, K)\n",
    "\n",
    "F_p = torch.randn(K, embedding_dim)\n",
    "F_a0 = torch.randn(K, embedding_dim)\n",
    "F_a1 = torch.randn(K, embedding_dim)\n",
    "\n",
    "g_p = torch.randn(batch_size, K)\n",
    "g_a0 = torch.randn(batch_size, K)\n",
    "g_a1 = torch.randn(batch_size, K)\n",
    "\n",
    "# Generating some negative samples (let's assume 5 negative samples per batch entry)\n",
    "num_negatives = 8\n",
    "negatives_p = torch.randn(num_negatives, embedding_dim)\n",
    "negatives_a0 = torch.randn(num_negatives, embedding_dim)\n",
    "negatives_a1 = torch.randn(num_negatives, embedding_dim)\n",
    "\n",
    "# Initialize loss function\n",
    "lambda_orthogonality = 1e-3\n",
    "\n",
    "t = 8  # Number of descriptors with smallest weights for negative samples\n",
    "M = t\n",
    "\n",
    "loss_fn = FRISSLoss(lambda_orthogonality, M, t)\n",
    "\n",
    "# Organizing inputs into dictionaries\n",
    "p = {\"v\": v_p, \"vhat\": vhat_p, \"d\": d_p, \"g\": g_p, \"F\": F_p}\n",
    "a0 = {\"v\": v_a0, \"vhat\": vhat_a0, \"d\": d_a0, \"g\": g_a0, \"F\": F_a0}\n",
    "a1 = {\"v\": v_a1, \"vhat\": vhat_a1, \"d\": d_a1, \"g\": g_a1, \"F\": F_a1}\n",
    "\n",
    "loss_fn = FRISSLoss(lambda_orthogonality, M, t)\n",
    "loss = loss_fn(p, a0, a1, negatives_p, negatives_a0, negatives_a1)\n",
    "print(\"FRiSSLoss output:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FRISSUnsupervised\n",
    "\n",
    "The `FRISSUnsupervised` layer integrates multiple autoencoders and the previously described `FRISSLoss` layer to achieve an unsupervised learning process over the predicates and their arguments.\n",
    "\n",
    "### Forward Method:\n",
    "\n",
    "**Inputs**:\n",
    "1. **v_p**: Embedding of the predicate with size: [batch_size, D_w].\n",
    "2. **v_a0**: Embedding of the ARG0 (first argument) with size: [batch_size, D_w].\n",
    "3. **v_a1**: Embedding of the ARG1 (second argument) with size: [batch_size, D_w].\n",
    "4. **v_article**: Embedding of the article with size: [batch_size, D_w].\n",
    "5. **negatives**: Tensor containing negative samples with size: [batch_size, num_negatives, D_w].\n",
    "6. **tau**: A scalar parameter for the Gumbel softmax in the autoencoder.\n",
    "\n",
    "**Outputs**:\n",
    "- A dictionary `results` containing:\n",
    "    - **loss**: A tensor representing the combined unsupervised loss over the batch with size: [batch_size].\n",
    "    - **p**: Dictionary containing components for the predicate, including reconstructed embedding (`vhat`), descriptor weights (`d`), Gumbel softmax result (`g`), and the descriptor matrix (`F`).\n",
    "    - **a0**: Same as `p` but for ARG0.\n",
    "    - **a1**: Same as `p` but for ARG1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results' Shapes:\n",
      "loss: tensor([1.7158e+08, 1.7158e+08], grad_fn=<AddBackward0>)\n",
      "p -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n",
      "a0 -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n",
      "a1 -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming you have already defined CombinedAutoencoder and its methods as provided earlier.\n",
    "\n",
    "class FRISSUnsupervised(nn.Module):\n",
    "    def __init__(self, D_w, D_h, K, num_frames, lambda_orthogonality, M, t, dropout_prob=0.3):\n",
    "        super(FRISSUnsupervised, self).__init__()\n",
    "        \n",
    "        self.loss_fn = FRISSLoss(lambda_orthogonality, M, t)      \n",
    "        \n",
    "        # Using the CombinedAutoencoder instead of individual Autoencoders\n",
    "        self.combined_autoencoder = CombinedAutoencoder(D_w, D_h, K, dropout_prob=dropout_prob)\n",
    "\n",
    "    def forward(self, v_p, v_a0, v_a1, v_sentence, p_negatives, a0_negatives, a1_negatives, tau):\n",
    "        outputs = self.combined_autoencoder(v_p, v_a0, v_a1, v_sentence, tau)\n",
    "\n",
    "        outputs_p = outputs[\"p\"]\n",
    "        outputs_p[\"v\"] = v_p\n",
    "        \n",
    "        outputs_a0 = outputs[\"a0\"]\n",
    "        outputs_a0[\"v\"] = v_a0\n",
    "        \n",
    "        outputs_a1 = outputs[\"a1\"]\n",
    "        outputs_a1[\"v\"] = v_a1\n",
    "        \n",
    "        loss = self.loss_fn(\n",
    "            outputs_p,\n",
    "            outputs_a0, \n",
    "            outputs_a1, \n",
    "            p_negatives, a0_negatives, a1_negatives\n",
    "        )\n",
    "\n",
    "        results = {\n",
    "            \"loss\": loss,\n",
    "            \"p\": outputs[\"p\"],\n",
    "            \"a0\": outputs[\"a0\"],\n",
    "            \"a1\": outputs[\"a1\"]\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Mock Data Preparation\n",
    "D_h = 768\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "K = 20\n",
    "num_frames = 15\n",
    "tau = 0.9\n",
    "lambda_orthogonality = 0.1  # Placeholder value, please replace with your actual value\n",
    "M = 7  # Placeholder value, please replace with your actual value\n",
    "t = 7  # Placeholder value, please replace with your actual value\n",
    "\n",
    "# Generating mock embeddings for article, predicate, ARG0, ARG1, and their corresponding sentence embeddings\n",
    "article_embedding = torch.randn(batch_size, embedding_dim)\n",
    "v_p = torch.randn(batch_size, embedding_dim)\n",
    "v_a0 = torch.randn(batch_size, embedding_dim)\n",
    "v_a1 = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "# Generating some negative samples (let's assume 5 negative samples per batch entry)\n",
    "num_negatives = 10\n",
    "negatives_p = torch.randn(num_negatives, embedding_dim)\n",
    "negatives_a0 = torch.randn(num_negatives, embedding_dim)\n",
    "negatives_a1 = torch.randn(num_negatives, embedding_dim)\n",
    "\n",
    "# Testing FRISSUnsupervised\n",
    "unsupervised_module = FRISSUnsupervised(embedding_dim, D_h, K, num_frames, lambda_orthogonality, M, t)\n",
    "results = unsupervised_module(v_p, v_a0, v_a1, article_embedding, negatives_p, negatives_a0, negatives_a1, tau)\n",
    "\n",
    "# Print the results' shapes for verification\n",
    "print(\"Results' Shapes:\")\n",
    "for key, value in results.items():\n",
    "    if key == \"loss\":\n",
    "        print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(f\"{key} -> vhat: {value['vhat'].shape}, d: {value['d'].shape}, g: {value['g'].shape}, F: {value['F'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. FRISSSupervised\n",
    "\n",
    "The layer takes the embeddings from the args and the sentence and predicts frames. \n",
    "\n",
    "The embeddings for the args are averaged for each arg individually and then averaged on args level. The final embedding is feed into a linear layer and passed through a sigmoid function. \n",
    "\n",
    "The sentence embedding is feed into a linear layer and then into a relu function. After again in a linear function and then averaged. The average embeddung is again feed into a linear layer and lastly in a signoid function. \n",
    "\n",
    "It returns a span and sentence based prediction of shape [batch_size, num_frames]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 32, 15]), torch.Size([2, 15]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FRISSSupervised(nn.Module):\n",
    "    def __init__(self, D_w, K, num_frames, dropout_prob=0.3):\n",
    "        super(FRISSSupervised, self).__init__()\n",
    "\n",
    "        self.D_w = D_w\n",
    "                \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.feed_forward_sentence1 = nn.Linear(D_w, D_w)\n",
    "        self.feed_forward_sentence2 = nn.Linear(D_w, num_frames)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, d_p, d_a0, d_a1, vs):\n",
    "        # Span-based Classification   \n",
    "\n",
    "        # aggregate the SRL descriptors to have one descriptor per sentence\n",
    "        d_p = d_p.mean(dim=2)\n",
    "        d_a0 = d_a0.mean(dim=2)\n",
    "        d_a1 = d_a1.mean(dim=2)\n",
    "\n",
    "        # take the mean over descriptors\n",
    "        d_v = (d_p + d_a0 + d_a1) / 3\n",
    "\n",
    "        # feed in softmax\n",
    "        # yu_hat = self.softmax(d_v) # do not use as we use crossentropyloss\n",
    "\n",
    "        # Sentence-based Classification\n",
    "\n",
    "        ws = self.relu(self.feed_forward_sentence1(vs))\n",
    "\n",
    "        ws = self.feed_forward_sentence2(ws)\n",
    "\n",
    "        # mean over sentences\n",
    "        ws = ws.mean(dim=1)\n",
    "\n",
    "        # softmax\n",
    "        # ys_hat = self.softmax(ws) # do not use as we use crossentropyloss\n",
    "\n",
    "        return d_v, ws\n",
    "\n",
    "\n",
    "# Mock Data Preparation\n",
    "\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "num_frames = 15  # Assuming the number of frames is equal to K for simplicity\n",
    "num_sentences = 32\n",
    "K = 15\n",
    "num_args = 9\n",
    "\n",
    "# Generating mock dsz representations for predicate, ARG0, ARG1\n",
    "d_p = torch.randn(batch_size, num_sentences, num_args, K)\n",
    "d_a0 = torch.randn(batch_size, num_sentences, num_args, K)\n",
    "d_a1 = torch.randn(batch_size, num_sentences, num_args, K) \n",
    "\n",
    "# Adjusting the num_heads parameter\n",
    "srl_heads = 4\n",
    "sentence_heads = 8\n",
    "\n",
    "# Adjust the mock sentence embeddings shape\n",
    "vs = torch.randn(batch_size, num_sentences, embedding_dim)\n",
    "\n",
    "# Initialize and test the supervised module\n",
    "supervised_module = FRISSSupervised(embedding_dim, K, num_frames)\n",
    "\n",
    "# Forward pass the mock data\n",
    "yu_hat, ys_hat = supervised_module(d_p, d_a0, d_a1, vs)\n",
    "yu_hat.shape, ys_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FRISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(3.7889e+08, grad_fn=<DivBackward0>),\n",
       " torch.Size([2, 8, 14]),\n",
       " torch.Size([2, 14]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FRISS(nn.Module):\n",
    "    def __init__(self, embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob=0.3, bert_model_name=\"bert-base-uncased\"):\n",
    "        super(FRISS, self).__init__()\n",
    "        \n",
    "        # Aggregation layer replaced with SRL_Embeddings\n",
    "        self.aggregation = SRL_Embeddings(bert_model_name)\n",
    "        \n",
    "        # Unsupervised training module\n",
    "        self.unsupervised = FRISSUnsupervised(embedding_dim, D_h, K, num_frames, lambda_orthogonality, M, t, dropout_prob=dropout_prob)\n",
    "        \n",
    "        # Supervised training module\n",
    "        self.supervised = FRISSSupervised(embedding_dim, K, num_frames, dropout_prob=dropout_prob)\n",
    "        \n",
    "    def negative_sampling(self, embeddings, num_negatives=8):\n",
    "        batch_size, num_sentences, num_args, embedding_dim = embeddings.size()\n",
    "        all_negatives = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(num_sentences):\n",
    "                # Flatten the arguments dimension to sample across all arguments in the sentence\n",
    "                flattened_embeddings = embeddings[i, j].view(-1, embedding_dim)\n",
    "                \n",
    "                # Get indices of non-padded embeddings (assuming padding is represented by all-zero vectors)\n",
    "                non_padded_indices = torch.where(torch.any(flattened_embeddings != 0, dim=1))[0]\n",
    "\n",
    "                # Randomly sample negative indices from non-padded embeddings\n",
    "                if len(non_padded_indices) > 0:\n",
    "                    negative_indices = non_padded_indices[torch.randint(0, len(non_padded_indices), (num_negatives,))]\n",
    "                else:\n",
    "                    # If no non-padded embeddings, use zeros\n",
    "                    negative_indices = torch.zeros(num_negatives, dtype=torch.long)\n",
    "\n",
    "                negative_samples = flattened_embeddings[negative_indices, :]\n",
    "                all_negatives.append(negative_samples)\n",
    "\n",
    "        # Concatenate all negative samples into a single tensor\n",
    "        all_negatives = torch.cat(all_negatives, dim=0)\n",
    "\n",
    "        # If more samples than required, randomly select 'num_negatives' samples\n",
    "        if all_negatives.size(0) > num_negatives:\n",
    "            indices = torch.randperm(all_negatives.size(0))[:num_negatives]\n",
    "            all_negatives = all_negatives[indices]\n",
    "\n",
    "        return all_negatives\n",
    "    \n",
    "    def forward(self, sentence_ids, predicate_ids, arg0_ids, arg1_ids, tau):\n",
    "        # Convert input IDs to embeddings\n",
    "        sentence_embeddings, predicate_embeddings, arg0_embeddings, arg1_embeddings = self.aggregation(sentence_ids, predicate_ids, arg0_ids, arg1_ids)\n",
    "        \n",
    "        # Handle multiple spans by averaging predictions\n",
    "        unsupervised_losses = torch.zeros((sentence_embeddings.size(0),), device=sentence_embeddings.device)\n",
    "        \n",
    "        # Creating storage for aggregated d tensors\n",
    "        d_p_list, d_a0_list, d_a1_list = [], [], []\n",
    "        \n",
    "        negatives_p = self.negative_sampling(predicate_embeddings)\n",
    "        negatives_a0 = self.negative_sampling(arg0_embeddings)\n",
    "        negatives_a1 = self.negative_sampling(arg1_embeddings)\n",
    "\n",
    "        # Process each sentence \n",
    "        for sentence_idx in range(sentence_embeddings.size(1)):\n",
    "            s_sentence_span = sentence_embeddings[:, sentence_idx, :]\n",
    "\n",
    "            d_p_sentence_list = []\n",
    "            d_a0_sentence_list = []\n",
    "            d_a1_sentence_list = []\n",
    "\n",
    "            # Process each span\n",
    "            for span_idx in range(predicate_embeddings.size(2)):                \n",
    "                v_p_span = predicate_embeddings[:, sentence_idx, span_idx, :]\n",
    "                v_a0_span = arg0_embeddings[:, sentence_idx, span_idx, :]\n",
    "                v_a1_span = arg1_embeddings[:, sentence_idx, span_idx, :]\n",
    "\n",
    "                # Feed the embeddings to the unsupervised module\n",
    "                unsupervised_results = self.unsupervised(v_p_span, v_a0_span, v_a1_span, s_sentence_span, negatives_p, negatives_a0, negatives_a1, tau)                \n",
    "                unsupervised_losses += unsupervised_results[\"loss\"]\n",
    "                \n",
    "                if torch.isnan(unsupervised_results[\"loss\"]).any():\n",
    "                    print(\"loss is nan\")\n",
    "                \n",
    "                # Use the vhat (reconstructed embeddings) for supervised predictions\n",
    "                d_p_sentence_list.append(unsupervised_results['p']['d'])\n",
    "                d_a0_sentence_list.append(unsupervised_results['a0']['d'])\n",
    "                d_a1_sentence_list.append(unsupervised_results['a1']['d'])        \n",
    "\n",
    "\n",
    "            # Aggregating across all spans\n",
    "            d_p_sentence = torch.stack(d_p_sentence_list, dim=1)\n",
    "            d_a0_sentence = torch.stack(d_a0_sentence_list, dim=1)\n",
    "            d_a1_sentence = torch.stack(d_a1_sentence_list, dim=1)\n",
    "\n",
    "            d_p_list.append(d_p_sentence)\n",
    "            d_a0_list.append(d_a0_sentence)\n",
    "            d_a1_list.append(d_a1_sentence)\n",
    "\n",
    "        # Aggregating across all spans\n",
    "        d_p_aggregated = torch.stack(d_p_list, dim=1)\n",
    "        d_a0_aggregated = torch.stack(d_a0_list, dim=1)\n",
    "        d_a1_aggregated = torch.stack(d_a1_list, dim=1)\n",
    "        \n",
    "        # Supervised predictions\n",
    "        span_pred, sentence_pred = self.supervised(d_p_aggregated, d_a0_aggregated, d_a1_aggregated, sentence_embeddings)\n",
    "    \n",
    "        # Identify valid (non-nan) losses\n",
    "        valid_losses = ~torch.isnan(unsupervised_losses)\n",
    "\n",
    "        # Sum only the valid losses\n",
    "        #unsupervised_loss = unsupervised_losses[valid_losses].sum()\n",
    "        \n",
    "        # Take average by summing the valid losses and dividing by num sentences so that padded sentences are also taken in equation\n",
    "        unsupervised_loss = unsupervised_losses[valid_losses].sum() / sentence_embeddings.shape[1]\n",
    "        \n",
    "        return unsupervised_loss, span_pred, sentence_pred\n",
    "\n",
    "\n",
    "# Set the necessary parameters\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "K = 14  # Number of frames/descriptors\n",
    "num_frames = 14  # Assuming the number of frames is equal to K for simplicity\n",
    "D_h = 512  # Dimension of the hidden representation\n",
    "lambda_orthogonality = 0.1\n",
    "M = 8\n",
    "t = 8\n",
    "tau = 1.0\n",
    "\n",
    "# Define some mock token IDs data parameters\n",
    "max_sentences_per_article = 8\n",
    "max_sentence_length = 10\n",
    "num_sentences = max_sentences_per_article\n",
    "max_args_per_sentence = 3\n",
    "\n",
    "# Generating mock token IDs for predicate, ARG0, ARG1, and their corresponding sentences\n",
    "# We assume a vocab size of 30522 (standard BERT vocab size) for simplicity.\n",
    "vocab_size = 30522\n",
    "\n",
    "sentence_ids = torch.randint(0, vocab_size, (batch_size, max_sentences_per_article, max_sentence_length))\n",
    "predicate_ids = torch.randint(0, vocab_size, (batch_size, max_sentences_per_article, max_args_per_sentence, max_sentence_length))\n",
    "arg0_ids = torch.randint(0, vocab_size, (batch_size, max_sentences_per_article, max_args_per_sentence, max_sentence_length))\n",
    "arg1_ids = torch.randint(0, vocab_size, (batch_size, max_sentences_per_article, max_args_per_sentence, max_sentence_length))\n",
    "\n",
    "sentence_embeddings = torch.randn(batch_size, max_sentences_per_article, embedding_dim)\n",
    "predicate_embeddings = torch.randn(batch_size, max_sentences_per_article, max_args_per_sentence, embedding_dim)\n",
    "arg0_embeddings = torch.randn(batch_size, max_sentences_per_article, max_args_per_sentence, embedding_dim)\n",
    "arg1_embeddings = torch.randn(batch_size, max_sentences_per_article, max_args_per_sentence, embedding_dim)\n",
    "\n",
    "# Initialize the FRISS model\n",
    "friss_model = FRISS(embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K=K, num_frames=num_frames)\n",
    "\n",
    "# Forward pass the mock data\n",
    "unsupervised_loss, span_pred, sentence_pred = friss_model(sentence_ids, predicate_ids, arg0_ids, arg1_ids, 1)\n",
    "unsupervised_loss, span_pred.shape, sentence_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1-Score (micro-averaged) and Average Precision Score are chosen as primary metrics for evaluating the multi-label classification task due to the following reasons:\n",
    "\n",
    "1. **F1-Score (Micro)**:\n",
    "    - The micro-averaged F1-score computes global counts of true positives, false negatives, and false positives. \n",
    "    - It provides a balance between precision (the number of correct positive results divided by the number of all positive results) and recall (the number of correct positive results divided by the number of positive results that should have been returned).\n",
    "    - Given the imbalance in the label distribution observed in the dataset, the micro-averaged F1-score is robust against this imbalance, making it a suitable metric for optimization.\n",
    "\n",
    "2. **Average Precision Score**:\n",
    "    - This metric summarizes the precision-recall curve, giving a single value that represents the average of precision values at different recall levels.\n",
    "    - It's especially valuable when class imbalances exist, as it gives more weight to the positive class (the rarer class in an imbalanced dataset).\n",
    "\n",
    "Using these metrics will ensure that the model is optimized for a balanced performance across all labels, even if some labels are rarer than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "from math import exp\n",
    "import json \n",
    "import csv\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, optimizer, loss_function, alpha=0.5, num_epochs=10, tau_min=1, tau_decay=0.95, device='cuda', save_path='../notebooks/', save=False):\n",
    "    tau = 1\n",
    "    \n",
    "    metrics = {\n",
    "        'f1_span_micro': [],\n",
    "        'f1_sentence_micro': [],\n",
    "        'f1_span_macro': [],\n",
    "        'f1_sentence_macro': []\n",
    "    }\n",
    "    \n",
    "    scheduler = StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        model.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        supervised_total_loss = 0\n",
    "        unsupervised_total_loss = 0\n",
    "        \n",
    "        batch_progress = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=\"Batches\", leave=False)\n",
    "        for batch_idx, batch in batch_progress:            \n",
    "            iteration = iteration + 1\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            sentence_ids = batch['sentence_ids'].to(device)\n",
    "            predicate_ids = batch['predicate_ids'].to(device)\n",
    "            arg0_ids = batch['arg0_ids'].to(device)\n",
    "            arg1_ids = batch['arg1_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            unsupervised_loss, span_logits, sentence_logits = model(sentence_ids, predicate_ids, arg0_ids, arg1_ids, tau)\n",
    "                    \n",
    "            span_loss = 0\n",
    "            sentence_loss = 0\n",
    "\n",
    "            # loop over spans\n",
    "            for i in range(span_logits.size(1)):\n",
    "                span_loss += loss_function(span_logits[:, i, :], labels.float())\n",
    "            \n",
    "            # span_loss = loss_function(span_logits, labels.float())            \n",
    "            sentence_loss = loss_function(sentence_logits, labels.float())\n",
    "            \n",
    "            supervised_loss = span_loss + sentence_loss\n",
    "            \n",
    "            combined_loss = alpha * supervised_loss + (1-alpha) * unsupervised_loss\n",
    "            \n",
    "            if torch.isnan(combined_loss):\n",
    "                print(f\"NaN loss detected at epoch {epoch+1}, batch {batch_idx+1}. Stopping...\")\n",
    "                return\n",
    "        \n",
    "            combined_loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # After the backward pass\n",
    "            if any(p.grad is not None and torch.isnan(p.grad).any() for p in model.parameters()):\n",
    "                print(f\"NaN gradients detected at epoch {epoch+1}, batch {batch_idx+1}. Stopping...\")\n",
    "                return\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += combined_loss.item()\n",
    "            supervised_total_loss += supervised_loss.item()\n",
    "            unsupervised_total_loss += unsupervised_loss.item()\n",
    "\n",
    "            batch_progress.set_description(f\"Epoch {epoch+1} ({iteration}) Total Loss: {combined_loss.item():.3f}, SRLs: {span_loss:.3f}, Sentence: {sentence_loss:.3f}, CombinedS: {supervised_loss.item():.3f}, Unsupervised: {unsupervised_loss.item():.3f}\")\n",
    "                        \n",
    "            if save:\n",
    "                # Log metrics to CSV\n",
    "                with open(save_path + 'training_metrics.csv', 'a') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow([batch_idx, epoch+1, combined_loss.item(), supervised_loss.item(), unsupervised_loss.item()])\n",
    "\n",
    "            # Explicitly delete tensors to free up memory\n",
    "            del sentence_ids, predicate_ids, arg0_ids, arg1_ids, labels, unsupervised_loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Combined Loss: {total_loss/len(train_dataloader)}, Supervised Loss: {supervised_total_loss/len(train_dataloader)}, Unsupervised Loss: {unsupervised_total_loss/len(train_dataloader)}\")\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        span_preds = []\n",
    "        sentence_preds = []\n",
    "        combined_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                sentence_ids = batch['sentence_ids'].to(device)\n",
    "                predicate_ids = batch['predicate_ids'].to(device)\n",
    "                arg0_ids = batch['arg0_ids'].to(device)\n",
    "                arg1_ids = batch['arg1_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                _, span_logits, sentence_logits = model(sentence_ids, predicate_ids, arg0_ids, arg1_ids, tau)\n",
    "\n",
    "                span_pred = []\n",
    "\n",
    "                # loop over span and apply softmax\n",
    "                for i in range(span_logits.size(1)):\n",
    "                    span_pred.append((torch.softmax(span_logits[:, i, :], dim=1) > 0.5).float().cpu().numpy())\n",
    "\n",
    "                sentence_pred = (torch.sigmoid(sentence_logits) > 0.5).float()\n",
    "                \n",
    "                span_preds.append(span_pred)\n",
    "                sentence_preds.append(sentence_pred.cpu().numpy())\n",
    "                combined_preds.append(combined_pred.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "                # Explicitly delete tensors to free up memory\n",
    "                del sentence_ids, predicate_ids, arg0_ids, arg1_ids, labels, span_logits, sentence_logits, span_pred, sentence_pred, combined_pred\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        all_span_preds = np.vstack(span_preds)\n",
    "        all_sentence_preds = np.vstack(sentence_preds)\n",
    "        all_combined_preds = np.vstack(combined_preds)\n",
    "        all_labels = np.vstack(all_labels)\n",
    "\n",
    "        f1_span_micro = f1_score(all_labels, all_span_preds, average='micro')\n",
    "        f1_sentence_micro = f1_score(all_labels, all_sentence_preds, average='micro')\n",
    "        \n",
    "        f1_span_macro = f1_score(all_labels, all_span_preds, average='macro')\n",
    "        f1_sentence_macro = f1_score(all_labels, all_sentence_preds, average='macro')\n",
    "\n",
    "        metrics['f1_span_micro'].append(f1_span_micro)\n",
    "        metrics['f1_sentence_micro'].append(f1_sentence_micro)\n",
    "        \n",
    "        metrics['f1_span_macro'].append(f1_span_macro)\n",
    "        metrics['f1_sentence_macro'].append(f1_sentence_macro)\n",
    "\n",
    "        print(f\"Validation Metrics - micro F1 - Span/Sentence: {f1_span_micro:.4f}/{f1_sentence_micro:.4f}, macro F1 - Span/Sentence: {f1_span_macro:.4f}/{f1_sentence_macro:.4f}\")\n",
    "        \n",
    "        # Anneal tau at the end of the epoch\n",
    "        tau = max(tau_min, exp(-tau_decay * iteration))\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "    if save:\n",
    "        model_save_path = os.path.join(save_path, 'model1.pth')\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    \n",
    "        with open(os.path.join(save_path, 'metrics.json'), 'w') as f:\n",
    "            json.dump(metrics, f)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load SRL from Pickle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9850ed42065f43b18a6a36d4e20a6ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10211714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 6097\n",
      "X_srl: 6097\n",
      "y: 6097\n",
      "CREATING DATASETS\n",
      "TRAIN TEST SPLIT DONE\n",
      "CREATION DONE\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "num_sentences = 32\n",
    "batch_size = 48\n",
    "max_sentence_length = 64\n",
    "max_arg_length = 12\n",
    "\n",
    "train_dataset, test_dataset, train_dataloader, test_dataloader = get_datasets_dataloaders(df, tokenizer, recalculate_srl=False, batch_size=batch_size, max_sentences_per_article=num_sentences, max_sentence_length=max_sentence_length, max_arg_length=max_arg_length, pickle_path=\"../notebooks/FRISS_srl.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_friss_model(embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob, bert_model_name=\"bert-base-uncased\", load=True, path=\"\", device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads the weights into an instance of the model class from the given path.\n",
    "    \n",
    "    Args:\n",
    "    - model_class (torch.nn.Module): The class of the model (uninitialized).\n",
    "    - path (str): Path to the saved weights.\n",
    "    - device (str): Device to load the model on ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - model (torch.nn.Module): Model with weights loaded.\n",
    "    \"\"\"\n",
    "\n",
    "    # Model instantiation\n",
    "    model = FRISS(embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob=dropout_prob, bert_model_name=bert_model_name)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if load:\n",
    "        assert path != \"\"\n",
    "        model.load_state_dict(torch.load(path, map_location=device))\n",
    "    \n",
    "    #model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a244d728274f0ca929f0f1ff57bf53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4ad309d62d402f917c313e388e59f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.set_printoptions(profile=\"full\")\n",
    "\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 768\n",
    "num_frames = 15\n",
    "\n",
    "D_h = 768\n",
    "lambda_orthogonality = 1e-3\n",
    "\n",
    "K = num_frames\n",
    "t = 8\n",
    "M = 8\n",
    "tau_min = 0.5\n",
    "tau_decay = 5e-4\n",
    "\n",
    "dropout_prob = 0.3\n",
    "\n",
    "friss_model_path = \"bert-base-uncased\"\n",
    "bert_model_path = \"bert-base-uncased\"\n",
    "\n",
    "# Model instantiation\n",
    "model = get_friss_model(embedding_dim, \n",
    "                        D_h, \n",
    "                        lambda_orthogonality, \n",
    "                        M, \n",
    "                        t, \n",
    "                        num_sentences, \n",
    "                        K, \n",
    "                        num_frames, \n",
    "                        dropout_prob=dropout_prob,\n",
    "                        bert_model_name=bert_model_path,\n",
    "                        load=False,\n",
    "                        path=friss_model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# LOSS\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4)#, weight_decay=1e-5)\n",
    "\n",
    "# Train the model\n",
    "alpha_value = 0.5\n",
    "num_epochs_value = 10\n",
    "\n",
    "save_path = \"models/\"\n",
    "\n",
    "metrics = train(model, train_dataloader, test_dataloader, optimizer, loss_function, tau_min=tau_min, tau_decay=tau_decay, alpha=alpha_value, num_epochs=num_epochs_value, device=device, save=True, save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training combination 1/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 1e-06, 5, 0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405a308d18f24be08d45646218126baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 10742.722412109375, Supervised Loss: 8.291224559148153, Unsupervised Loss: 21477.15380859375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2967/0.3478/0.2987, macro F1 - Span/Sentence/Combined: 0.1632/0.2973/0.2817\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 10730.689615885416, Supervised Loss: 6.861002524693807, Unsupervised Loss: 21454.51806640625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3200/0.3226/0.3306, macro F1 - Span/Sentence/Combined: 0.2137/0.2673/0.3265\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 10719.414794921875, Supervised Loss: 6.123968799908956, Unsupervised Loss: 21432.705729166668\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3033/0.3279/0.2849, macro F1 - Span/Sentence/Combined: 0.2183/0.2714/0.2820\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 10707.821126302084, Supervised Loss: 5.65417758623759, Unsupervised Loss: 21409.987955729168\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3069/0.3443/0.3448, macro F1 - Span/Sentence/Combined: 0.2677/0.2820/0.3463\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 10696.410237630209, Supervised Loss: 5.399417002995809, Unsupervised Loss: 21387.4208984375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3433/0.3297/0.3673, macro F1 - Span/Sentence/Combined: 0.2962/0.2721/0.3546\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 10685.546630859375, Supervised Loss: 5.135079741477966, Unsupervised Loss: 21365.958170572918\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3351/0.3425/0.3916, macro F1 - Span/Sentence/Combined: 0.2871/0.2778/0.3657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 10674.185953776041, Supervised Loss: 4.990791877110799, Unsupervised Loss: 21343.381184895832\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3529/0.3416/0.3988, macro F1 - Span/Sentence/Combined: 0.2929/0.2797/0.3600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 10662.970296223959, Supervised Loss: 4.8219631512959795, Unsupervised Loss: 21321.118326822918\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3381/0.3370/0.4337, macro F1 - Span/Sentence/Combined: 0.2792/0.2779/0.3708\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 10652.023518880209, Supervised Loss: 4.621726155281067, Unsupervised Loss: 21299.42529296875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3615/0.3370/0.4286, macro F1 - Span/Sentence/Combined: 0.3024/0.2785/0.3797\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 10641.364908854166, Supervised Loss: 4.492606043815613, Unsupervised Loss: 21278.2373046875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3536/0.3370/0.4343, macro F1 - Span/Sentence/Combined: 0.2888/0.2786/0.3809\n",
      "Training combination 2/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 1e-06, 5, 0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c75534895942e1ba35e3dce6795eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 10190.951822916666, Supervised Loss: 8.75970216592153, Unsupervised Loss: 20373.14404296875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3902/0.2838/0.2865, macro F1 - Span/Sentence/Combined: 0.2312/0.2487/0.2812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 10179.213541666666, Supervised Loss: 6.869309186935425, Unsupervised Loss: 20351.55810546875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3940/0.2745/0.3178, macro F1 - Span/Sentence/Combined: 0.2569/0.2496/0.3118\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 10168.390869140625, Supervised Loss: 6.330201546351115, Unsupervised Loss: 20330.45166015625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3868/0.2617/0.2889, macro F1 - Span/Sentence/Combined: 0.2536/0.2347/0.2878\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 10157.849283854166, Supervised Loss: 5.987180471420288, Unsupervised Loss: 20309.71142578125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3780/0.2658/0.2927, macro F1 - Span/Sentence/Combined: 0.2472/0.2397/0.2832\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 10146.744303385416, Supervised Loss: 5.644481778144836, Unsupervised Loss: 20287.844075520832\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3760/0.2609/0.3149, macro F1 - Span/Sentence/Combined: 0.2945/0.2355/0.3021\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 10135.916585286459, Supervised Loss: 5.388167937596639, Unsupervised Loss: 20266.444661458332\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3683/0.2626/0.3519, macro F1 - Span/Sentence/Combined: 0.3095/0.2367/0.3284\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 10125.211181640625, Supervised Loss: 5.17006532351176, Unsupervised Loss: 20245.25244140625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3353/0.2585/0.3522, macro F1 - Span/Sentence/Combined: 0.2972/0.2325/0.3221\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 10114.843831380209, Supervised Loss: 5.048325300216675, Unsupervised Loss: 20224.63916015625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3205/0.2761/0.3412, macro F1 - Span/Sentence/Combined: 0.2823/0.2523/0.3168\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 10104.213785807291, Supervised Loss: 4.93995726108551, Unsupervised Loss: 20203.48779296875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3167/0.2761/0.3512, macro F1 - Span/Sentence/Combined: 0.2775/0.2538/0.3282\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 10094.11669921875, Supervised Loss: 4.81717864672343, Unsupervised Loss: 20183.416015625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3158/0.2667/0.3578, macro F1 - Span/Sentence/Combined: 0.2758/0.2406/0.3365\n",
      "Training combination 3/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 1e-06, 5, 0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7fdd17c2be453cab1cf9da36cadcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 10889.759033203125, Supervised Loss: 8.680102030436197, Unsupervised Loss: 21770.837890625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3882/0.3393/0.3228, macro F1 - Span/Sentence/Combined: 0.1941/0.2768/0.3199\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 10877.263509114584, Supervised Loss: 7.137778798739116, Unsupervised Loss: 21747.389322916668\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3776/0.3483/0.3271, macro F1 - Span/Sentence/Combined: 0.2255/0.2932/0.3226\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa52239f4d441fa82759900c3c7639c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 10865.6943359375, Supervised Loss: 6.487258434295654, Unsupervised Loss: 21724.9013671875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4022/0.3401/0.3421, macro F1 - Span/Sentence/Combined: 0.2451/0.3035/0.3481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b62587a36b949b28d105561ef817fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 10854.372802734375, Supervised Loss: 6.104423880577087, Unsupervised Loss: 21702.641276041668\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4183/0.3550/0.3280, macro F1 - Span/Sentence/Combined: 0.2585/0.3129/0.3239\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7f81653a5f4abaa9303e2923249159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 10842.699381510416, Supervised Loss: 5.8525660037994385, Unsupervised Loss: 21679.5458984375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3646/0.3491/0.3520, macro F1 - Span/Sentence/Combined: 0.2960/0.3082/0.3352\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830840db6ecc497ea799045bd9a80a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 10831.598388671875, Supervised Loss: 5.639658451080322, Unsupervised Loss: 21657.55712890625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3446/0.3529/0.3736, macro F1 - Span/Sentence/Combined: 0.2847/0.3129/0.3484\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 10819.965494791666, Supervised Loss: 5.351883252461751, Unsupervised Loss: 21634.579264322918\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3295/0.3488/0.3722, macro F1 - Span/Sentence/Combined: 0.2863/0.3093/0.3438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 10809.4560546875, Supervised Loss: 5.23209011554718, Unsupervised Loss: 21613.679850260418\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3226/0.3509/0.3859, macro F1 - Span/Sentence/Combined: 0.2821/0.3122/0.3574\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 10797.915445963541, Supervised Loss: 5.12457013130188, Unsupervised Loss: 21590.7060546875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3186/0.3526/0.4056, macro F1 - Span/Sentence/Combined: 0.2726/0.3132/0.3706\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 10786.681884765625, Supervised Loss: 5.110768675804138, Unsupervised Loss: 21568.2529296875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3198/0.3499/0.3810, macro F1 - Span/Sentence/Combined: 0.2779/0.3103/0.3472\n",
      "Training combination 4/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 1e-06, 5, 0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d49d4d1099f4c4fb33ca6994c484a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 9974.148681640625, Supervised Loss: 8.798275073369345, Unsupervised Loss: 19939.499186197918\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.1446/0.3211/0.3152, macro F1 - Span/Sentence/Combined: 0.0639/0.2597/0.2892\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 9963.288004557291, Supervised Loss: 7.577118873596191, Unsupervised Loss: 19918.998860677082\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.1455/0.3155/0.3402, macro F1 - Span/Sentence/Combined: 0.0723/0.2744/0.3515\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 9952.843098958334, Supervised Loss: 7.189167340596517, Unsupervised Loss: 19898.496907552082\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.1372/0.3333/0.3246, macro F1 - Span/Sentence/Combined: 0.0805/0.2882/0.3236\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 9942.1005859375, Supervised Loss: 6.78916863600413, Unsupervised Loss: 19877.412109375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.1352/0.3353/0.3295, macro F1 - Span/Sentence/Combined: 0.0916/0.2884/0.3221\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 9931.013997395834, Supervised Loss: 6.574899474779765, Unsupervised Loss: 19855.453125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.1924/0.3155/0.3398, macro F1 - Span/Sentence/Combined: 0.1526/0.2747/0.3204\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 9920.526285807291, Supervised Loss: 6.34416921933492, Unsupervised Loss: 19834.708333333332\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2918/0.3274/0.3473, macro F1 - Span/Sentence/Combined: 0.2717/0.2830/0.3259\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 9910.022705078125, Supervised Loss: 6.203058997790019, Unsupervised Loss: 19813.84228515625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3606/0.3186/0.3687, macro F1 - Span/Sentence/Combined: 0.3271/0.2758/0.3420\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 9899.96484375, Supervised Loss: 6.0096962451934814, Unsupervised Loss: 19793.920247395832\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3526/0.3274/0.3581, macro F1 - Span/Sentence/Combined: 0.3084/0.2839/0.3416\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 9888.709716796875, Supervised Loss: 5.851763089497884, Unsupervised Loss: 19771.56787109375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3677/0.3333/0.3621, macro F1 - Span/Sentence/Combined: 0.3246/0.2888/0.3361\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 9878.758707682291, Supervised Loss: 5.774728059768677, Unsupervised Loss: 19751.743001302082\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3410/0.3195/0.3491, macro F1 - Span/Sentence/Combined: 0.3032/0.2794/0.3231\n",
      "Training combination 5/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 1e-06, 8, 0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37713c2403c451d8705c5218e556b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 10472.898600260416, Supervised Loss: 8.675973693529764, Unsupervised Loss: 20937.120930989582\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3221/0.3909/0.3262, macro F1 - Span/Sentence/Combined: 0.2134/0.2905/0.3136\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 10461.129801432291, Supervised Loss: 6.713871479034424, Unsupervised Loss: 20915.5458984375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3444/0.3833/0.3298, macro F1 - Span/Sentence/Combined: 0.2582/0.3262/0.3041\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 10449.904947916666, Supervised Loss: 6.133942723274231, Unsupervised Loss: 20893.675944010418\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3569/0.3761/0.3438, macro F1 - Span/Sentence/Combined: 0.2177/0.3206/0.3153\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 10438.66845703125, Supervised Loss: 5.636502305666606, Unsupervised Loss: 20871.7001953125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3417/0.3782/0.3491, macro F1 - Span/Sentence/Combined: 0.2327/0.3282/0.3232\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 10427.66650390625, Supervised Loss: 5.401580254236857, Unsupervised Loss: 20849.931315104168\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2841/0.3804/0.3609, macro F1 - Span/Sentence/Combined: 0.1995/0.3294/0.3327\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 10416.75, Supervised Loss: 5.191921949386597, Unsupervised Loss: 20828.307942708332\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3342/0.3725/0.3701, macro F1 - Span/Sentence/Combined: 0.3036/0.3239/0.3460\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 10405.425618489584, Supervised Loss: 5.0025105476379395, Unsupervised Loss: 20805.848958333332\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2963/0.3793/0.3551, macro F1 - Span/Sentence/Combined: 0.2624/0.3300/0.3341\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 10395.133219401041, Supervised Loss: 4.788841724395752, Unsupervised Loss: 20785.4775390625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3118/0.3790/0.3836, macro F1 - Span/Sentence/Combined: 0.2724/0.3303/0.3442\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 10384.02587890625, Supervised Loss: 4.654747128486633, Unsupervised Loss: 20763.39697265625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3181/0.3768/0.4174, macro F1 - Span/Sentence/Combined: 0.2770/0.3290/0.3778\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 10373.810628255209, Supervised Loss: 4.545279900232951, Unsupervised Loss: 20743.076009114582\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3298/0.3779/0.4142, macro F1 - Span/Sentence/Combined: 0.2922/0.3301/0.3807\n",
      "Training combination 6/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 1e-06, 8, 0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc510459852949a7b558bae225cd15c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 10980.270670572916, Supervised Loss: 8.815333366394043, Unsupervised Loss: 21951.725748697918\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3957/0.3537/0.3190, macro F1 - Span/Sentence/Combined: 0.3047/0.2801/0.3049\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 10967.957682291666, Supervised Loss: 6.867220759391785, Unsupervised Loss: 21929.04833984375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4101/0.3176/0.3299, macro F1 - Span/Sentence/Combined: 0.3122/0.2788/0.3162\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 10956.1005859375, Supervised Loss: 6.361283143361409, Unsupervised Loss: 21905.840169270832\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4141/0.3198/0.3128, macro F1 - Span/Sentence/Combined: 0.3354/0.2866/0.3051\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 10945.05810546875, Supervised Loss: 5.910351792971293, Unsupervised Loss: 21884.205891927082\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3981/0.3104/0.3395, macro F1 - Span/Sentence/Combined: 0.3138/0.2728/0.3264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 10933.777994791666, Supervised Loss: 5.604281187057495, Unsupervised Loss: 21861.951985677082\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3844/0.3036/0.3641, macro F1 - Span/Sentence/Combined: 0.3142/0.2718/0.3388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 10922.520100911459, Supervised Loss: 5.306490778923035, Unsupervised Loss: 21839.733561197918\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2769/0.3072/0.3743, macro F1 - Span/Sentence/Combined: 0.2451/0.2732/0.3502\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 10911.166015625, Supervised Loss: 5.182777166366577, Unsupervised Loss: 21817.1494140625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2597/0.3086/0.3807, macro F1 - Span/Sentence/Combined: 0.2158/0.2759/0.3462\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 10899.875244140625, Supervised Loss: 5.044976313908895, Unsupervised Loss: 21794.70556640625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2759/0.3086/0.4046, macro F1 - Span/Sentence/Combined: 0.2505/0.2753/0.3577\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 10888.578206380209, Supervised Loss: 4.870901902516683, Unsupervised Loss: 21772.285319010418\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2875/0.3003/0.4198, macro F1 - Span/Sentence/Combined: 0.2550/0.2669/0.3803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 10877.590738932291, Supervised Loss: 4.7318166097005205, Unsupervised Loss: 21750.449544270832\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3123/0.3054/0.4343, macro F1 - Span/Sentence/Combined: 0.2718/0.2721/0.3845\n",
      "Training combination 7/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 1e-06, 8, 0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f723eadb79416598192202158955be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 10283.68115234375, Supervised Loss: 8.682020227114359, Unsupervised Loss: 20558.680013020832\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3392/0.3657/0.3239, macro F1 - Span/Sentence/Combined: 0.2222/0.3343/0.3044\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 10272.524251302084, Supervised Loss: 7.281842033068339, Unsupervised Loss: 20537.7666015625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3577/0.3776/0.3222, macro F1 - Span/Sentence/Combined: 0.2138/0.3409/0.2984\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 10261.26220703125, Supervised Loss: 6.619966904322307, Unsupervised Loss: 20515.904296875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3577/0.3631/0.3370, macro F1 - Span/Sentence/Combined: 0.2138/0.3249/0.3243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 10250.696126302084, Supervised Loss: 6.141745766003926, Unsupervised Loss: 20495.25048828125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3607/0.3460/0.3464, macro F1 - Span/Sentence/Combined: 0.2345/0.3045/0.3317\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 10239.940673828125, Supervised Loss: 5.854116280873616, Unsupervised Loss: 20474.027506510418\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3727/0.3540/0.3533, macro F1 - Span/Sentence/Combined: 0.3065/0.3096/0.3395\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 10229.525146484375, Supervised Loss: 5.690005381902059, Unsupervised Loss: 20453.360514322918\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3642/0.3588/0.3815, macro F1 - Span/Sentence/Combined: 0.3126/0.3167/0.3610\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 10217.711507161459, Supervised Loss: 5.514790058135986, Unsupervised Loss: 20429.908203125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3526/0.3473/0.3942, macro F1 - Span/Sentence/Combined: 0.2871/0.3061/0.3703\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 10207.613606770834, Supervised Loss: 5.3506219784418745, Unsupervised Loss: 20409.876790364582\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3408/0.3522/0.3965, macro F1 - Span/Sentence/Combined: 0.2717/0.3104/0.3611\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 10196.924397786459, Supervised Loss: 5.141824126243591, Unsupervised Loss: 20388.70703125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3548/0.3582/0.4142, macro F1 - Span/Sentence/Combined: 0.2857/0.3137/0.3673\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 10186.587809244791, Supervised Loss: 5.079385995864868, Unsupervised Loss: 20368.096028645832\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3571/0.3571/0.4073, macro F1 - Span/Sentence/Combined: 0.2944/0.3115/0.3590\n",
      "Training combination 8/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 1e-06, 8, 0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a0934972ce4b40a7d13877d243b748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 10661.678548177084, Supervised Loss: 8.744121392567953, Unsupervised Loss: 21314.61279296875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2689/0.3158/0.3059, macro F1 - Span/Sentence/Combined: 0.1409/0.2694/0.2905\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 10650.69970703125, Supervised Loss: 7.643310546875, Unsupervised Loss: 21293.75634765625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3272/0.3386/0.2825, macro F1 - Span/Sentence/Combined: 0.1871/0.2818/0.2847\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 10639.35595703125, Supervised Loss: 6.981243133544922, Unsupervised Loss: 21271.730794270832\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3121/0.3675/0.3137, macro F1 - Span/Sentence/Combined: 0.1964/0.3014/0.3205\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 10627.574788411459, Supervised Loss: 6.722132166226705, Unsupervised Loss: 21248.427571614582\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2974/0.3780/0.3390, macro F1 - Span/Sentence/Combined: 0.2155/0.3077/0.3350\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 10617.595703125, Supervised Loss: 6.502209305763245, Unsupervised Loss: 21228.689453125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3269/0.3731/0.3398, macro F1 - Span/Sentence/Combined: 0.2920/0.3048/0.3297\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 10605.669514973959, Supervised Loss: 6.18706750869751, Unsupervised Loss: 21205.15234375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3307/0.3853/0.3343, macro F1 - Span/Sentence/Combined: 0.3075/0.3178/0.3231\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 10594.732096354166, Supervised Loss: 6.050868272781372, Unsupervised Loss: 21183.41357421875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3536/0.3769/0.3546, macro F1 - Span/Sentence/Combined: 0.3236/0.3112/0.3402\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 10583.742106119791, Supervised Loss: 5.899555683135986, Unsupervised Loss: 21161.584635416668\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3585/0.3804/0.3642, macro F1 - Span/Sentence/Combined: 0.3234/0.3138/0.3500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 10572.910725911459, Supervised Loss: 5.73435652256012, Unsupervised Loss: 21140.08740234375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3616/0.3784/0.3615, macro F1 - Span/Sentence/Combined: 0.3292/0.3121/0.3457\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 10562.75537109375, Supervised Loss: 5.710389455159505, Unsupervised Loss: 21119.800130208332\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3533/0.3724/0.3626, macro F1 - Span/Sentence/Combined: 0.3255/0.3081/0.3522\n",
      "Training combination 9/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 1e-06, 10, 0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f1cc8ed1d14c5783c8354ea67d9167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 10482.662679036459, Supervised Loss: 8.592813889185587, Unsupervised Loss: 20956.732259114582\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3494/0.3179/0.3300, macro F1 - Span/Sentence/Combined: 0.2269/0.2579/0.3002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 10471.0810546875, Supervised Loss: 6.837433973948161, Unsupervised Loss: 20935.32470703125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2826/0.3333/0.3500, macro F1 - Span/Sentence/Combined: 0.1769/0.2904/0.3207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 10461.225423177084, Supervised Loss: 6.129064321517944, Unsupervised Loss: 20916.321614583332\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2857/0.3508/0.3868, macro F1 - Span/Sentence/Combined: 0.1777/0.3113/0.3588\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 10449.521647135416, Supervised Loss: 5.768710215886434, Unsupervised Loss: 20893.274576822918\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2857/0.3519/0.3651, macro F1 - Span/Sentence/Combined: 0.1763/0.3180/0.3302\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 10438.036946614584, Supervised Loss: 5.458353281021118, Unsupervised Loss: 20870.615397135418\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3263/0.3415/0.3802, macro F1 - Span/Sentence/Combined: 0.2594/0.3092/0.3368\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 10427.958984375, Supervised Loss: 5.199507474899292, Unsupervised Loss: 20850.718424479168\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3333/0.3333/0.3820, macro F1 - Span/Sentence/Combined: 0.2936/0.3024/0.3407\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 10416.8671875, Supervised Loss: 4.971852461496989, Unsupervised Loss: 20828.762532552082\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4080/0.3415/0.4012, macro F1 - Span/Sentence/Combined: 0.3675/0.3110/0.3547\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 10407.068033854166, Supervised Loss: 4.8070288101832075, Unsupervised Loss: 20809.3291015625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4034/0.3425/0.4189, macro F1 - Span/Sentence/Combined: 0.3465/0.3107/0.3699\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 10396.803629557291, Supervised Loss: 4.7101595004399615, Unsupervised Loss: 20788.897135416668\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4045/0.3364/0.4493, macro F1 - Span/Sentence/Combined: 0.3454/0.3057/0.3891\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 10386.396158854166, Supervised Loss: 4.5806015729904175, Unsupervised Loss: 20768.211751302082\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4104/0.3323/0.4716, macro F1 - Span/Sentence/Combined: 0.3457/0.3053/0.3981\n",
      "Training combination 10/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 1e-06, 10, 0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc51037cf39412f8a3116aa02684c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 10661.548014322916, Supervised Loss: 8.51769240697225, Unsupervised Loss: 21314.578287760418\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3848/0.3081/0.3740, macro F1 - Span/Sentence/Combined: 0.2319/0.2634/0.3511\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 10649.832845052084, Supervised Loss: 6.975442926088969, Unsupervised Loss: 21292.690266927082\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3536/0.3236/0.3929, macro F1 - Span/Sentence/Combined: 0.2072/0.2759/0.3646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 10638.334879557291, Supervised Loss: 6.255524317423503, Unsupervised Loss: 21270.414225260418\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3494/0.3298/0.4152, macro F1 - Span/Sentence/Combined: 0.2028/0.2812/0.3711\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 10628.563313802084, Supervised Loss: 5.846243659655253, Unsupervised Loss: 21251.2802734375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3420/0.3395/0.4239, macro F1 - Span/Sentence/Combined: 0.2287/0.2881/0.3794\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 10616.934814453125, Supervised Loss: 5.595973213513692, Unsupervised Loss: 21228.273600260418\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3636/0.3360/0.4074, macro F1 - Span/Sentence/Combined: 0.2958/0.2852/0.3487\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 10606.157063802084, Supervised Loss: 5.349491198857625, Unsupervised Loss: 21206.964680989582\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3354/0.3439/0.4196, macro F1 - Span/Sentence/Combined: 0.2770/0.2925/0.3611\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 10595.854085286459, Supervised Loss: 5.191064119338989, Unsupervised Loss: 21186.517415364582\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3448/0.3429/0.4266, macro F1 - Span/Sentence/Combined: 0.2728/0.2912/0.3637\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 10584.482096354166, Supervised Loss: 4.964810808499654, Unsupervised Loss: 21163.999348958332\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3450/0.3360/0.4298, macro F1 - Span/Sentence/Combined: 0.2714/0.2886/0.3731\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 10574.242106119791, Supervised Loss: 4.889966408411662, Unsupervised Loss: 21143.59423828125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3503/0.3386/0.4350, macro F1 - Span/Sentence/Combined: 0.2746/0.2894/0.3735\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 10562.73046875, Supervised Loss: 4.746785998344421, Unsupervised Loss: 21120.714029947918\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3473/0.3395/0.4505, macro F1 - Span/Sentence/Combined: 0.2678/0.2911/0.3884\n",
      "Training combination 11/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 1e-06, 10, 0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dad77f668734365a3adf183c4f658fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 10688.896484375, Supervised Loss: 8.475899457931519, Unsupervised Loss: 21369.316731770832\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3608/0.3193/0.3646, macro F1 - Span/Sentence/Combined: 0.2339/0.2521/0.3376\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 10678.33056640625, Supervised Loss: 7.021985411643982, Unsupervised Loss: 21349.63916015625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3740/0.3533/0.3632, macro F1 - Span/Sentence/Combined: 0.2268/0.3122/0.3462\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 10666.422281901041, Supervised Loss: 6.47585932413737, Unsupervised Loss: 21326.36865234375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3641/0.3440/0.3820, macro F1 - Span/Sentence/Combined: 0.2282/0.2948/0.3584\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 10655.522379557291, Supervised Loss: 6.12101682027181, Unsupervised Loss: 21304.923665364582\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3665/0.3536/0.4032, macro F1 - Span/Sentence/Combined: 0.2655/0.3022/0.3684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 10644.155192057291, Supervised Loss: 5.876736640930176, Unsupervised Loss: 21282.433756510418\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3325/0.3440/0.4164, macro F1 - Span/Sentence/Combined: 0.2571/0.2927/0.3767\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 10633.530110677084, Supervised Loss: 5.644309560457866, Unsupervised Loss: 21261.416015625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3791/0.3567/0.4254, macro F1 - Span/Sentence/Combined: 0.3380/0.3048/0.3828\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 10622.7236328125, Supervised Loss: 5.4167799949646, Unsupervised Loss: 21240.030598958332\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3438/0.3567/0.4225, macro F1 - Span/Sentence/Combined: 0.3146/0.3046/0.3712\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 10611.48876953125, Supervised Loss: 5.286987543106079, Unsupervised Loss: 21217.690592447918\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3064/0.3460/0.4401, macro F1 - Span/Sentence/Combined: 0.2633/0.2903/0.3821\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 10600.439371744791, Supervised Loss: 5.1128023862838745, Unsupervised Loss: 21195.765787760418\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3116/0.3563/0.4496, macro F1 - Span/Sentence/Combined: 0.2563/0.3057/0.3901\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 10590.037109375, Supervised Loss: 5.0074706474939985, Unsupervised Loss: 21175.06689453125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3116/0.3557/0.4457, macro F1 - Span/Sentence/Combined: 0.2595/0.3059/0.3764\n",
      "Training combination 12/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 1e-06, 10, 0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c4c84086e84e998b7861e0c38341e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 10853.129720052084, Supervised Loss: 8.587827205657959, Unsupervised Loss: 21697.671549479168\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2614/0.3343/0.3452, macro F1 - Span/Sentence/Combined: 0.1379/0.2703/0.3282\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 10842.634114583334, Supervised Loss: 7.759606242179871, Unsupervised Loss: 21677.508463541668\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2278/0.3446/0.3200, macro F1 - Span/Sentence/Combined: 0.1317/0.2981/0.2881\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 10830.299967447916, Supervised Loss: 7.236850261688232, Unsupervised Loss: 21653.362955729168\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2222/0.3228/0.3270, macro F1 - Span/Sentence/Combined: 0.1402/0.2758/0.3075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 10818.595540364584, Supervised Loss: 6.867072065671285, Unsupervised Loss: 21630.324055989582\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2326/0.3269/0.3361, macro F1 - Span/Sentence/Combined: 0.1560/0.2777/0.3155\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 10808.556396484375, Supervised Loss: 6.509990056355794, Unsupervised Loss: 21610.602864583332\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2424/0.3141/0.3660, macro F1 - Span/Sentence/Combined: 0.1809/0.2710/0.3338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 10797.155924479166, Supervised Loss: 6.36370583375295, Unsupervised Loss: 21587.948079427082\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2706/0.3218/0.3551, macro F1 - Span/Sentence/Combined: 0.2397/0.2754/0.3241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 10785.569010416666, Supervised Loss: 6.124614755312602, Unsupervised Loss: 21565.013346354168\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2690/0.3121/0.3676, macro F1 - Span/Sentence/Combined: 0.2427/0.2700/0.3352\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 10775.020670572916, Supervised Loss: 5.955784320831299, Unsupervised Loss: 21544.085611979168\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2485/0.3121/0.3657, macro F1 - Span/Sentence/Combined: 0.2216/0.2687/0.3280\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 10765.179606119791, Supervised Loss: 5.870326280593872, Unsupervised Loss: 21524.488606770832\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2622/0.3175/0.3799, macro F1 - Span/Sentence/Combined: 0.2326/0.2741/0.3348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 10753.415120442709, Supervised Loss: 5.715578158696492, Unsupervised Loss: 21501.11474609375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2831/0.3155/0.4146, macro F1 - Span/Sentence/Combined: 0.2500/0.2750/0.3776\n",
      "Training combination 13/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 1e-06, 20, 0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c805ac958684b108e6782b9192ee444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 10862.632893880209, Supervised Loss: 8.767576416333517, Unsupervised Loss: 21716.498372395832\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2656/0.3653/0.3099, macro F1 - Span/Sentence/Combined: 0.1682/0.2997/0.2974\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 10852.928548177084, Supervised Loss: 6.791433771451314, Unsupervised Loss: 21699.065755208332\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2656/0.3394/0.3255, macro F1 - Span/Sentence/Combined: 0.1682/0.2938/0.3018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 10844.151448567709, Supervised Loss: 6.2412029504776, Unsupervised Loss: 21682.061848958332\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2735/0.3701/0.3413, macro F1 - Span/Sentence/Combined: 0.1850/0.3229/0.3244\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 10833.110921223959, Supervised Loss: 5.7721532980601, Unsupervised Loss: 21660.449544270832\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3046/0.3680/0.3352, macro F1 - Span/Sentence/Combined: 0.2295/0.3223/0.3265\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 10825.307535807291, Supervised Loss: 5.457232117652893, Unsupervised Loss: 21645.15771484375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3646/0.3721/0.3988, macro F1 - Span/Sentence/Combined: 0.3163/0.3300/0.3774\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 10815.844645182291, Supervised Loss: 5.204054395357768, Unsupervised Loss: 21626.48486328125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4254/0.3728/0.4373, macro F1 - Span/Sentence/Combined: 0.3530/0.3316/0.3971\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 10803.450927734375, Supervised Loss: 5.01534100373586, Unsupervised Loss: 21601.88671875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4298/0.3561/0.4490, macro F1 - Span/Sentence/Combined: 0.3542/0.3092/0.3951\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 10793.752766927084, Supervised Loss: 4.84704593817393, Unsupervised Loss: 21582.658528645832\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4162/0.3680/0.4611, macro F1 - Span/Sentence/Combined: 0.3350/0.3237/0.4036\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 10787.490397135416, Supervised Loss: 4.665523727734883, Unsupervised Loss: 21570.315104166668\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3989/0.3631/0.4324, macro F1 - Span/Sentence/Combined: 0.3248/0.3144/0.3826\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 10776.193522135416, Supervised Loss: 4.528242707252502, Unsupervised Loss: 21547.858561197918\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3897/0.3582/0.4169, macro F1 - Span/Sentence/Combined: 0.3462/0.3103/0.3608\n",
      "Training combination 14/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 1e-06, 20, 0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c09f19e77444cf289b1f67197ef7f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 11072.079182942709, Supervised Loss: 8.750488917032877, Unsupervised Loss: 22135.407877604168\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.5088/0.3382/0.2989, macro F1 - Span/Sentence/Combined: 0.2721/0.2705/0.2736\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 11060.159016927084, Supervised Loss: 7.0830899477005005, Unsupervised Loss: 22113.23486328125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3923/0.3294/0.2744, macro F1 - Span/Sentence/Combined: 0.1278/0.2759/0.2676\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 11049.218831380209, Supervised Loss: 6.455702106157939, Unsupervised Loss: 22091.982096354168\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3905/0.3257/0.3000, macro F1 - Span/Sentence/Combined: 0.1278/0.2756/0.2884\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 11041.113850911459, Supervised Loss: 6.041161100069682, Unsupervised Loss: 22076.1865234375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3504/0.3152/0.3268, macro F1 - Span/Sentence/Combined: 0.1792/0.2655/0.3071\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 11029.133138020834, Supervised Loss: 5.634960055351257, Unsupervised Loss: 22052.631184895832\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2517/0.3152/0.3041, macro F1 - Span/Sentence/Combined: 0.2171/0.2685/0.2762\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 11021.022379557291, Supervised Loss: 5.463170011838277, Unsupervised Loss: 22036.581705729168\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2372/0.3314/0.3425, macro F1 - Span/Sentence/Combined: 0.2117/0.2794/0.3127\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 11013.899658203125, Supervised Loss: 5.227872967720032, Unsupervised Loss: 22022.5712890625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2545/0.3191/0.3571, macro F1 - Span/Sentence/Combined: 0.2227/0.2714/0.3269\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 11002.421223958334, Supervised Loss: 5.091001311937968, Unsupervised Loss: 21999.75146484375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2590/0.3121/0.3450, macro F1 - Span/Sentence/Combined: 0.2254/0.2661/0.3183\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 10993.06884765625, Supervised Loss: 4.91452431678772, Unsupervised Loss: 21981.22314453125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2950/0.3209/0.3591, macro F1 - Span/Sentence/Combined: 0.2579/0.2777/0.3318\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 10984.048258463541, Supervised Loss: 4.7844158411026, Unsupervised Loss: 21963.312174479168\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2939/0.3121/0.3556, macro F1 - Span/Sentence/Combined: 0.2549/0.2690/0.3216\n",
      "Training combination 15/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 1e-06, 20, 0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b0e491025547ba99178011258807a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 10807.234212239584, Supervised Loss: 8.557300686836243, Unsupervised Loss: 21605.911295572918\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3315/0.3616/0.3255, macro F1 - Span/Sentence/Combined: 0.2064/0.3084/0.2813\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 10794.1396484375, Supervised Loss: 7.133374849955241, Unsupervised Loss: 21581.14599609375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2927/0.3429/0.3362, macro F1 - Span/Sentence/Combined: 0.1852/0.2924/0.3106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 10786.63330078125, Supervised Loss: 6.581046064694722, Unsupervised Loss: 21566.685546875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2957/0.3641/0.3072, macro F1 - Span/Sentence/Combined: 0.1936/0.3110/0.2969\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 10778.104817708334, Supervised Loss: 6.0083597501118975, Unsupervised Loss: 21550.201171875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2949/0.3606/0.3304, macro F1 - Span/Sentence/Combined: 0.2097/0.3073/0.3272\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 10767.636881510416, Supervised Loss: 5.83240818977356, Unsupervised Loss: 21529.441080729168\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3032/0.3616/0.3642, macro F1 - Span/Sentence/Combined: 0.2333/0.3052/0.3464\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 10756.294352213541, Supervised Loss: 5.554507692654927, Unsupervised Loss: 21507.034016927082\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3175/0.3596/0.3977, macro F1 - Span/Sentence/Combined: 0.2518/0.3056/0.3757\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 10747.184000651041, Supervised Loss: 5.396040439605713, Unsupervised Loss: 21488.972330729168\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3260/0.3672/0.4146, macro F1 - Span/Sentence/Combined: 0.2432/0.3124/0.3824\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 10738.750569661459, Supervised Loss: 5.323269208272298, Unsupervised Loss: 21472.177734375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3135/0.3657/0.4251, macro F1 - Span/Sentence/Combined: 0.2325/0.3123/0.3898\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 10730.197428385416, Supervised Loss: 5.101106246312459, Unsupervised Loss: 21455.293782552082\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3117/0.3712/0.3902, macro F1 - Span/Sentence/Combined: 0.2306/0.3139/0.3563\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 10719.26904296875, Supervised Loss: 4.999478340148926, Unsupervised Loss: 21433.538411458332\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2876/0.3590/0.3902, macro F1 - Span/Sentence/Combined: 0.2073/0.3027/0.3576\n",
      "Training combination 16/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 1e-06, 20, 0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01344a5e57374ec4b0ff935d856518a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 10772.437744140625, Supervised Loss: 8.799370447794596, Unsupervised Loss: 21536.076334635418\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4221/0.3944/0.3029, macro F1 - Span/Sentence/Combined: 0.2826/0.3173/0.2734\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 10762.497884114584, Supervised Loss: 7.6548612515131635, Unsupervised Loss: 21517.340983072918\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4450/0.3875/0.2795, macro F1 - Span/Sentence/Combined: 0.2792/0.3297/0.2713\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 10750.782877604166, Supervised Loss: 7.217889865239461, Unsupervised Loss: 21494.347819010418\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4499/0.3871/0.3439, macro F1 - Span/Sentence/Combined: 0.2633/0.3242/0.3212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 10745.469319661459, Supervised Loss: 6.825592756271362, Unsupervised Loss: 21484.113444010418\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4450/0.3919/0.3245, macro F1 - Span/Sentence/Combined: 0.2633/0.3293/0.3062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 10733.807779947916, Supervised Loss: 6.469754099845886, Unsupervised Loss: 21461.145833333332\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4548/0.3873/0.3324, macro F1 - Span/Sentence/Combined: 0.2842/0.3201/0.3195\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 10725.038167317709, Supervised Loss: 6.359177549680074, Unsupervised Loss: 21443.717122395832\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4066/0.3848/0.3548, macro F1 - Span/Sentence/Combined: 0.2965/0.3154/0.3251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 10715.6357421875, Supervised Loss: 6.1142875750859575, Unsupervised Loss: 21425.1572265625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3736/0.3884/0.3454, macro F1 - Span/Sentence/Combined: 0.3067/0.3190/0.3180\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 10706.145751953125, Supervised Loss: 5.882074912389119, Unsupervised Loss: 21406.40966796875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3471/0.3862/0.3416, macro F1 - Span/Sentence/Combined: 0.2898/0.3189/0.3174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 10698.200032552084, Supervised Loss: 5.691256244977315, Unsupervised Loss: 21390.708658854168\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3324/0.3862/0.3757, macro F1 - Span/Sentence/Combined: 0.2897/0.3178/0.3430\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 10687.199625651041, Supervised Loss: 5.73581596215566, Unsupervised Loss: 21368.663411458332\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3175/0.3918/0.3844, macro F1 - Span/Sentence/Combined: 0.2742/0.3207/0.3458\n",
      "Training combination 17/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0001, 5, 0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaac0e6be8b945309df9da0b079d0268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 1022939.1979166666, Supervised Loss: 8.646355708440145, Unsupervised Loss: 2045869.7708333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3142/0.3215/0.3192, macro F1 - Span/Sentence/Combined: 0.2129/0.2651/0.2861\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 1021821.9322916666, Supervised Loss: 6.834060192108154, Unsupervised Loss: 2043637.0208333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3141/0.3526/0.3333, macro F1 - Span/Sentence/Combined: 0.2322/0.3121/0.3093\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 1020706.21875, Supervised Loss: 6.24955689907074, Unsupervised Loss: 2041406.1875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3141/0.3523/0.3631, macro F1 - Span/Sentence/Combined: 0.2322/0.3147/0.3250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 1019595.0520833334, Supervised Loss: 5.8865586916605634, Unsupervised Loss: 2039184.2291666667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3055/0.3536/0.3661, macro F1 - Span/Sentence/Combined: 0.2218/0.3168/0.3347\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 1018488.5885416666, Supervised Loss: 5.605002681414287, Unsupervised Loss: 2036971.5729166667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3293/0.3594/0.3573, macro F1 - Span/Sentence/Combined: 0.2602/0.3215/0.3120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 1017385.3125, Supervised Loss: 5.438571214675903, Unsupervised Loss: 2034765.1666666667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3919/0.3663/0.3673, macro F1 - Span/Sentence/Combined: 0.3404/0.3258/0.3213\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 1016282.984375, Supervised Loss: 5.240002512931824, Unsupervised Loss: 2032560.71875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4021/0.3605/0.3372, macro F1 - Span/Sentence/Combined: 0.3495/0.3219/0.2973\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 1015183.0572916666, Supervised Loss: 5.032978296279907, Unsupervised Loss: 2030361.0833333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3807/0.3460/0.3642, macro F1 - Span/Sentence/Combined: 0.3321/0.3096/0.3225\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 1014083.84375, Supervised Loss: 4.870824376742045, Unsupervised Loss: 2028162.8229166667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3842/0.3557/0.3927, macro F1 - Span/Sentence/Combined: 0.3436/0.3150/0.3469\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 1012985.796875, Supervised Loss: 4.795763770739238, Unsupervised Loss: 2025966.8125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3875/0.3432/0.3757, macro F1 - Span/Sentence/Combined: 0.3522/0.3077/0.3360\n",
      "Training combination 18/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0001, 5, 0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12d250274594636bbcf12716451f61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 1008809.8177083334, Supervised Loss: 8.572818438212076, Unsupervised Loss: 2017611.0520833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2908/0.3145/0.3684, macro F1 - Span/Sentence/Combined: 0.1667/0.2726/0.3388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 1007710.21875, Supervised Loss: 7.078641772270203, Unsupervised Loss: 2015413.375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2908/0.3226/0.3239, macro F1 - Span/Sentence/Combined: 0.1667/0.2920/0.3134\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 1006611.5416666666, Supervised Loss: 6.457607626914978, Unsupervised Loss: 2013216.625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2908/0.3254/0.3253, macro F1 - Span/Sentence/Combined: 0.1667/0.2971/0.3119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 1005513.8958333334, Supervised Loss: 6.044237931569417, Unsupervised Loss: 2011021.7604166667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2915/0.3353/0.3529, macro F1 - Span/Sentence/Combined: 0.1810/0.3083/0.3189\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 1004416.5885416666, Supervised Loss: 5.8351731300354, Unsupervised Loss: 2008827.34375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2874/0.3294/0.3673, macro F1 - Span/Sentence/Combined: 0.2100/0.3015/0.3330\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 1003319.9895833334, Supervised Loss: 5.631080508232117, Unsupervised Loss: 2006634.34375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3051/0.3372/0.3578, macro F1 - Span/Sentence/Combined: 0.2553/0.3096/0.3245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 1002226.9791666666, Supervised Loss: 5.448183139165242, Unsupervised Loss: 2004448.5104166667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3500/0.3265/0.3689, macro F1 - Span/Sentence/Combined: 0.3122/0.2969/0.3274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 1001139.09375, Supervised Loss: 5.2573643525441485, Unsupervised Loss: 2002272.9270833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3677/0.3294/0.3642, macro F1 - Span/Sentence/Combined: 0.3288/0.3027/0.3206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 1000052.5, Supervised Loss: 5.168694893519084, Unsupervised Loss: 2000099.8333333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3686/0.3284/0.3776, macro F1 - Span/Sentence/Combined: 0.3210/0.2966/0.3317\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 998967.71875, Supervised Loss: 5.031277656555176, Unsupervised Loss: 1997930.4166666667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3602/0.3323/0.3988, macro F1 - Span/Sentence/Combined: 0.3093/0.3026/0.3532\n",
      "Training combination 19/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0001, 5, 0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a77c552f38846948dd2779b51010776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 1035247.6666666666, Supervised Loss: 8.7121022939682, Unsupervised Loss: 2070486.6145833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3648/0.2786/0.3612, macro F1 - Span/Sentence/Combined: 0.1786/0.2194/0.3502\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 1034124.75, Supervised Loss: 7.218478957811992, Unsupervised Loss: 2068242.28125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2828/0.2722/0.3448, macro F1 - Span/Sentence/Combined: 0.1389/0.2364/0.3212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 1033003.9322916666, Supervised Loss: 6.622124075889587, Unsupervised Loss: 2066001.25\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2928/0.2628/0.3896, macro F1 - Span/Sentence/Combined: 0.1743/0.2258/0.3694\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 1031883.171875, Supervised Loss: 6.206777930259705, Unsupervised Loss: 2063760.1354166667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2938/0.2795/0.3948, macro F1 - Span/Sentence/Combined: 0.1714/0.2458/0.3657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 1030764.9583333334, Supervised Loss: 5.897368709246318, Unsupervised Loss: 2061524.0208333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3254/0.2761/0.3763, macro F1 - Span/Sentence/Combined: 0.2217/0.2421/0.3403\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 1029649.4270833334, Supervised Loss: 5.769960721333821, Unsupervised Loss: 2059293.0729166667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3425/0.2716/0.3892, macro F1 - Span/Sentence/Combined: 0.2894/0.2386/0.3535\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 1028535.2708333334, Supervised Loss: 5.618027528127034, Unsupervised Loss: 2057064.9166666667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3155/0.2595/0.3871, macro F1 - Span/Sentence/Combined: 0.2642/0.2277/0.3465\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 1027421.453125, Supervised Loss: 5.42593518892924, Unsupervised Loss: 2054837.4791666667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3218/0.2795/0.3812, macro F1 - Span/Sentence/Combined: 0.2684/0.2456/0.3274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 1026310.7604166666, Supervised Loss: 5.300283034642537, Unsupervised Loss: 2052616.21875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3121/0.2679/0.3866, macro F1 - Span/Sentence/Combined: 0.2699/0.2357/0.3364\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 1025200.15625, Supervised Loss: 5.198058366775513, Unsupervised Loss: 2050395.1145833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3503/0.2741/0.4088, macro F1 - Span/Sentence/Combined: 0.2984/0.2413/0.3650\n",
      "Training combination 20/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0001, 5, 0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c00a5b11fce435dbfa0183ed8e7faab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 1027629.6302083334, Supervised Loss: 8.692203481992086, Unsupervised Loss: 2055250.5625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2105/0.3284/0.2899, macro F1 - Span/Sentence/Combined: 0.0721/0.2606/0.2786\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 1026515.6875, Supervised Loss: 7.3586892286936445, Unsupervised Loss: 2053024.0104166667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2241/0.3000/0.3187, macro F1 - Span/Sentence/Combined: 0.0914/0.2395/0.3158\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 1025403.65625, Supervised Loss: 7.135681827863057, Unsupervised Loss: 2050800.1666666667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.1429/0.2830/0.3324, macro F1 - Span/Sentence/Combined: 0.0755/0.2292/0.3253\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 1024291.734375, Supervised Loss: 6.703427155812581, Unsupervised Loss: 2048576.75\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.1780/0.2975/0.3652, macro F1 - Span/Sentence/Combined: 0.0919/0.2387/0.3521\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 1023180.171875, Supervised Loss: 6.554903030395508, Unsupervised Loss: 2046353.7916666667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2000/0.3165/0.3621, macro F1 - Span/Sentence/Combined: 0.1163/0.2548/0.3526\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 1022069.7395833334, Supervised Loss: 6.40088677406311, Unsupervised Loss: 2044133.0833333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2583/0.3135/0.3590, macro F1 - Span/Sentence/Combined: 0.1832/0.2492/0.3461\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 1020960.6197916666, Supervised Loss: 6.196600000063579, Unsupervised Loss: 2041915.0416666667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3127/0.3028/0.3684, macro F1 - Span/Sentence/Combined: 0.2616/0.2492/0.3521\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 1019852.0625, Supervised Loss: 6.096534649531047, Unsupervised Loss: 2039698.03125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2896/0.3178/0.3557, macro F1 - Span/Sentence/Combined: 0.2817/0.2503/0.3392\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 1018744.0104166666, Supervised Loss: 5.934183120727539, Unsupervised Loss: 2037482.09375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2706/0.3171/0.3686, macro F1 - Span/Sentence/Combined: 0.2706/0.2517/0.3512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 1017636.90625, Supervised Loss: 5.8237336079279585, Unsupervised Loss: 2035267.9895833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2640/0.3178/0.3540, macro F1 - Span/Sentence/Combined: 0.2727/0.2514/0.3365\n",
      "Training combination 21/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0001, 8, 0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a3d137365f437ea0d42b792f363998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 1075454.7083333333, Supervised Loss: 8.655739585558573, Unsupervised Loss: 2150900.75\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3984/0.3818/0.3874, macro F1 - Span/Sentence/Combined: 0.3256/0.3260/0.3714\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 1074303.84375, Supervised Loss: 6.870443383852641, Unsupervised Loss: 2148600.8333333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4215/0.3647/0.4041, macro F1 - Span/Sentence/Combined: 0.3213/0.3119/0.3870\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 1073156.1145833333, Supervised Loss: 6.214135368665059, Unsupervised Loss: 2146306.0208333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3801/0.3491/0.3805, macro F1 - Span/Sentence/Combined: 0.2867/0.2965/0.3548\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 1072010.0416666667, Supervised Loss: 5.95727260907491, Unsupervised Loss: 2144014.1666666665\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3865/0.3363/0.3861, macro F1 - Span/Sentence/Combined: 0.2964/0.2877/0.3591\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 1070868.8541666667, Supervised Loss: 5.599584420522054, Unsupervised Loss: 2141732.0833333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3823/0.3323/0.4069, macro F1 - Span/Sentence/Combined: 0.2895/0.2809/0.3694\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 1069728.6770833333, Supervised Loss: 5.36693263053894, Unsupervised Loss: 2139452.0\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3595/0.3293/0.4162, macro F1 - Span/Sentence/Combined: 0.2903/0.2803/0.3754\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 1068590.53125, Supervised Loss: 5.187165816624959, Unsupervised Loss: 2137175.875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3513/0.3304/0.4235, macro F1 - Span/Sentence/Combined: 0.2871/0.2795/0.3752\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 1067453.09375, Supervised Loss: 5.059086124102275, Unsupervised Loss: 2134901.1041666665\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3253/0.3303/0.4204, macro F1 - Span/Sentence/Combined: 0.2724/0.2795/0.3747\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 1066317.8541666667, Supervised Loss: 4.882219115893046, Unsupervised Loss: 2132630.8541666665\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2943/0.3294/0.4464, macro F1 - Span/Sentence/Combined: 0.2487/0.2783/0.3924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 1065184.5104166667, Supervised Loss: 4.810760299364726, Unsupervised Loss: 2130364.2291666665\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3235/0.3333/0.4548, macro F1 - Span/Sentence/Combined: 0.2696/0.2835/0.3991\n",
      "Training combination 22/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0001, 8, 0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3aaea72bf745f19b3f70e5a3262694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 1052611.5729166667, Supervised Loss: 8.657408237457275, Unsupervised Loss: 2105214.4791666665\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3310/0.3807/0.3125, macro F1 - Span/Sentence/Combined: 0.2538/0.3160/0.2742\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 1051468.9895833333, Supervised Loss: 7.008389155069987, Unsupervised Loss: 2102930.9583333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3311/0.3673/0.3022, macro F1 - Span/Sentence/Combined: 0.2462/0.3278/0.2927\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 1050329.4479166667, Supervised Loss: 6.389734546343486, Unsupervised Loss: 2100652.5208333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3349/0.3519/0.3497, macro F1 - Span/Sentence/Combined: 0.2478/0.3152/0.3294\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 1049192.8541666667, Supervised Loss: 6.061456481615703, Unsupervised Loss: 2098379.625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3261/0.3519/0.3371, macro F1 - Span/Sentence/Combined: 0.2325/0.3151/0.3124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 1048056.8229166666, Supervised Loss: 5.844711820284526, Unsupervised Loss: 2096107.78125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3342/0.3626/0.3506, macro F1 - Span/Sentence/Combined: 0.2473/0.3192/0.3281\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 1046922.6510416666, Supervised Loss: 5.652663230895996, Unsupervised Loss: 2093839.6458333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3395/0.3573/0.3476, macro F1 - Span/Sentence/Combined: 0.2687/0.3169/0.3059\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 1045788.2395833334, Supervised Loss: 5.441960692405701, Unsupervised Loss: 2091571.03125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3610/0.3547/0.3663, macro F1 - Span/Sentence/Combined: 0.3069/0.3195/0.3345\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 1044654.8541666666, Supervised Loss: 5.298833290735881, Unsupervised Loss: 2089304.40625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2988/0.3353/0.3804, macro F1 - Span/Sentence/Combined: 0.2738/0.3029/0.3529\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 1043524.6510416666, Supervised Loss: 5.126968622207642, Unsupervised Loss: 2087044.1770833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3063/0.3468/0.3944, macro F1 - Span/Sentence/Combined: 0.2808/0.3106/0.3562\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 1042396.203125, Supervised Loss: 5.077034989992778, Unsupervised Loss: 2084787.3333333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2994/0.3440/0.3826, macro F1 - Span/Sentence/Combined: 0.2748/0.3084/0.3516\n",
      "Training combination 23/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0001, 8, 0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3177e07b47b348d3b6bcc8860cad2ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 1017622.4010416666, Supervised Loss: 9.054109414418539, Unsupervised Loss: 2035235.75\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3974/0.3272/0.3594, macro F1 - Span/Sentence/Combined: 0.2070/0.2893/0.3545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 1016510.890625, Supervised Loss: 7.268301645914714, Unsupervised Loss: 2033014.5208333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4290/0.3190/0.3679, macro F1 - Span/Sentence/Combined: 0.2505/0.2834/0.3449\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 1015399.8020833334, Supervised Loss: 6.704127152760823, Unsupervised Loss: 2030792.9166666667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4278/0.3333/0.3568, macro F1 - Span/Sentence/Combined: 0.2503/0.2980/0.3403\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 1014290.1875, Supervised Loss: 6.344834526379903, Unsupervised Loss: 2028574.0208333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4270/0.3234/0.3747, macro F1 - Span/Sentence/Combined: 0.2525/0.2890/0.3571\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 1013184.3697916666, Supervised Loss: 6.101673404375712, Unsupervised Loss: 2026362.625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4360/0.3373/0.3740, macro F1 - Span/Sentence/Combined: 0.3163/0.2985/0.3485\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 1012081.1197916666, Supervised Loss: 5.864035884539287, Unsupervised Loss: 2024156.375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3920/0.3393/0.3733, macro F1 - Span/Sentence/Combined: 0.3246/0.2959/0.3459\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 1010979.9375, Supervised Loss: 5.653890053431193, Unsupervised Loss: 2021954.2291666667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3363/0.3273/0.3458, macro F1 - Span/Sentence/Combined: 0.2865/0.2853/0.3142\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 1009879.6770833334, Supervised Loss: 5.569001793861389, Unsupervised Loss: 2019753.78125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2972/0.3373/0.3652, macro F1 - Span/Sentence/Combined: 0.2699/0.2995/0.3241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 1008779.734375, Supervised Loss: 5.434290766716003, Unsupervised Loss: 2017554.03125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2961/0.3343/0.3588, macro F1 - Span/Sentence/Combined: 0.2618/0.2971/0.3260\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 1007682.4791666666, Supervised Loss: 5.332916935284932, Unsupervised Loss: 2015359.6354166667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3032/0.3263/0.3594, macro F1 - Span/Sentence/Combined: 0.2667/0.2923/0.3236\n",
      "Training combination 24/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0001, 8, 0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52e4b11ae4044daad6e78ac8f99567a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 1052317.0833333333, Supervised Loss: 8.93797500928243, Unsupervised Loss: 2104625.2083333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.1723/0.3450/0.3362, macro F1 - Span/Sentence/Combined: 0.0882/0.2544/0.2955\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 1051180.625, Supervised Loss: 7.688993612925212, Unsupervised Loss: 2102353.5416666665\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2492/0.3382/0.2747, macro F1 - Span/Sentence/Combined: 0.1308/0.2772/0.2492\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 1050049.6979166667, Supervised Loss: 7.095403750737508, Unsupervised Loss: 2100092.2916666665\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2500/0.3343/0.3112, macro F1 - Span/Sentence/Combined: 0.1312/0.2824/0.2838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 1048921.5520833333, Supervised Loss: 6.872310121854146, Unsupervised Loss: 2097836.2083333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2260/0.3343/0.2982, macro F1 - Span/Sentence/Combined: 0.1225/0.2834/0.2803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 1047794.0833333334, Supervised Loss: 6.546778281529744, Unsupervised Loss: 2095581.625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2143/0.3450/0.3401, macro F1 - Span/Sentence/Combined: 0.1254/0.2958/0.3114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 1046667.171875, Supervised Loss: 6.387494802474976, Unsupervised Loss: 2093327.9583333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.1846/0.3314/0.3743, macro F1 - Span/Sentence/Combined: 0.1246/0.2813/0.3407\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 1045541.640625, Supervised Loss: 6.215101877848308, Unsupervised Loss: 2091077.0625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2279/0.3362/0.3704, macro F1 - Span/Sentence/Combined: 0.2044/0.2820/0.3313\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 1044416.4583333334, Supervised Loss: 5.9350398778915405, Unsupervised Loss: 2088826.9583333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2517/0.3372/0.3837, macro F1 - Span/Sentence/Combined: 0.2266/0.2888/0.3440\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 1043291.5989583334, Supervised Loss: 5.990215500195821, Unsupervised Loss: 2086577.1979166667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2812/0.3392/0.3732, macro F1 - Span/Sentence/Combined: 0.2508/0.2865/0.3358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 1042168.5677083334, Supervised Loss: 5.821558554967244, Unsupervised Loss: 2084331.3229166667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2938/0.3372/0.3882, macro F1 - Span/Sentence/Combined: 0.2604/0.2846/0.3513\n",
      "Training combination 25/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0001, 10, 0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9ef24e62ec47e0a44aa6eb4ec380b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 1059161.71875, Supervised Loss: 8.810429533322653, Unsupervised Loss: 2118314.625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2198/0.3562/0.3509, macro F1 - Span/Sentence/Combined: 0.1060/0.3158/0.3276\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 1058008.1770833333, Supervised Loss: 7.031955242156982, Unsupervised Loss: 2116009.2916666665\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2374/0.3916/0.3782, macro F1 - Span/Sentence/Combined: 0.1439/0.3385/0.3482\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 1056855.5208333333, Supervised Loss: 6.332887848218282, Unsupervised Loss: 2113704.7083333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2374/0.3743/0.3753, macro F1 - Span/Sentence/Combined: 0.1439/0.3092/0.3432\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 1055704.7395833333, Supervised Loss: 5.820229887962341, Unsupervised Loss: 2111403.6458333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2335/0.3818/0.3514, macro F1 - Span/Sentence/Combined: 0.1427/0.3135/0.3232\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 1054554.8645833333, Supervised Loss: 5.653546094894409, Unsupervised Loss: 2109104.1041666665\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.1966/0.3793/0.3516, macro F1 - Span/Sentence/Combined: 0.1235/0.3087/0.3176\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 1053405.3541666667, Supervised Loss: 5.402155876159668, Unsupervised Loss: 2106805.3125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2910/0.3647/0.3567, macro F1 - Span/Sentence/Combined: 0.2452/0.2972/0.3199\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 1052256.8645833333, Supervised Loss: 5.20716651280721, Unsupervised Loss: 2104508.5208333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3478/0.3714/0.3977, macro F1 - Span/Sentence/Combined: 0.2851/0.3016/0.3545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 1051109.65625, Supervised Loss: 5.058372616767883, Unsupervised Loss: 2102214.2708333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3715/0.3746/0.4071, macro F1 - Span/Sentence/Combined: 0.2923/0.2989/0.3606\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 1049962.0520833333, Supervised Loss: 4.939073920249939, Unsupervised Loss: 2099919.1458333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3827/0.3710/0.4023, macro F1 - Span/Sentence/Combined: 0.2897/0.3051/0.3613\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 1048816.3697916667, Supervised Loss: 4.784777760505676, Unsupervised Loss: 2097627.9375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3812/0.3668/0.4195, macro F1 - Span/Sentence/Combined: 0.3024/0.2969/0.3669\n",
      "Training combination 26/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0001, 10, 0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34245dfbe865484f900b57a36fcefdd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 1040862.3802083334, Supervised Loss: 8.827716708183289, Unsupervised Loss: 2081715.9270833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2849/0.3679/0.3149, macro F1 - Span/Sentence/Combined: 0.1618/0.2830/0.3179\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 1039737.4479166666, Supervised Loss: 7.102806965510051, Unsupervised Loss: 2079467.78125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3082/0.3366/0.3351, macro F1 - Span/Sentence/Combined: 0.1575/0.2576/0.3227\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 1038616.4895833334, Supervised Loss: 6.519251545270284, Unsupervised Loss: 2077226.4479166667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3072/0.3438/0.3307, macro F1 - Span/Sentence/Combined: 0.1575/0.2797/0.3261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 1037498.0052083334, Supervised Loss: 6.109185655911763, Unsupervised Loss: 2074989.90625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2994/0.3457/0.3802, macro F1 - Span/Sentence/Combined: 0.1575/0.2817/0.3536\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 1036380.9895833334, Supervised Loss: 5.883611003557841, Unsupervised Loss: 2072756.1041666667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2857/0.3467/0.3695, macro F1 - Span/Sentence/Combined: 0.1648/0.2873/0.3394\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 1035265.6614583334, Supervised Loss: 5.549590508143107, Unsupervised Loss: 2070525.7708333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3323/0.3375/0.3672, macro F1 - Span/Sentence/Combined: 0.2617/0.2828/0.3475\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 1034152.71875, Supervised Loss: 5.5172576904296875, Unsupervised Loss: 2068299.90625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3512/0.3465/0.3683, macro F1 - Span/Sentence/Combined: 0.3256/0.2870/0.3391\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 1033039.1354166666, Supervised Loss: 5.29772945245107, Unsupervised Loss: 2066072.9895833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3604/0.3446/0.3594, macro F1 - Span/Sentence/Combined: 0.3365/0.2853/0.3268\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 1031927.9947916666, Supervised Loss: 5.050878365834554, Unsupervised Loss: 2063850.9479166667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3602/0.3364/0.4195, macro F1 - Span/Sentence/Combined: 0.3225/0.2806/0.3619\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 1030818.9791666666, Supervised Loss: 4.9852661689122515, Unsupervised Loss: 2061632.96875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3591/0.3476/0.4201, macro F1 - Span/Sentence/Combined: 0.3206/0.2902/0.3725\n",
      "Training combination 27/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0001, 10, 0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fce4733c3124b8cbf100e18d4e54b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 1037883.0052083334, Supervised Loss: 8.60640835762024, Unsupervised Loss: 2075757.3854166667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3514/0.3834/0.3300, macro F1 - Span/Sentence/Combined: 0.2234/0.2859/0.3045\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 1036758.203125, Supervised Loss: 7.156996965408325, Unsupervised Loss: 2073509.2395833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4157/0.3653/0.3385, macro F1 - Span/Sentence/Combined: 0.2908/0.3012/0.3113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 1035635.2083333334, Supervised Loss: 6.612960974375407, Unsupervised Loss: 2071263.8020833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4157/0.3529/0.3433, macro F1 - Span/Sentence/Combined: 0.2908/0.2916/0.3131\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 1034514.2916666666, Supervised Loss: 6.32066547870636, Unsupervised Loss: 2069022.25\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4235/0.3591/0.3681, macro F1 - Span/Sentence/Combined: 0.2944/0.2965/0.3412\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 1033392.7760416666, Supervised Loss: 6.0085997978846235, Unsupervised Loss: 2066779.5416666667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4269/0.3478/0.3723, macro F1 - Span/Sentence/Combined: 0.3003/0.2825/0.3414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 1032271.9791666666, Supervised Loss: 5.713930765787761, Unsupervised Loss: 2064538.25\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4119/0.3344/0.4151, macro F1 - Span/Sentence/Combined: 0.3128/0.2739/0.3743\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 1031152.8854166666, Supervised Loss: 5.588889757792155, Unsupervised Loss: 2062300.1875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4000/0.3489/0.4191, macro F1 - Span/Sentence/Combined: 0.3255/0.2856/0.3820\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 1030034.9010416666, Supervised Loss: 5.4408061901728315, Unsupervised Loss: 2060064.3854166667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3953/0.3508/0.3956, macro F1 - Span/Sentence/Combined: 0.3341/0.2860/0.3538\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 1028919.015625, Supervised Loss: 5.317253073056539, Unsupervised Loss: 2057832.6979166667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3821/0.3529/0.4167, macro F1 - Span/Sentence/Combined: 0.3311/0.2878/0.3777\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 1027805.4322916666, Supervised Loss: 5.303211212158203, Unsupervised Loss: 2055605.5416666667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3939/0.3489/0.4066, macro F1 - Span/Sentence/Combined: 0.3391/0.2849/0.3756\n",
      "Training combination 28/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0001, 10, 0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34db280fc545408393523a86b8a22fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 1032878.375, Supervised Loss: 8.893491824467977, Unsupervised Loss: 2065747.875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2136/0.2583/0.3684, macro F1 - Span/Sentence/Combined: 0.1314/0.2218/0.3405\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 1031763.0260416666, Supervised Loss: 7.710412263870239, Unsupervised Loss: 2063518.34375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3292/0.2644/0.3529, macro F1 - Span/Sentence/Combined: 0.2225/0.2404/0.3358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 1030651.8177083334, Supervised Loss: 7.361545443534851, Unsupervised Loss: 2061296.28125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3292/0.2922/0.3785, macro F1 - Span/Sentence/Combined: 0.2225/0.2622/0.3581\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 1029545.5104166666, Supervised Loss: 6.903848926226298, Unsupervised Loss: 2059084.125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3249/0.2913/0.4031, macro F1 - Span/Sentence/Combined: 0.2170/0.2561/0.3810\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 1028442.9010416666, Supervised Loss: 6.686878522237142, Unsupervised Loss: 2056879.1145833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3209/0.2949/0.4149, macro F1 - Span/Sentence/Combined: 0.2136/0.2598/0.3815\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 1027342.8645833334, Supervised Loss: 6.484445095062256, Unsupervised Loss: 2054679.2604166667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2761/0.3028/0.4062, macro F1 - Span/Sentence/Combined: 0.2193/0.2654/0.3774\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 1026243.78125, Supervised Loss: 6.252323945363362, Unsupervised Loss: 2052481.3020833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2824/0.2930/0.4011, macro F1 - Span/Sentence/Combined: 0.2542/0.2528/0.3633\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 1025150.9479166666, Supervised Loss: 6.041106383005778, Unsupervised Loss: 2050295.8645833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3006/0.2885/0.4169, macro F1 - Span/Sentence/Combined: 0.2559/0.2498/0.3818\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 1024063.5208333334, Supervised Loss: 5.999568343162537, Unsupervised Loss: 2048121.0625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2765/0.2939/0.4171, macro F1 - Span/Sentence/Combined: 0.2298/0.2529/0.3825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 1022980.9635416666, Supervised Loss: 5.786456147829692, Unsupervised Loss: 2045956.1458333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2838/0.2921/0.4213, macro F1 - Span/Sentence/Combined: 0.2264/0.2521/0.3858\n",
      "Training combination 29/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0001, 20, 0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d9f67d1f354fb29b5b5f49776d308e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 1011525.5729166666, Supervised Loss: 8.894418875376383, Unsupervised Loss: 2023042.2395833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3680/0.3626/0.2888, macro F1 - Span/Sentence/Combined: 0.2038/0.2980/0.2530\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 1010411.7239583334, Supervised Loss: 6.92743456363678, Unsupervised Loss: 2020816.5416666667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3944/0.3567/0.3246, macro F1 - Span/Sentence/Combined: 0.1802/0.2875/0.3167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 1009301.2708333334, Supervised Loss: 6.303691466649373, Unsupervised Loss: 2018596.2395833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4053/0.3494/0.3333, macro F1 - Span/Sentence/Combined: 0.2072/0.2796/0.3143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 1008188.4583333334, Supervised Loss: 5.849680145581563, Unsupervised Loss: 2016371.0625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4203/0.3481/0.3081, macro F1 - Span/Sentence/Combined: 0.2245/0.2812/0.2932\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 1007083.7083333334, Supervised Loss: 5.697693228721619, Unsupervised Loss: 2014161.71875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4606/0.3578/0.3478, macro F1 - Span/Sentence/Combined: 0.3172/0.2865/0.3181\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 1005981.7083333334, Supervised Loss: 5.4366774161656695, Unsupervised Loss: 2011957.96875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4665/0.3501/0.3878, macro F1 - Span/Sentence/Combined: 0.3901/0.2810/0.3605\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 1004882.5052083334, Supervised Loss: 5.276327768961589, Unsupervised Loss: 2009759.7395833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4350/0.3567/0.3851, macro F1 - Span/Sentence/Combined: 0.3846/0.2874/0.3555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 1003789.0520833334, Supervised Loss: 5.10604190826416, Unsupervised Loss: 2007573.0\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4108/0.3647/0.3966, macro F1 - Span/Sentence/Combined: 0.3534/0.2904/0.3582\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 1002696.046875, Supervised Loss: 4.97578227519989, Unsupervised Loss: 2005387.1145833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4203/0.3588/0.3895, macro F1 - Span/Sentence/Combined: 0.3477/0.2895/0.3552\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 1001605.3385416666, Supervised Loss: 4.815107663472493, Unsupervised Loss: 2003205.8541666667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4062/0.3588/0.3798, macro F1 - Span/Sentence/Combined: 0.3405/0.2883/0.3416\n",
      "Training combination 30/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0001, 20, 0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd480af02044075b67f64bcec29f2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 1026183.453125, Supervised Loss: 8.449546376864115, Unsupervised Loss: 2052358.4583333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3370/0.3333/0.3805, macro F1 - Span/Sentence/Combined: 0.1466/0.2717/0.3648\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 1025065.2864583334, Supervised Loss: 7.026081562042236, Unsupervised Loss: 2050123.5729166667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3023/0.3280/0.3979, macro F1 - Span/Sentence/Combined: 0.0833/0.2778/0.3741\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 1023948.671875, Supervised Loss: 6.475104967753093, Unsupervised Loss: 2047890.84375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3295/0.3291/0.3874, macro F1 - Span/Sentence/Combined: 0.0891/0.2807/0.3651\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 1022831.9791666666, Supervised Loss: 6.138879776000977, Unsupervised Loss: 2045657.8125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2824/0.3333/0.4000, macro F1 - Span/Sentence/Combined: 0.1098/0.2917/0.3765\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 1021722.2291666666, Supervised Loss: 5.815087596575419, Unsupervised Loss: 2043438.6458333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2769/0.3396/0.3900, macro F1 - Span/Sentence/Combined: 0.1787/0.2951/0.3611\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 1020616.0104166666, Supervised Loss: 5.567280848821004, Unsupervised Loss: 2041226.4583333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3142/0.3375/0.4047, macro F1 - Span/Sentence/Combined: 0.2349/0.2944/0.3615\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 1019512.90625, Supervised Loss: 5.403448422749837, Unsupervised Loss: 2039020.40625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2997/0.3427/0.4118, macro F1 - Span/Sentence/Combined: 0.2504/0.2974/0.3665\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 1018414.4947916666, Supervised Loss: 5.276472290356954, Unsupervised Loss: 2036823.7083333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3344/0.3396/0.3977, macro F1 - Span/Sentence/Combined: 0.2902/0.2938/0.3573\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 1017314.4739583334, Supervised Loss: 5.109228849411011, Unsupervised Loss: 2034623.8333333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3486/0.3333/0.4069, macro F1 - Span/Sentence/Combined: 0.3063/0.2898/0.3727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 1016213.4270833334, Supervised Loss: 5.009296735127767, Unsupervised Loss: 2032421.8541666667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3515/0.3323/0.4290, macro F1 - Span/Sentence/Combined: 0.3083/0.2898/0.3851\n",
      "Training combination 31/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0001, 20, 0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c482862ccfc4ab7b629bb1f6a9236bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 1032078.8802083334, Supervised Loss: 8.917195796966553, Unsupervised Loss: 2064148.8541666667\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2820/0.2899/0.2684, macro F1 - Span/Sentence/Combined: 0.1424/0.2676/0.2679\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 1030966.6510416666, Supervised Loss: 7.405276894569397, Unsupervised Loss: 2061925.8958333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3264/0.2948/0.2961, macro F1 - Span/Sentence/Combined: 0.1814/0.2768/0.3021\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 1029850.3020833334, Supervised Loss: 6.705034891764323, Unsupervised Loss: 2059693.90625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3264/0.3140/0.3226, macro F1 - Span/Sentence/Combined: 0.1814/0.2850/0.3281\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 1028739.8541666666, Supervised Loss: 6.274211883544922, Unsupervised Loss: 2057473.4270833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3333/0.3205/0.3333, macro F1 - Span/Sentence/Combined: 0.1993/0.2871/0.3346\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 1027628.7916666666, Supervised Loss: 6.0585817495981855, Unsupervised Loss: 2055251.53125\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3549/0.3149/0.3642, macro F1 - Span/Sentence/Combined: 0.2606/0.2825/0.3534\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 1026517.8958333334, Supervised Loss: 5.873836835225423, Unsupervised Loss: 2053029.90625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3458/0.3090/0.3519, macro F1 - Span/Sentence/Combined: 0.3163/0.2758/0.3403\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 1025410.953125, Supervised Loss: 5.649048368136088, Unsupervised Loss: 2050816.2395833333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3392/0.3059/0.3769, macro F1 - Span/Sentence/Combined: 0.3122/0.2729/0.3493\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 1024304.109375, Supervised Loss: 5.510397434234619, Unsupervised Loss: 2048602.71875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3699/0.3149/0.3916, macro F1 - Span/Sentence/Combined: 0.3358/0.2795/0.3629\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 1023202.6354166666, Supervised Loss: 5.367011706034343, Unsupervised Loss: 2046399.90625\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3771/0.3149/0.3927, macro F1 - Span/Sentence/Combined: 0.3488/0.2806/0.3611\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 1022095.75, Supervised Loss: 5.281500021616618, Unsupervised Loss: 2044186.2083333333\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3837/0.3099/0.3939, macro F1 - Span/Sentence/Combined: 0.3534/0.2785/0.3595\n",
      "Training combination 32/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0001, 20, 0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31554e1ad1245fba9674d4cfbe36528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 1065420.2604166667, Supervised Loss: 8.761080702145895, Unsupervised Loss: 2130831.7708333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3230/0.3373/0.3626, macro F1 - Span/Sentence/Combined: 0.1877/0.2684/0.3453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 1064296.25, Supervised Loss: 7.553808927536011, Unsupervised Loss: 2128584.9583333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2564/0.3471/0.3459, macro F1 - Span/Sentence/Combined: 0.1232/0.2900/0.3317\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 1063169.5729166667, Supervised Loss: 7.058484315872192, Unsupervised Loss: 2126332.0833333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2564/0.3526/0.3416, macro F1 - Span/Sentence/Combined: 0.1232/0.3038/0.3234\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 1062049.8020833333, Supervised Loss: 6.996112942695618, Unsupervised Loss: 2124092.6041666665\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2976/0.3509/0.3520, macro F1 - Span/Sentence/Combined: 0.1668/0.3021/0.3167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 1060925.1041666667, Supervised Loss: 6.484516501426697, Unsupervised Loss: 2121843.7083333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3622/0.3509/0.3722, macro F1 - Span/Sentence/Combined: 0.2606/0.3026/0.3273\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 1059805.28125, Supervised Loss: 6.484723409016927, Unsupervised Loss: 2119604.0833333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3925/0.3567/0.3726, macro F1 - Span/Sentence/Combined: 0.3493/0.3066/0.3397\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 1058687.3541666667, Supervised Loss: 6.188777009646098, Unsupervised Loss: 2117368.5208333335\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3883/0.3460/0.3842, macro F1 - Span/Sentence/Combined: 0.3352/0.2974/0.3429\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 1057568.9479166667, Supervised Loss: 6.0745006402333575, Unsupervised Loss: 2115131.8541666665\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3900/0.3481/0.3831, macro F1 - Span/Sentence/Combined: 0.3403/0.2974/0.3464\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 1056454.125, Supervised Loss: 5.843164443969727, Unsupervised Loss: 2112902.375\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3827/0.3430/0.3782, macro F1 - Span/Sentence/Combined: 0.3258/0.2968/0.3395\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 1055341.875, Supervised Loss: 5.819536050160726, Unsupervised Loss: 2110677.875\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3886/0.3402/0.3944, macro F1 - Span/Sentence/Combined: 0.3299/0.2933/0.3491\n",
      "Training combination 33/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0005, 5, 0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69285c2af1aa484b91dc3d07d977d8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 5402971.5, Supervised Loss: 8.543102542559305, Unsupervised Loss: 10805934.416666666\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3741/0.3771/0.3978, macro F1 - Span/Sentence/Combined: 0.2668/0.3119/0.3834\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 5397180.833333333, Supervised Loss: 7.335523049036662, Unsupervised Loss: 10794354.333333334\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3741/0.3866/0.3945, macro F1 - Span/Sentence/Combined: 0.2668/0.3323/0.3809\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 5391399.333333333, Supervised Loss: 6.714542786280314, Unsupervised Loss: 10782792.083333334\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3741/0.3908/0.4031, macro F1 - Span/Sentence/Combined: 0.2668/0.3432/0.3835\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 5385622.666666667, Supervised Loss: 6.207761168479919, Unsupervised Loss: 10771239.166666666\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3776/0.3862/0.4225, macro F1 - Span/Sentence/Combined: 0.2708/0.3360/0.3940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 5379860.0, Supervised Loss: 5.992459972699483, Unsupervised Loss: 10759714.0\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3676/0.3908/0.4199, macro F1 - Span/Sentence/Combined: 0.2699/0.3407/0.3830\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 5374112.833333333, Supervised Loss: 5.770439147949219, Unsupervised Loss: 10748219.833333334\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3784/0.3919/0.4068, macro F1 - Span/Sentence/Combined: 0.3138/0.3438/0.3674\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 5368372.333333333, Supervised Loss: 5.678651809692383, Unsupervised Loss: 10736738.833333334\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3667/0.4035/0.4124, macro F1 - Span/Sentence/Combined: 0.3289/0.3524/0.3788\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 5362636.083333333, Supervised Loss: 5.492526690165202, Unsupervised Loss: 10725266.583333334\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3290/0.4000/0.3977, macro F1 - Span/Sentence/Combined: 0.2973/0.3518/0.3624\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 5356906.041666667, Supervised Loss: 5.388827006022136, Unsupervised Loss: 10713806.916666666\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3093/0.4012/0.3790, macro F1 - Span/Sentence/Combined: 0.2798/0.3488/0.3418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 5351181.791666667, Supervised Loss: 5.294974128405253, Unsupervised Loss: 10702358.416666666\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3207/0.3942/0.4126, macro F1 - Span/Sentence/Combined: 0.2881/0.3447/0.3606\n",
      "Training combination 34/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0005, 5, 0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b42c05efee4f03a71b74be3ee728b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 5094787.416666667, Supervised Loss: 8.72429366906484, Unsupervised Loss: 10189566.166666666\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3774/0.3732/0.4214, macro F1 - Span/Sentence/Combined: 0.2134/0.3169/0.4074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 5089200.958333333, Supervised Loss: 7.292556802431743, Unsupervised Loss: 10178394.583333334\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3672/0.3499/0.4054, macro F1 - Span/Sentence/Combined: 0.1833/0.3055/0.3842\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 5083628.416666667, Supervised Loss: 6.914999167124431, Unsupervised Loss: 10167249.833333334\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3672/0.3303/0.3830, macro F1 - Span/Sentence/Combined: 0.1833/0.2913/0.3670\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 5078061.208333333, Supervised Loss: 6.396375775337219, Unsupervised Loss: 10156116.083333334\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3672/0.3436/0.3571, macro F1 - Span/Sentence/Combined: 0.1833/0.3041/0.3432\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 5072500.416666667, Supervised Loss: 6.228524843851726, Unsupervised Loss: 10144994.583333334\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3630/0.3515/0.4044, macro F1 - Span/Sentence/Combined: 0.1827/0.3147/0.3786\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 5066950.0, Supervised Loss: 5.98348331451416, Unsupervised Loss: 10133893.916666666\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3590/0.3353/0.3857, macro F1 - Span/Sentence/Combined: 0.2251/0.2980/0.3585\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 5061406.166666667, Supervised Loss: 5.840917785962422, Unsupervised Loss: 10122806.416666666\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3386/0.3423/0.3764, macro F1 - Span/Sentence/Combined: 0.2883/0.3181/0.3494\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 5055873.208333333, Supervised Loss: 5.649261037508647, Unsupervised Loss: 10111740.583333334\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3519/0.3373/0.3764, macro F1 - Span/Sentence/Combined: 0.3152/0.3003/0.3537\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 5050352.833333333, Supervised Loss: 5.548040787378947, Unsupervised Loss: 10100700.0\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3486/0.3404/0.3743, macro F1 - Span/Sentence/Combined: 0.2876/0.2999/0.3494\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 5044844.416666667, Supervised Loss: 5.400491992632548, Unsupervised Loss: 10089683.666666666\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3526/0.3415/0.3718, macro F1 - Span/Sentence/Combined: 0.2914/0.3042/0.3455\n",
      "Training combination 35/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0005, 5, 0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947c43ba8ae64c9694671160a293ff84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 5039601.958333333, Supervised Loss: 8.692140022913614, Unsupervised Loss: 10079195.25\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3917/0.2601/0.2804, macro F1 - Span/Sentence/Combined: 0.2145/0.2061/0.2645\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 5034062.208333333, Supervised Loss: 7.503797729810079, Unsupervised Loss: 10068116.916666666\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2784/0.2593/0.2967, macro F1 - Span/Sentence/Combined: 0.1280/0.2190/0.2897\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 5028525.708333333, Supervised Loss: 6.938533306121826, Unsupervised Loss: 10057044.416666666\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.2774/0.2654/0.3278, macro F1 - Span/Sentence/Combined: 0.1280/0.2245/0.3124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 5022994.333333333, Supervised Loss: 6.691024343172709, Unsupervised Loss: 10045981.916666666\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3344/0.2622/0.3273, macro F1 - Span/Sentence/Combined: 0.1678/0.2233/0.2999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 5017466.291666667, Supervised Loss: 6.381245454152425, Unsupervised Loss: 10034926.25\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3310/0.2346/0.3226, macro F1 - Span/Sentence/Combined: 0.1751/0.2060/0.3073\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Combined Loss: 5011941.75, Supervised Loss: 6.205052216847737, Unsupervised Loss: 10023877.25\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3333/0.2407/0.3121, macro F1 - Span/Sentence/Combined: 0.2197/0.2069/0.2968\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Combined Loss: 5006421.875, Supervised Loss: 6.081241329511006, Unsupervised Loss: 10012837.5\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3333/0.2439/0.3314, macro F1 - Span/Sentence/Combined: 0.2218/0.2114/0.3147\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Combined Loss: 5000905.5, Supervised Loss: 5.891339619954427, Unsupervised Loss: 10001805.0\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3344/0.2553/0.3353, macro F1 - Span/Sentence/Combined: 0.2719/0.2184/0.3198\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Combined Loss: 4995392.833333333, Supervised Loss: 5.728643417358398, Unsupervised Loss: 9990779.75\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3699/0.2462/0.3423, macro F1 - Span/Sentence/Combined: 0.3158/0.2132/0.3250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Combined Loss: 4989884.083333333, Supervised Loss: 5.655168056488037, Unsupervised Loss: 9979762.5\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.3590/0.2553/0.3576, macro F1 - Span/Sentence/Combined: 0.3159/0.2200/0.3326\n",
      "Training combination 36/18432: (0.5, 1e-05, 0.5, 0.0005, 5, 768, 0.0005, 5, 0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d05d3f9375842ac930fda64134471a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Combined Loss: 5072247.708333333, Supervised Loss: 8.759739518165588, Unsupervised Loss: 10144486.5\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4190/0.2951/0.3577, macro F1 - Span/Sentence/Combined: 0.2732/0.2527/0.3356\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Combined Loss: 5066678.833333333, Supervised Loss: 7.9084471464157104, Unsupervised Loss: 10133349.75\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4092/0.3048/0.3601, macro F1 - Span/Sentence/Combined: 0.2678/0.2643/0.3427\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Combined Loss: 5061117.541666667, Supervised Loss: 7.469520489374797, Unsupervised Loss: 10122227.666666666\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4052/0.3197/0.3799, macro F1 - Span/Sentence/Combined: 0.2648/0.2669/0.3624\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Combined Loss: 5055575.625, Supervised Loss: 7.190291881561279, Unsupervised Loss: 10111144.166666666\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4145/0.3230/0.3881, macro F1 - Span/Sentence/Combined: 0.2736/0.2711/0.3644\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Combined Loss: 5050053.583333333, Supervised Loss: 6.922079046567281, Unsupervised Loss: 10100100.25\n",
      "Validation Metrics - micro F1 - Span/Sentence/Combined: 0.4085/0.3178/0.3810, macro F1 - Span/Sentence/Combined: 0.2740/0.2643/0.3578\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4dd1cbc87249b28793e525e16d7d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 112\u001b[0m\n\u001b[1;32m     99\u001b[0m search_space \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.8\u001b[39m],\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1e-5\u001b[39m, \u001b[38;5;241m2e-5\u001b[39m, \u001b[38;5;241m5e-4\u001b[39m, \u001b[38;5;241m1e-3\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.5\u001b[39m]\n\u001b[1;32m    109\u001b[0m }\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Call the grid search function\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m results\n",
      "Cell \u001b[0;32mIn [38], line 65\u001b[0m, in \u001b[0;36mgrid_search\u001b[0;34m(train_dataloader, test_dataloader, search_space, num_epochs)\u001b[0m\n\u001b[1;32m     60\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Define loss_function if needed (add this if your train function requires it)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Train the model with the current hyperparameters\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m epoch_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Write the metrics to the CSV file\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n",
      "Cell \u001b[0;32mIn [33], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_function, alpha, num_epochs, tau_min, tau_decay, device, save_path, save)\u001b[0m\n\u001b[1;32m     27\u001b[0m unsupervised_total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     28\u001b[0m batch_progress \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_dataloader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m batch_progress:            \n\u001b[1;32m     31\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m iteration \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     33\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tqdm/notebook.py:259\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn [21], line 43\u001b[0m, in \u001b[0;36mArticleDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     40\u001b[0m     sentence_ids\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_sentence_length)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Tokenize and pad/truncate the SRL items\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m predicate_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(predicate, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_arg_length, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m predicate \u001b[38;5;129;01min\u001b[39;00m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m srl]]\n\u001b[1;32m     44\u001b[0m arg0_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(arg0, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_arg_length, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m arg0 \u001b[38;5;129;01min\u001b[39;00m [item\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mARG0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m srl]]\n\u001b[1;32m     45\u001b[0m arg1_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(arg1, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_arg_length, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m arg1 \u001b[38;5;129;01min\u001b[39;00m [item\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mARG1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m srl]]\n",
      "Cell \u001b[0;32mIn [21], line 43\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m     sentence_ids\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_sentence_length)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Tokenize and pad/truncate the SRL items\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m predicate_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_arg_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m predicate \u001b[38;5;129;01min\u001b[39;00m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m srl]]\n\u001b[1;32m     44\u001b[0m arg0_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(arg0, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_arg_length, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m arg0 \u001b[38;5;129;01min\u001b[39;00m [item\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mARG0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m srl]]\n\u001b[1;32m     45\u001b[0m arg1_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(arg1, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_arg_length, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m arg1 \u001b[38;5;129;01min\u001b[39;00m [item\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mARG1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m srl]]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2256\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2219\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2220\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2221\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2239\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   2240\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2241\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2242\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2243\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2256\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2266\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2588\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2578\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2580\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2581\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2585\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2586\u001b[0m )\n\u001b[0;32m-> 2588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2591\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils.py:652\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    650\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_for_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfirst_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpair_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msecond_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:3035\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.prepare_for_model\u001b[0;34m(self, ids, pair_ids, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[1;32m   3033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_special_tokens:\n\u001b[1;32m   3034\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_inputs_with_special_tokens(ids, pair_ids)\n\u001b[0;32m-> 3035\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_token_type_ids_from_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3036\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3037\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m ids \u001b[38;5;241m+\u001b[39m pair_ids \u001b[38;5;28;01mif\u001b[39;00m pair \u001b[38;5;28;01melse\u001b[39;00m ids\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/tokenization_bert.py:344\u001b[0m, in \u001b[0;36mBertTokenizer.create_token_type_ids_from_sequences\u001b[0;34m(self, token_ids_0, token_ids_1)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_token_type_ids_from_sequences\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m, token_ids_0: List[\u001b[38;5;28mint\u001b[39m], token_ids_1: Optional[List[\u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    323\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    324\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m    pair mask has the following format:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m        `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m     sep \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msep_token_id\u001b[49m]\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token_id]\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token_ids_1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:1107\u001b[0m, in \u001b[0;36mSpecialTokensMixin.sep_token_id\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sep_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msep_token\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils.py:575\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokens, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_token_to_id_with_added_voc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils.py:588\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._convert_token_to_id_with_added_voc\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder:\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder[token]\n\u001b[0;32m--> 588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_token_to_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/tokenization_bert.py:257\u001b[0m, in \u001b[0;36mBertTokenizer._convert_token_to_id\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_token_to_id\u001b[39m(\u001b[38;5;28mself\u001b[39m, token):\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a token (str) in an id using the vocab.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 768\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def grid_search(train_dataloader, test_dataloader, search_space, num_epochs=10):\n",
    "    # Store the results for each hyperparameter combination\n",
    "    results = {}\n",
    "\n",
    "    # Fixed values for K and num_frames\n",
    "    K = 14\n",
    "    num_frames = 14\n",
    "\n",
    "    # Fixed values for dropout_prob and bert_model_name (adjust if necessary)\n",
    "    bert_model_name = \"../notebooks/models/fine-tuned-model/\"\n",
    "\n",
    "    # Initialize the file to write metrics\n",
    "    with open(\"../notebooks/grid_search_metrics.csv\", \"w\", newline='') as csvfile:\n",
    "        fieldnames = ['combination', 'alpha', 'lr', 'D_h', 'lambda_orthogonality', 'M', 't', 'tau_min', 'tau_decay', 'dropout_prob', 'epoch', 'f1_span_micro', 'f1_span_macro', 'f1_sentence_micro', 'f1_sentence_macro', 'f1_combined_micro', 'f1_combined_macro']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Calculate the total number of combinations\n",
    "        total_combinations = 1\n",
    "        for key, values in search_space.items():\n",
    "            total_combinations *= len(values)\n",
    "\n",
    "        # Loop through all combinations\n",
    "        for idx, combination in enumerate(product(*search_space.values())):\n",
    "            print(f\"Training combination {idx + 1}/{total_combinations}: {combination}\")\n",
    "\n",
    "            # Extract hyperparameters from the current combination\n",
    "            alpha, lr, tau_min, tau_decay, t, D_h, lambda_orthogonality, M, dropout_prob = combination\n",
    "\n",
    "            # Initialize the model with current hyperparameters\n",
    "            model = FRISS(embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob, bert_model_name)\n",
    "            model.to(device)\n",
    "        \n",
    "                \n",
    "            # Compute the `weight` parameter for each label\n",
    "            label_frequencies = y.mean()\n",
    "            weights = 1 / (label_frequencies + 1e-10)  # Adding a small value to avoid division by zero\n",
    "\n",
    "            # Compute the `pos_weight` parameter\n",
    "            pos_weights = (1 - label_frequencies) / (label_frequencies + 1e-10)\n",
    "\n",
    "            # Convert the computed weights and pos_weights to PyTorch tensors\n",
    "            weights_tensor = torch.tensor(weights.values, dtype=torch.float32).to(device)\n",
    "            pos_weights_tensor = torch.tensor(pos_weights.values, dtype=torch.float32).to(device)\n",
    "\n",
    "            loss_function = nn.BCEWithLogitsLoss(weight=weights_tensor, pos_weight=pos_weights_tensor, reduction=\"mean\")\n",
    "        \n",
    "            # Define the optimizer\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "            # Define loss_function if needed (add this if your train function requires it)\n",
    "\n",
    "            # Train the model with the current hyperparameters\n",
    "            epoch_metrics = train(model, train_dataloader, test_dataloader, optimizer, loss_function, alpha=alpha, num_epochs=num_epochs, tau_min=tau_min, tau_decay=tau_decay, device=device, save=False)\n",
    "\n",
    "            # Write the metrics to the CSV file\n",
    "            for epoch in range(num_epochs):\n",
    "                f1_span_micro = epoch_metrics['f1_span_micro'][epoch]\n",
    "                f1_span_macro = epoch_metrics['f1_span_macro'][epoch]\n",
    "                f1_sentence_micro = epoch_metrics['f1_sentence_micro'][epoch]\n",
    "                f1_sentence_macro = epoch_metrics['f1_sentence_macro'][epoch]\n",
    "                f1_combined_micro = epoch_metrics['f1_combined_micro'][epoch]\n",
    "                f1_combined_macro = epoch_metrics['f1_combined_macro'][epoch]\n",
    "                row = {\n",
    "                    'combination': idx,\n",
    "                    'alpha': alpha,\n",
    "                    'lr': lr,\n",
    "                    'D_h': D_h,\n",
    "                    'lambda_orthogonality': lambda_orthogonality,\n",
    "                    'M': M,\n",
    "                    't': t,\n",
    "                    'tau_min': tau_min,\n",
    "                    'tau_decay': tau_decay,\n",
    "                    'dropout_prob': dropout_prob,\n",
    "                    'epoch': epoch + 1,\n",
    "                    'f1_span_micro': f1_span_micro,\n",
    "                    'f1_span_macro': f1_span_macro,\n",
    "                    'f1_sentence_micro': f1_sentence_micro,\n",
    "                    'f1_sentence_macro': f1_sentence_macro,\n",
    "                    'f1_combined_micro': f1_combined_micro,\n",
    "                    'f1_combined_macro': f1_combined_macro\n",
    "                }\n",
    "                writer.writerow(row)\n",
    "                csvfile.flush()\n",
    "\n",
    "    return results\n",
    "\n",
    "search_space = {\n",
    "    'alpha': [0.5, 0.2, 0.8],\n",
    "    'lr': [1e-5, 2e-5, 5e-4, 1e-3],\n",
    "    'tau_min': [0.5],\n",
    "    'tau_decay': [5e-4],\n",
    "    't': [5, 8, 10, 20],\n",
    "    'D_h': [768, 768 * 2, 768 // 2, 768 * 3],\n",
    "    'lambda_orthogonality': [1e-6, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2],\n",
    "    'M': [5, 8, 10, 20],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "# Call the grid search function\n",
    "results = grid_search(train_dataloader, test_dataloader, search_space, 10)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_path(path, embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob, sentence_heads, srl_heads, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads the weights into an instance of the model class from the given path.\n",
    "    \n",
    "    Args:\n",
    "    - model_class (torch.nn.Module): The class of the model (uninitialized).\n",
    "    - path (str): Path to the saved weights.\n",
    "    - device (str): Device to load the model on ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - model (torch.nn.Module): Model with weights loaded.\n",
    "    \"\"\"\n",
    "\n",
    "    # Model instantiation\n",
    "    model = FRISS(embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob=dropout_prob, sentence_heads=sentence_heads, srl_heads=srl_heads)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    \n",
    "    #model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 768\n",
    "num_frames = 14\n",
    "\n",
    "D_h = 768\n",
    "lambda_orthogonality = 0.000001\n",
    "\n",
    "K = 14\n",
    "t = 5\n",
    "M = 10\n",
    "tau_min = 0.5\n",
    "tau_decay = 5e-4\n",
    "\n",
    "dropout_prob = 0.1\n",
    "\n",
    "sentence_heads = 8\n",
    "srl_heads = 7\n",
    "\n",
    "\n",
    "model = load_model_from_path('models/model1.pth', embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob, sentence_heads, srl_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, y_columns, device='cuda'):\n",
    "    \"\"\"\n",
    "    Make predictions with the given model and dataloader.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): The model to make predictions with.\n",
    "    - dataloader (DataLoader): DataLoader for the dataset to predict on.\n",
    "    - y_columns (pandas.Index): Column names from the y dataframe which correspond to labels.\n",
    "    - device (str): Device to make predictions on ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - predicted_labels (list of lists): List containing the predicted labels for each instance.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds_span = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move data to device\n",
    "            sentence_ids = batch['sentence_ids'].to(device)\n",
    "            predicate_ids = batch['predicate_ids'].to(device)\n",
    "            arg0_ids = batch['arg0_ids'].to(device)\n",
    "            arg1_ids = batch['arg1_ids'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            _, span_logits, sentence_logits, combined_logits = model(sentence_ids, predicate_ids, arg0_ids, arg1_ids, 0.6)\n",
    "            combined_pred = (torch.sigmoid(combined_logits) > 0.5).float()\n",
    "\n",
    "            all_preds_span.append(combined_pred.cpu().numpy())\n",
    "                \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    predictions = np.vstack(all_preds_span)\n",
    "    \n",
    "    # Convert boolean predictions to labels\n",
    "    predicted_labels = []\n",
    "    for pred in predictions:\n",
    "        labels = list(y_columns[pred.astype(bool)])\n",
    "        predicted_labels.append(labels)\n",
    "    \n",
    "    return predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed article 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Morality',\n",
       "  'Security_and_defense',\n",
       "  'Legality_Constitutionality_and_jurisprudence',\n",
       "  'Political',\n",
       "  'External_regulation_and_reputation']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# article813452859\n",
    "article = \"\"\"Sadiq Khan Slammed for Pro-EU 'Message of Support' During Firework Display\n",
    "\n",
    "The spectacular fireworks that lit up the London sky on Monday night caused a stir on social media over the display's pro-EU message, at a time when the nation is divided over its looming withdrawal from the bloc.\n",
    "London Mayor Sadiq Khan faced mounting criticism after the capital's New Year's Eve fireworks display, which celebrated ties with the European Union, left a bad taste in the mouths of some Brits.\n",
    "The 135-metre-high London Eye was lit up in blue while its tubs turned yellow, with the giant Ferris wheel resembling the star-studded flag of the European Union.Sadiq Khan called his fireworks display a \"message of support\" to EU citizens living in London.\n",
    "\"Our one million EU citizens are Londoners, they make a huge contribution, and no matter the outcome of Brexit — they will always be welcome\", he said.\n",
    "To the one million EU citizens who have made our city your home: you are Londoners, you make a huge contribution and you are welcome here.\n",
    "I'm proud that tonight we will welcome in the new year with a message of support to you.\n",
    "#LondonNYE #LondonIsOpen https://t.co/XctrgfXXaM — Sadiq Khan (@SadiqKhan) 31 декабря 2018 г.\n",
    "However, a host of Londoners rushed to Twitter to accuse their mayor of \"politicising\" the celebrations — with some are even calling for his resignation.\n",
    "I cannot believe this event has been politicised.\n",
    "This man has no shame.\n",
    "Just resign.\n",
    "— wayne campbell (@campbs177) 31 декабря 2018 г.\n",
    "Thanks a lot Sadiq Khan you ruined the fireworks display by talking about Europe, need I remind you about Brexit.\n",
    "You have started of the new year by talking about relationships with the European Union.\n",
    "Well done.\n",
    "We need Boris Johnson back.\n",
    "— Mitchell T Cannon (@MitchellTCanno1) 1 января 2019 г.\n",
    "Another shameless attempt at using party politics on what is supposed to be a happy occasion — droneguy (@shelbyguitars) 1 января 2019 г.\n",
    "Politicising another innocent event that should be no different to anyone no matter who they are or where they are from!\n",
    "Shameful!\n",
    "!\n",
    "— Mike Dyer (@Miked2372Mike) 31 декабря 2018 г.\n",
    "Someone was stabbed down the road from me last night.\n",
    "How about sorting that stuff out instead of politicizing something that should be fun for everyone?How many times does it have to be said.\n",
    "Commenting on Brexit isn't your job.\n",
    "— Peter Rockett (@rockettp) 31 декабря 2018 г.\n",
    "The UK voted to leave the EU in June 2016 via a nationwide referendum, with 51.9 per cent voting in favour of pulling out of the bloc, while 48.1 per cent wanted to remain.\n",
    "The withdrawal is scheduled for the end of March; the Article 50 deadline.\n",
    "The Remain sentiment dominated London, with nearly 60 percent of voters wanting Britain to stay in the European Union.\n",
    "Sadiq Khan, an outspoken Remainer himself, earlier called for a second referendum on Brexit.\n",
    "\"The government's abject failure — and the huge risk we face of a bad deal or a 'no deal' Brexit — means that giving people a fresh say is now the right — and only — approach left for our country,\" he said in September.\n",
    "\"\"\"\n",
    "\n",
    "test_article = get_article_dataloader(article, tokenizer)\n",
    "predict(model, test_article, y.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run test for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed article 1/53\n",
      "Processed article 2/53\n",
      "Processed article 3/53\n",
      "Processed article 4/53\n",
      "Processed article 5/53\n",
      "Processed article 6/53\n",
      "Processed article 7/53\n",
      "Processed article 8/53\n",
      "Processed article 9/53\n",
      "Processed article 10/53\n",
      "Processed article 11/53\n",
      "Processed article 12/53\n",
      "Processed article 13/53\n",
      "Processed article 14/53\n",
      "Processed article 15/53\n",
      "Processed article 16/53\n",
      "Processed article 17/53\n",
      "Processed article 18/53\n",
      "Processed article 19/53\n",
      "Processed article 20/53\n",
      "Processed article 21/53\n",
      "Processed article 22/53\n",
      "Processed article 23/53\n",
      "Processed article 24/53\n",
      "Processed article 25/53\n",
      "Processed article 26/53\n",
      "Processed article 27/53\n",
      "Processed article 28/53\n",
      "Processed article 29/53\n",
      "Processed article 30/53\n",
      "Processed article 31/53\n",
      "Processed article 32/53\n",
      "Processed article 33/53\n",
      "Processed article 34/53\n",
      "Processed article 35/53\n",
      "Processed article 36/53\n",
      "Processed article 37/53\n",
      "Processed article 38/53\n",
      "Processed article 39/53\n",
      "Processed article 40/53\n",
      "Processed article 41/53\n",
      "Processed article 42/53\n",
      "Processed article 43/53\n",
      "Processed article 44/53\n",
      "Processed article 45/53\n",
      "Processed article 46/53\n",
      "Processed article 47/53\n",
      "Processed article 48/53\n",
      "Processed article 49/53\n",
      "Processed article 50/53\n",
      "Processed article 51/53\n",
      "Processed article 52/53\n",
      "Processed article 53/53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = get_test_dataloader(df_test[\"content\"], tokenizer, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(model, test_dataloader, y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.concat([df_test, df_preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds[\"pred_frames\"] = df_preds.apply(lambda l: list([l[0], l[1], l[2], l[3], l[4], l[5]]), axis=1)\n",
    "\n",
    "df_preds[\"pred_frames\"] = df_preds[\"pred_frames\"].apply(lambda l: \",\".join([ f for f in l if f is not None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.to_csv(\"../notebooks/test.csv\", sep=\"\\t\", index=False, columns=[\"article_id\", \"pred_frames\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(model, dataloader, y_columns, device='cuda'):\n",
    "    \"\"\"\n",
    "    Make predictions with the given model and dataloader.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): The model to make predictions with.\n",
    "    - dataloader (DataLoader): DataLoader for the dataset to predict on.\n",
    "    - y_columns (pandas.Index): Column names from the y dataframe which correspond to labels.\n",
    "    - device (str): Device to make predictions on ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - predicted_labels (list of lists): List containing the predicted labels for each instance.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds_span = []\n",
    "    \n",
    "    # Initialize usage lists for each label\n",
    "    num_labels = len(y_columns)\n",
    "    all_used_labels_p = []\n",
    "    all_used_labels_a0 = []\n",
    "    all_used_labels_a1 = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            used_labels_p = []\n",
    "            used_labels_a0 = []\n",
    "            used_labels_a1 = []\n",
    "    \n",
    "            # Move data to device\n",
    "            sentence_ids = batch['sentence_ids'].to(device)\n",
    "            predicate_ids = batch['predicate_ids'].to(device)\n",
    "            arg0_ids = batch['arg0_ids'].to(device)\n",
    "            arg1_ids = batch['arg1_ids'].to(device)\n",
    "            \n",
    "            sentence_embeddings, predicate_embeddings, arg0_embeddings, arg1_embeddings = model.aggregation(sentence_ids, predicate_ids, arg0_ids, arg1_ids)\n",
    "            \n",
    "            # Process each span\n",
    "            for span_idx in range(sentence_embeddings.size(1)):\n",
    "                s_sentence_span = sentence_embeddings[:, span_idx, :]\n",
    "                v_p_span = predicate_embeddings[:, span_idx, :]\n",
    "                v_a0_span = arg0_embeddings[:, span_idx, :]\n",
    "                v_a1_span = arg1_embeddings[:, span_idx, :]\n",
    "            \n",
    "                #unsupervised.combined_autoencoder v_p, v_a0, v_a1, v_sentence, tau\n",
    "                output = model.unsupervised.combined_autoencoder(v_p_span, v_a0_span, v_a1_span, s_sentence_span, 0.6)\n",
    "                \n",
    "                #print(output[\"p\"][\"g\"].cpu().numpy())\n",
    "                used_labels_p.append(output[\"p\"][\"g\"].cpu().numpy())\n",
    "                used_labels_a0.append(output[\"a0\"][\"g\"].cpu().numpy())\n",
    "                used_labels_a1.append(output[\"a1\"][\"g\"].cpu().numpy())\n",
    "\n",
    "            \n",
    "            # Forward pass\n",
    "            _, span_logits, sentence_logits, combined_logits = model(sentence_ids, predicate_ids, arg0_ids, arg1_ids, 0.6)\n",
    "            combined_pred = (torch.sigmoid(combined_logits) > 0.5).float()\n",
    "\n",
    "            all_preds_span.append(combined_pred.cpu().numpy())\n",
    "                \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            all_used_labels_p.append(used_labels_p)\n",
    "            all_used_labels_a0.append(used_labels_a0)\n",
    "            all_used_labels_a1.append(used_labels_a1)\n",
    "\n",
    "    predictions = np.vstack(all_preds_span)\n",
    "    \n",
    "    # Convert boolean predictions to labels\n",
    "    predicted_labels = []\n",
    "    for pred in predictions:\n",
    "        labels = list(y_columns[pred.astype(bool)])\n",
    "        predicted_labels.append(labels)\n",
    "    \n",
    "    return predicted_labels, all_used_labels_p, all_used_labels_a0, all_used_labels_a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load SRL from Pickle\n",
      "                                           Class  Train Distribution (%)  \\\n",
      "0                                       Morality               47.938144   \n",
      "1                           Security_and_defense               42.783505   \n",
      "2             Policy_prescription_and_evaluation               14.690722   \n",
      "3   Legality_Constitutionality_and_jurisprudence               46.391753   \n",
      "4                                       Economic                6.185567   \n",
      "5                                      Political               54.381443   \n",
      "6                           Crime_and_punishment               53.350515   \n",
      "7             External_regulation_and_reputation               26.804124   \n",
      "8                                 Public_opinion                5.670103   \n",
      "9                          Fairness_and_equality               27.577320   \n",
      "10                        Capacity_and_resources                6.443299   \n",
      "11                               Quality_of_life               20.360825   \n",
      "12                             Cultural_identity                6.958763   \n",
      "13                             Health_and_safety               14.175258   \n",
      "\n",
      "    Test Distribution (%)  \n",
      "0               36.363636  \n",
      "1               47.727273  \n",
      "2               18.181818  \n",
      "3               50.000000  \n",
      "4                9.090909  \n",
      "5               52.272727  \n",
      "6               45.454545  \n",
      "7               36.363636  \n",
      "8                2.272727  \n",
      "9               15.909091  \n",
      "10               9.090909  \n",
      "11              18.181818  \n",
      "12               9.090909  \n",
      "13              13.636364  \n",
      "CREATION DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 32\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset, test_dataset, train_dataloader, test_dataloader = get_datasets_dataloaders(X, y, tokenizer, recalculate_srl=False, batch_size=batch_size, max_sentences_per_article=num_sentences, max_sentence_length=64, max_arg_length=12, pickle_path=\"notebooks/X_srl_full.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted_labels, used_labels_p, used_labels_a0, used_labels_a1 = inspect(model, train_dataloader, y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(used_labels_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(y.columns)\n",
    "\n",
    "category_lists_p = {category: [] for category in categories}\n",
    "category_lists_a1 = {category: [] for category in categories}\n",
    "category_lists_a0 = {category: [] for category in categories}\n",
    "\n",
    "loader = test_dataloader\n",
    "\n",
    "for batch_idx in range(len(loader.dataset)):\n",
    "    # Iterate over each sentence\n",
    "    ds = loader.dataset[batch_idx]\n",
    "\n",
    "    for sentence_idx in range(len(used_labels_p[batch_idx])):\n",
    "\n",
    "        # Update the lists for each category\n",
    "        for cat_idx, category in enumerate(categories):\n",
    "            \n",
    "            if used_labels_p[batch_idx][cat_idx][0][cat_idx] > 0.8:\n",
    "                category_lists_p[category].append(ds[\"predicate_ids\"][sentence_idx].numpy())\n",
    "            \n",
    "            if used_labels_a0[batch_idx][cat_idx][0][cat_idx] > 0.8:\n",
    "                category_lists_a0[category].append(ds[\"arg0_ids\"][sentence_idx].numpy())\n",
    "                \n",
    "            if used_labels_a1[batch_idx][cat_idx][0][cat_idx] > 0.8:\n",
    "                category_lists_a1[category].append(ds[\"arg1_ids\"][sentence_idx].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def decode_tokens(token_dict, stop_words):\n",
    "    decoded_data = {}\n",
    "    for category, token_lists in token_dict.items():\n",
    "        decoded_data[category] = []\n",
    "        for tokens in token_lists:\n",
    "            if np.any(tokens > 0):\n",
    "                # Decode the tokens\n",
    "                decoded_text = tokenizer.decode(tokens, skip_special_tokens=True).strip()\n",
    "                # Tokenize and remove stop words\n",
    "                words = word_tokenize(decoded_text)\n",
    "                filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "                # Join the words back into a string\n",
    "                decoded_data[category].append(' '.join(filtered_words))\n",
    "    return decoded_data\n",
    "\n",
    "stop_words = set(stopwords.words('english'))  # Assuming your text is in English\n",
    "\n",
    "# Decode the token IDs for each ARG\n",
    "decoded_predicate = decode_tokens(category_lists_p, stop_words)\n",
    "decoded_arg0 = decode_tokens(category_lists_a0, stop_words)\n",
    "decoded_arg1 = decode_tokens(category_lists_a1, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66/3321433862.py:26: FutureWarning: this method is deprecated in favour of `Styler.hide(axis=\"index\")`\n",
      "  df_full_table.style.hide_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_cc394\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_cc394_level0_col0\" class=\"col_heading level0 col0\" >Frame</th>\n",
       "      <th id=\"T_cc394_level0_col1\" class=\"col_heading level0 col1\" >Predicate</th>\n",
       "      <th id=\"T_cc394_level0_col2\" class=\"col_heading level0 col2\" >ARG0</th>\n",
       "      <th id=\"T_cc394_level0_col3\" class=\"col_heading level0 col3\" >ARG1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_cc394_row0_col0\" class=\"data row0 col0\" >Cultural_identity</td>\n",
       "      <td id=\"T_cc394_row0_col1\" class=\"data row0 col1\" >, grants, prohibit, keep, used, back, stop, reports, occur, asked, breaking, bite, safeguard, agree, sending, said, opt</td>\n",
       "      <td id=\"T_cc394_row0_col2\" class=\"data row0 col2\" >, individual, author, biblical illiterates, creator, god, picture inside seal, john hancock , first signer declaration, american history found document, speaker -, person, preachers, completing poll, caesar, whosoever</td>\n",
       "      <td id=\"T_cc394_row0_col3\" class=\"data row0 col3\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cc394_row1_col0\" class=\"data row1 col0\" >Public_opinion</td>\n",
       "      <td id=\"T_cc394_row1_col1\" class=\"data row1 col1\" ></td>\n",
       "      <td id=\"T_cc394_row1_col2\" class=\"data row1 col2\" >us, american government employee stationed southern city, mike pompeo , us secretary state, </td>\n",
       "      <td id=\"T_cc394_row1_col3\" class=\"data row1 col3\" >“ would raise serious questions whether department, matter “ handled properly ”, justice department release special counsel robert mueller ’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cc394_row2_col0\" class=\"data row2 col0\" >Policy_prescription_and_evaluation</td>\n",
       "      <td id=\"T_cc394_row2_col1\" class=\"data row2 col1\" ></td>\n",
       "      <td id=\"T_cc394_row2_col2\" class=\"data row2 col2\" >, nominee, ramirez, russians , chinese , iranians ,, cia ’ front corporations narcotics business, ambellas intellihub, laura loomer , noxious far -, haig 's attorney marc victor, clinton regime, gop, ’, people, haig, [ senate majority leader ] mitch mcconnell chuck grass, federalist society, senate judiciary committee chairman chuck grassley ( r -, search warrant records, house democratic leader nancy pelosi ( ca ), paddock, trump, american public, two women, ford, waste - filled us military / security complex, one western media source, us democratic party, president eisenhower, intellihub, douglas haig , 55 - year - old, democratic senator chuck schumer ( ny ), high court, insane neoconservatives ,, sum, president trump, bonkers conspiracy site intellihub, cia cut -, corrupt filth, new yorker magazine, douglas haig ’ @ linkedin account, attorneys, washington insiders, presstitute media, ambellas, leaders democratic party , nancy pe, george soros ’ money, customer, democrats, haig 's linkedin profile, democratic minority leader us house representatives, media - savvy sleazeball lawyer, mesa man, left</td>\n",
       "      <td id=\"T_cc394_row2_col3\" class=\"data row2 col3\" >, man ’ actions, incident, access freedom outpost updates free charge, hat, poll - story, employer, questions concerns, jimenez cut party, entire event, meal, brett kavanaugh, america, jimenez worked “ part - time door, teenager wearing # maga hat, different opinions president, site 's privacy policy terms, aggression, bar, ugly head, altercation, boy ’ hat, field, since terminated employee , actions, rumble, business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cc394_row3_col0\" class=\"data row3 col0\" >Crime_and_punishment</td>\n",
       "      <td id=\"T_cc394_row3_col1\" class=\"data row3 col1\" >, mean, resumed, attack, preferred, run, launched, understand, ate, overthrew, need, vaporized, include, suspended, accelerate, take, ', called, travel, gotten, act, boasts, opt, shows, spending, uses, using, resumes, avoided, spread, moved, grants, made, means, add, relishing, fired, cajole, signed, talking, agree, threaten, mastered, deployed, skimped</td>\n",
       "      <td id=\"T_cc394_row3_col2\" class=\"data row3 col2\" >, us south korea, pakistan ‘, us power, washington, administration ’ amateur foreign policymakers, north korea test, north koreans , eccentric, us military, president trump, israel, north korea, nuclear explosions, shatter japan cripple south korea, north korea ’ kim jong - un, “</td>\n",
       "      <td id=\"T_cc394_row3_col3\" class=\"data row3 col3\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cc394_row4_col0\" class=\"data row4 col0\" >Fairness_and_equality</td>\n",
       "      <td id=\"T_cc394_row4_col1\" class=\"data row4 col1\" ></td>\n",
       "      <td id=\"T_cc394_row4_col2\" class=\"data row4 col2\" >, intellihub, douglas haig , 55 - year - old, ambellas, bonkers conspiracy site intellihub, paddock, cia cut -, ambellas intellihub, customer, laura loomer , noxious far -, haig 's attorney marc victor, haig 's linkedin profile, ’, mesa man, people, haig, douglas haig ’ @ linkedin account, search warrant records</td>\n",
       "      <td id=\"T_cc394_row4_col3\" class=\"data row4 col3\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cc394_row5_col0\" class=\"data row5 col0\" >Economic</td>\n",
       "      <td id=\"T_cc394_row5_col1\" class=\"data row5 col1\" >, resumed, need, suspended, ', travel, gotten, act, opt, shows, spending, using, resumes, grants, means, signed, agree, made, deployed</td>\n",
       "      <td id=\"T_cc394_row5_col2\" class=\"data row5 col2\" >, two, clinton, us, jim acosta, completing poll, trump, order, us forces, military, swisher, others, military force, hillary clinton, migrant caravan, president trump, caravan , made mostly male hondur, ruptly video, president donald trump, migrants, former secretary state hillary clinton, hillary, democratic party</td>\n",
       "      <td id=\"T_cc394_row5_col3\" class=\"data row5 col3\" >, image, attempt coup president, 1 , 000 billion dollar budget military, pelosi ’ false accusation, multiple shooters, trump ’ comments, trump putin, new information, junk, us, countless hours, inside @ mandalaybay valet center, donald trump, many unanswered questions dr, license plate numbers given police, large 1 , 000 billion, - called evidence, way, representation , lies justify war conflict, first step president trump taken reduce, entire narrative, notice extremely hostile reaction peace, nothing, fear undefined retribution, effect, series bizarre interviews led even, isis connection, president trump, russia, handwritten note reportedly written, definitely footage, stephenpaddock ‘ van, world, las vegas shooter ’ hotel check -, donations - election campaigns, fbi , along state local police, many, western media opposed peace, peace russia, explanation, recipients, outcry blatantly false, democratic party, public, cia bought - - paid -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cc394_row6_col0\" class=\"data row6 col0\" >Capacity_and_resources</td>\n",
       "      <td id=\"T_cc394_row6_col1\" class=\"data row6 col1\" >, transmitted, highlights, gather, states, affects, confirmed, work</td>\n",
       "      <td id=\"T_cc394_row6_col2\" class=\"data row6 col2\" >, nominee, ramirez, gop, people, [ senate majority leader ] mitch mcconnell chuck grass, federalist society, senate judiciary committee chairman chuck grassley ( r -, two women, ford, high court, president trump, new yorker magazine, attorneys, washington insiders, george soros ’ money, democrats, media - savvy sleazeball lawyer, left</td>\n",
       "      <td id=\"T_cc394_row6_col3\" class=\"data row6 col3\" >, group migrants, three lanes shut san, reinforcing positions, us, photos, three nb vehicle lanes, initial smaller group around 85 people , mostly, main ‘ caravan ’ thousands, press pass, take pacific coast route reach us, drones , helicopters night - vision capabilities ,, would close multiple entry lanes u ., barriers razor wire, seven us army members cbp, like child, lane closures san ysidro, site 's privacy policy terms, barricades razor wire, access truth uncensored updates free charge, san diego, 5 , 200 troops, put razor wire border san, field, concertina wire , pre - positioning jersey barriers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cc394_row7_col0\" class=\"data row7 col0\" >Health_and_safety</td>\n",
       "      <td id=\"T_cc394_row7_col1\" class=\"data row7 col1\" >apprehended, indicated, processed, patrolling</td>\n",
       "      <td id=\"T_cc394_row7_col2\" class=\"data row7 col2\" ></td>\n",
       "      <td id=\"T_cc394_row7_col3\" class=\"data row7 col3\" >, plague, one major causes spread, infection, whoever handles body, whole plague kind government conspiracy, country ’ cash - strapped government, least 15 famadihana ceremonies, turning bones ancestors — plague, plague lie, least 124 people madagascar, ’ coincidence outbreak coincides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cc394_row8_col0\" class=\"data row8 col0\" >External_regulation_and_reputation</td>\n",
       "      <td id=\"T_cc394_row8_col1\" class=\"data row8 col1\" ></td>\n",
       "      <td id=\"T_cc394_row8_col2\" class=\"data row8 col2\" >, president, president trump ’ policies, former speaker house , newt gingrich, conversations, president trump, people, completing poll</td>\n",
       "      <td id=\"T_cc394_row8_col3\" class=\"data row8 col3\" >, defy feds release mexican, today day vindication rights, garcia zarate ’ background nationality played, anybody get away violating u., case, detain deportation, prosecution defense got outcome wanted, sentence 16 months 3 years, sanctuary state california , false -, mexican national guilty murder mans, sig sauer . 40 caliber pistol ,, felon, evidence california, mate, investigation special prosecutor, verdict, people country, rights american legal system ,, jose ines garcia zarate , also known, ’ shocked — saddened shocked, career criminal even supposed, sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cc394_row9_col0\" class=\"data row9 col0\" >Morality</td>\n",
       "      <td id=\"T_cc394_row9_col1\" class=\"data row9 col1\" ></td>\n",
       "      <td id=\"T_cc394_row9_col2\" class=\"data row9 col2\" >lawmakers, democrats six house committees — including judiciary chairman jerry, giuliani</td>\n",
       "      <td id=\"T_cc394_row9_col3\" class=\"data row9 col3\" >, guns & ammo confiscation &, people safe, armor, access freedom outpost updates free charge, pull dick 's sporting goods demand, policy ama support : establishing laws allowing, magic age 21, drums hold 50 - 100 rounds, 18 - year - olds carry firearms ,, military, exactly mean high - capacity magazines, domestic violence, rights n't, protocols processes, government get determine gets, nothing medicine health, sorts government overreach unconstitutional, site 's privacy policy terms, political position, corrupt democrat politician, teachers students arrested tried dealt, huge laundry list things ama supports, field, 20 30 round magazines handguns, bypass fifth amendment protections concerning someone actually</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cc394_row10_col0\" class=\"data row10 col0\" >Security_and_defense</td>\n",
       "      <td id=\"T_cc394_row10_col1\" class=\"data row10 col1\" >put, , grants, writes, used, keep, stop, riding, released, putting, take, reported, encouraging, said, agree, need, opt, supports</td>\n",
       "      <td id=\"T_cc394_row10_col2\" class=\"data row10 col2\" >, 2 jihadi, law enforcement, completing poll, islam, fbi local law enforcement</td>\n",
       "      <td id=\"T_cc394_row10_col3\" class=\"data row10 col3\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cc394_row11_col0\" class=\"data row11 col0\" >Quality_of_life</td>\n",
       "      <td id=\"T_cc394_row11_col1\" class=\"data row11 col1\" >, took, changing, ’, commit, need, reduces, provide, recognize, inform, promoted, contend, employs, confirms, managed, say, reduce, know, means, go, characterized, hoping, condemned, expect, worsen</td>\n",
       "      <td id=\"T_cc394_row11_col2\" class=\"data row11 col2\" >, fbi, another tweet loomer, alternative media, russians , chinese , iranians ,, cia ’ front corporations narcotics business, clinton regime, authorities, house democratic leader nancy pelosi ( ca ), investigative reporter laura loomer, trump, american public, waste - filled us military / security complex, one western media source, us democratic party, president eisenhower, photographic evidence inside hotel parking garage, democratic senator chuck schumer ( ny ), least four videos scene shooting, insane neoconservatives ,, sum, corrupt filth, paddock ’ brother eric, presstitute media, photo, leaders democratic party , nancy pe, officials, video footage, law enforcement @ fbi, democratic minority leader us house representatives, @ fbi</td>\n",
       "      <td id=\"T_cc394_row11_col3\" class=\"data row11 col3\" >, caravan migrants marching toward u., black people, diversity, look, access freedom outpost updates free charge, paint broad brush every immigrant, senator cory booker former attorney general eric holder, military, eric holder clinton corrected , adding , “, think cory booker … saying ‘, move speak freely country, propaganda conservative political opponents mean ,, military force, ’ politically correct say value, thing, cory booker & eric holder, go around insulting people, ’ often called political correctness polite, people african - american , latino , lgbt, booker holder , every person dark, site 's privacy policy terms, conservative person uttered comments, near lynching spot, field, livelihood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cc394_row12_col0\" class=\"data row12 col0\" >Political</td>\n",
       "      <td id=\"T_cc394_row12_col1\" class=\"data row12 col1\" >, took, understand, keep, unmasked, thank, hope, said, think, done, believe, change, gaining, say, makes, posting, reads, delivered, provides, continue</td>\n",
       "      <td id=\"T_cc394_row12_col2\" class=\"data row12 col2\" ></td>\n",
       "      <td id=\"T_cc394_row12_col3\" class=\"data row12 col3\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cc394_row13_col0\" class=\"data row13 col0\" >Legality_Constitutionality_and_jurisprudence</td>\n",
       "      <td id=\"T_cc394_row13_col1\" class=\"data row13 col1\" ></td>\n",
       "      <td id=\"T_cc394_row13_col2\" class=\"data row13 col2\" >, investigators, agents, fbi, another tweet loomer, alternative media, daily mail, u. s. army, numerous news outlets, chatter referencing paper, doj officials, authorities, wolf blitzer, investigative reporter laura loomer, muslim named hafiz kazi, everyone internet, times, ragan, willful ignorance, gloria borger, photographic evidence inside hotel parking garage, least four videos scene shooting, media, paddock ’ brother eric, sean ragan , fbi special agent charge, police, photo, officers, video footage, officials, 800 - pound gorilla, law enforcement @ fbi, special counsel robert mueller, nevada sheriff joe lombardo, cnn ’ shimon prokupecz, las vegas shooter, panel member, @ fbi</td>\n",
       "      <td id=\"T_cc394_row13_col3\" class=\"data row13 col3\" ></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f8e59341730>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a list to collect DataFrame rows\n",
    "rows = []\n",
    "\n",
    "# Populate the list with rows\n",
    "for frame in set(decoded_predicate) | set(decoded_arg0) | set(decoded_arg1):\n",
    "    # Get the lists, joining multiple words with a comma\n",
    "    pred_words = ', '.join([ s.strip() for s in list(set(decoded_predicate.get(frame, []))) if s is not None or l != \"\"])\n",
    "    arg0_words = ', '.join(list(set(decoded_arg0.get(frame, []))))\n",
    "    arg1_words = ', '.join(list(set(decoded_arg1.get(frame, []))))\n",
    "\n",
    "    # Create a dictionary for the row\n",
    "    row = {\n",
    "        \"Frame\": frame,\n",
    "        \"Predicate\": pred_words,\n",
    "        \"ARG0\": arg0_words,\n",
    "        \"ARG1\": arg1_words\n",
    "    }\n",
    "    \n",
    "    # Append the row dictionary to the rows list\n",
    "    rows.append(row)\n",
    "\n",
    "# Convert the list of rows to a DataFrame\n",
    "df_full_table = pd.DataFrame(rows)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_full_table.style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05c4f9fa65704ea5ba7f80e879e08510": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f54181f857a4110bf9976a56c97de97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fbfe002ccd541f9b6ab62ed4eebef35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1048f064e69b46d497b65cb9b88d0142": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc871ed94e764fe884ffdafca1445bfe",
       "IPY_MODEL_b4e7e77911e5408b84d89ccaff134708",
       "IPY_MODEL_c322e99ad9504207b7cfb9ae2ca9d6b1"
      ],
      "layout": "IPY_MODEL_dd92db5892be4357a6446146d9e9a0dd"
     }
    },
    "145c8616bab245358c8052beac5bd2bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20d6b378a69744e4a327d929c391b475": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "364a4257f8a641e18b7bde39e63a618d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "409a73804fc34a63a619a42b9468be46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "475020323c07410e87d366a6f553aafb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_145c8616bab245358c8052beac5bd2bc",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca901348dcd1428ebecc55d659fa40d1",
      "value": 28
     }
    },
    "4ddbb3fa3b924a9e9650f51204580f8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a5458551f974a94b61ff658ae59fa70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b48b8b79d6a4ff1b0655a6bfcf7a422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c06d626b5ee4fe8bdb9be7fb879aae0",
      "placeholder": "​",
      "style": "IPY_MODEL_779202125255413cb719a43fe5d14ce7",
      "value": "Downloading (…)solve/main/vocab.txt: 100%"
     }
    },
    "5d40c6e034fd4b278969dd928cc07dff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f4b55ead6884f3790a3acfa43232fc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87907508c4eb43f3a27858f1d397ca99",
      "placeholder": "​",
      "style": "IPY_MODEL_d21ed2e527464a0ca0cb2b3b4fad928a",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "650f8e0f82e849f58a47507291b15500": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbb672669b9f4c36873048cfa1d2daf6",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cf42b39e3be94c90b3a63d05783481a3",
      "value": 231508
     }
    },
    "67ea6145091b440da6b4c5d5f8695aa9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ac495a816994aaeb8926c6c97d103ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ddbb3fa3b924a9e9650f51204580f8d",
      "placeholder": "​",
      "style": "IPY_MODEL_a115ed03534a4c64a17fa55d93a0f93a",
      "value": " 570/570 [00:00&lt;00:00, 47.0kB/s]"
     }
    },
    "71add6bf108545af8db6aeb392793759": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5f4b55ead6884f3790a3acfa43232fc9",
       "IPY_MODEL_475020323c07410e87d366a6f553aafb",
       "IPY_MODEL_e2454ec2f5904e0bacde56ae7d4089a9"
      ],
      "layout": "IPY_MODEL_0f54181f857a4110bf9976a56c97de97"
     }
    },
    "779202125255413cb719a43fe5d14ce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87907508c4eb43f3a27858f1d397ca99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c06d626b5ee4fe8bdb9be7fb879aae0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e13dccd922946ab873cc287fcca5baf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93fa55d130db463c8ddffc947f2e99cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a6d16261d5b4693b09acf83dcb38920": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0265a21ae504485ab59207228c1f6d6",
      "placeholder": "​",
      "style": "IPY_MODEL_8e13dccd922946ab873cc287fcca5baf",
      "value": " 232k/232k [00:00&lt;00:00, 9.51MB/s]"
     }
    },
    "a0265a21ae504485ab59207228c1f6d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a115ed03534a4c64a17fa55d93a0f93a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a74bacc051494d1ea0f4cbd656505cfe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4e7e77911e5408b84d89ccaff134708": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67ea6145091b440da6b4c5d5f8695aa9",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bef37b47c8ee49778464c05adcffa30d",
      "value": 466062
     }
    },
    "b83493088cd24629a04ec612aa522ed4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c222d2a5e4664f9b9aefed7f093d4a4e",
       "IPY_MODEL_fa94f90b735f418fb2b502f7877b5306",
       "IPY_MODEL_6ac495a816994aaeb8926c6c97d103ae"
      ],
      "layout": "IPY_MODEL_409a73804fc34a63a619a42b9468be46"
     }
    },
    "bef37b47c8ee49778464c05adcffa30d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c222d2a5e4664f9b9aefed7f093d4a4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_364a4257f8a641e18b7bde39e63a618d",
      "placeholder": "​",
      "style": "IPY_MODEL_93fa55d130db463c8ddffc947f2e99cd",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "c322e99ad9504207b7cfb9ae2ca9d6b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d40c6e034fd4b278969dd928cc07dff",
      "placeholder": "​",
      "style": "IPY_MODEL_f6b3c67d61114db5810bd78339a218d9",
      "value": " 466k/466k [00:00&lt;00:00, 1.88MB/s]"
     }
    },
    "ca901348dcd1428ebecc55d659fa40d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cf42b39e3be94c90b3a63d05783481a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d21ed2e527464a0ca0cb2b3b4fad928a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d568d9b894694b6fb432456031cee230": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbb672669b9f4c36873048cfa1d2daf6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd92db5892be4357a6446146d9e9a0dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2454ec2f5904e0bacde56ae7d4089a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20d6b378a69744e4a327d929c391b475",
      "placeholder": "​",
      "style": "IPY_MODEL_05c4f9fa65704ea5ba7f80e879e08510",
      "value": " 28.0/28.0 [00:00&lt;00:00, 1.97kB/s]"
     }
    },
    "e9ee9acc5e414c65ad10b8864cc07b07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f19c187b1e8c4bbe9fda366f24f2b79e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5b48b8b79d6a4ff1b0655a6bfcf7a422",
       "IPY_MODEL_650f8e0f82e849f58a47507291b15500",
       "IPY_MODEL_9a6d16261d5b4693b09acf83dcb38920"
      ],
      "layout": "IPY_MODEL_d568d9b894694b6fb432456031cee230"
     }
    },
    "f6b3c67d61114db5810bd78339a218d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa94f90b735f418fb2b502f7877b5306": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a74bacc051494d1ea0f4cbd656505cfe",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0fbfe002ccd541f9b6ab62ed4eebef35",
      "value": 570
     }
    },
    "fc871ed94e764fe884ffdafca1445bfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a5458551f974a94b61ff658ae59fa70",
      "placeholder": "​",
      "style": "IPY_MODEL_e9ee9acc5e414c65ad10b8864cc07b07",
      "value": "Downloading (…)/main/tokenizer.json: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
