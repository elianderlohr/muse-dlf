{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Waumhn_ldoGu"
   },
   "source": [
    "## Classifier - Try 8\n",
    "\n",
    "Classify articles frames using aggregated SRL and sentence embeddings.\n",
    "\n",
    "1. Try multi attention header for better identifying how each sentence corresponds to the full article frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21919,
     "status": "ok",
     "timestamp": 1696623996213,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "LM7e9jI4doGx",
    "outputId": "602ef0fa-e5fd-4626-be99-3fdef780fd4c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "  os.chdir('drive/MyDrive/Git/MasterThesis/data')\n",
    "else:\n",
    "  os.chdir('../data/')\n",
    "\n",
    "labels_path = \"data/en/train-labels-subtask-2.txt\"\n",
    "articles_path = \"data/en/train-articles-subtask-2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1296
    },
    "executionInfo": {
     "elapsed": 3772,
     "status": "ok",
     "timestamp": 1696624002536,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "DG_Xix7gdoGy",
    "outputId": "d6fad26e-e6f7-4c20-f4bb-c7b01d51eb33",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>frames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>832959523</td>\n",
       "      <td>Morality,Security_and_defense,Policy_prescript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>833039623</td>\n",
       "      <td>Political,Crime_and_punishment,External_regula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>833032367</td>\n",
       "      <td>Political,Crime_and_punishment,Fairness_and_eq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>814777937</td>\n",
       "      <td>Political,Morality,Fairness_and_equality,Exter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>821744708</td>\n",
       "      <td>Policy_prescription_and_evaluation,Political,L...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                             frames\n",
       "0   832959523  Morality,Security_and_defense,Policy_prescript...\n",
       "1   833039623  Political,Crime_and_punishment,External_regula...\n",
       "2   833032367  Political,Crime_and_punishment,Fairness_and_eq...\n",
       "3   814777937  Political,Morality,Fairness_and_equality,Exter...\n",
       "4   821744708  Policy_prescription_and_evaluation,Political,L..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dev-labels-subtask-2.txt file\n",
    "labels_df = pd.read_csv(labels_path, sep=\"\\t\")\n",
    "\n",
    "# Rename the columns for easier processing\n",
    "labels_df.columns = [\"article_id\", \"frames\"]\n",
    "\n",
    "\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 359378,
     "status": "ok",
     "timestamp": 1696624361911,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "wYeJBUyAdoGz",
    "outputId": "7bd17fad-1fda-4be9-8c66-a85f51e5f517",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>frames</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>832959523</td>\n",
       "      <td>Morality,Security_and_defense,Policy_prescript...</td>\n",
       "      <td>How Theresa May Botched\\n\\nThose were the time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>833039623</td>\n",
       "      <td>Political,Crime_and_punishment,External_regula...</td>\n",
       "      <td>Robert Mueller III Rests His Case—Dems NEVER W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>833032367</td>\n",
       "      <td>Political,Crime_and_punishment,Fairness_and_eq...</td>\n",
       "      <td>Robert Mueller Not Recommending Any More Indic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>814777937</td>\n",
       "      <td>Political,Morality,Fairness_and_equality,Exter...</td>\n",
       "      <td>The Far Right Is Trying to Co-opt the Yellow V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>821744708</td>\n",
       "      <td>Policy_prescription_and_evaluation,Political,L...</td>\n",
       "      <td>‘Special place in hell’ for those who promoted...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                             frames  \\\n",
       "0   832959523  Morality,Security_and_defense,Policy_prescript...   \n",
       "1   833039623  Political,Crime_and_punishment,External_regula...   \n",
       "2   833032367  Political,Crime_and_punishment,Fairness_and_eq...   \n",
       "3   814777937  Political,Morality,Fairness_and_equality,Exter...   \n",
       "4   821744708  Policy_prescription_and_evaluation,Political,L...   \n",
       "\n",
       "                                             content  \n",
       "0  How Theresa May Botched\\n\\nThose were the time...  \n",
       "1  Robert Mueller III Rests His Case—Dems NEVER W...  \n",
       "2  Robert Mueller Not Recommending Any More Indic...  \n",
       "3  The Far Right Is Trying to Co-opt the Yellow V...  \n",
       "4  ‘Special place in hell’ for those who promoted...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A function to read the article text given its ID\n",
    "def get_article_content(article_id):\n",
    "    try:\n",
    "        with open(f\"{articles_path}/article{article_id}.txt\", \"r\") as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "df = labels_df\n",
    "\n",
    "# Apply the function to get the article content\n",
    "df[\"content\"] = df[\"article_id\"].apply(get_article_content)\n",
    "\n",
    "# Drop rows where content could not be found\n",
    "df.dropna(subset=[\"content\"], inplace=True)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1696624361912,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "fU66l4twdoGz",
    "outputId": "48f06d3d-e677-4d16-dadb-aae312d25342",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>frames</th>\n",
       "      <th>content</th>\n",
       "      <th>frames_list</th>\n",
       "      <th>Morality</th>\n",
       "      <th>Security_and_defense</th>\n",
       "      <th>Policy_prescription_and_evaluation</th>\n",
       "      <th>Legality_Constitutionality_and_jurisprudence</th>\n",
       "      <th>Economic</th>\n",
       "      <th>Political</th>\n",
       "      <th>Crime_and_punishment</th>\n",
       "      <th>External_regulation_and_reputation</th>\n",
       "      <th>Public_opinion</th>\n",
       "      <th>Fairness_and_equality</th>\n",
       "      <th>Capacity_and_resources</th>\n",
       "      <th>Quality_of_life</th>\n",
       "      <th>Cultural_identity</th>\n",
       "      <th>Health_and_safety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>832959523</td>\n",
       "      <td>Morality,Security_and_defense,Policy_prescript...</td>\n",
       "      <td>How Theresa May Botched\\n\\nThose were the time...</td>\n",
       "      <td>[Morality, Security_and_defense, Policy_prescr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>833039623</td>\n",
       "      <td>Political,Crime_and_punishment,External_regula...</td>\n",
       "      <td>Robert Mueller III Rests His Case—Dems NEVER W...</td>\n",
       "      <td>[Political, Crime_and_punishment, External_reg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>833032367</td>\n",
       "      <td>Political,Crime_and_punishment,Fairness_and_eq...</td>\n",
       "      <td>Robert Mueller Not Recommending Any More Indic...</td>\n",
       "      <td>[Political, Crime_and_punishment, Fairness_and...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>814777937</td>\n",
       "      <td>Political,Morality,Fairness_and_equality,Exter...</td>\n",
       "      <td>The Far Right Is Trying to Co-opt the Yellow V...</td>\n",
       "      <td>[Political, Morality, Fairness_and_equality, E...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>821744708</td>\n",
       "      <td>Policy_prescription_and_evaluation,Political,L...</td>\n",
       "      <td>‘Special place in hell’ for those who promoted...</td>\n",
       "      <td>[Policy_prescription_and_evaluation, Political...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                             frames  \\\n",
       "0   832959523  Morality,Security_and_defense,Policy_prescript...   \n",
       "1   833039623  Political,Crime_and_punishment,External_regula...   \n",
       "2   833032367  Political,Crime_and_punishment,Fairness_and_eq...   \n",
       "3   814777937  Political,Morality,Fairness_and_equality,Exter...   \n",
       "4   821744708  Policy_prescription_and_evaluation,Political,L...   \n",
       "\n",
       "                                             content  \\\n",
       "0  How Theresa May Botched\\n\\nThose were the time...   \n",
       "1  Robert Mueller III Rests His Case—Dems NEVER W...   \n",
       "2  Robert Mueller Not Recommending Any More Indic...   \n",
       "3  The Far Right Is Trying to Co-opt the Yellow V...   \n",
       "4  ‘Special place in hell’ for those who promoted...   \n",
       "\n",
       "                                         frames_list  Morality  \\\n",
       "0  [Morality, Security_and_defense, Policy_prescr...         1   \n",
       "1  [Political, Crime_and_punishment, External_reg...         0   \n",
       "2  [Political, Crime_and_punishment, Fairness_and...         0   \n",
       "3  [Political, Morality, Fairness_and_equality, E...         1   \n",
       "4  [Policy_prescription_and_evaluation, Political...         0   \n",
       "\n",
       "   Security_and_defense  Policy_prescription_and_evaluation  \\\n",
       "0                     1                                   1   \n",
       "1                     0                                   1   \n",
       "2                     0                                   0   \n",
       "3                     1                                   0   \n",
       "4                     0                                   1   \n",
       "\n",
       "   Legality_Constitutionality_and_jurisprudence  Economic  Political  \\\n",
       "0                                             1         1          0   \n",
       "1                                             1         0          1   \n",
       "2                                             1         0          1   \n",
       "3                                             0         1          1   \n",
       "4                                             1         0          1   \n",
       "\n",
       "   Crime_and_punishment  External_regulation_and_reputation  Public_opinion  \\\n",
       "0                     0                                   0               0   \n",
       "1                     1                                   1               1   \n",
       "2                     1                                   1               0   \n",
       "3                     0                                   1               1   \n",
       "4                     0                                   1               0   \n",
       "\n",
       "   Fairness_and_equality  Capacity_and_resources  Quality_of_life  \\\n",
       "0                      0                       0                0   \n",
       "1                      0                       0                0   \n",
       "2                      1                       0                0   \n",
       "3                      1                       0                0   \n",
       "4                      0                       0                0   \n",
       "\n",
       "   Cultural_identity  Health_and_safety  \n",
       "0                  0                  0  \n",
       "1                  0                  0  \n",
       "2                  0                  0  \n",
       "3                  0                  0  \n",
       "4                  0                  0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the frames column into a list of frames\n",
    "df[\"frames_list\"] = df[\"frames\"].str.split(\",\")\n",
    "\n",
    "# create for each frame a new column with the frame as name and 1 if the frame is present in the article and 0 if not\n",
    "for frame in df[\"frames_list\"].explode().unique():\n",
    "    df[frame] = df[\"frames_list\"].apply(lambda x: 1 if frame in x else 0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>frames</th>\n",
       "      <th>content</th>\n",
       "      <th>frames_list</th>\n",
       "      <th>Morality</th>\n",
       "      <th>Security_and_defense</th>\n",
       "      <th>Policy_prescription_and_evaluation</th>\n",
       "      <th>Legality_Constitutionality_and_jurisprudence</th>\n",
       "      <th>Economic</th>\n",
       "      <th>Political</th>\n",
       "      <th>Crime_and_punishment</th>\n",
       "      <th>External_regulation_and_reputation</th>\n",
       "      <th>Public_opinion</th>\n",
       "      <th>Fairness_and_equality</th>\n",
       "      <th>Capacity_and_resources</th>\n",
       "      <th>Quality_of_life</th>\n",
       "      <th>Cultural_identity</th>\n",
       "      <th>Health_and_safety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>833039623</td>\n",
       "      <td>Political,Crime_and_punishment,External_regula...</td>\n",
       "      <td>Robert Mueller III Rests His Case—Dems NEVER W...</td>\n",
       "      <td>[Political, Crime_and_punishment, External_reg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>833032367</td>\n",
       "      <td>Political,Crime_and_punishment,Fairness_and_eq...</td>\n",
       "      <td>Robert Mueller Not Recommending Any More Indic...</td>\n",
       "      <td>[Political, Crime_and_punishment, Fairness_and...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>814777937</td>\n",
       "      <td>Political,Morality,Fairness_and_equality,Exter...</td>\n",
       "      <td>The Far Right Is Trying to Co-opt the Yellow V...</td>\n",
       "      <td>[Political, Morality, Fairness_and_equality, E...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>821744708</td>\n",
       "      <td>Policy_prescription_and_evaluation,Political,L...</td>\n",
       "      <td>‘Special place in hell’ for those who promoted...</td>\n",
       "      <td>[Policy_prescription_and_evaluation, Political...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>833036489</td>\n",
       "      <td>Political,External_regulation_and_reputation,P...</td>\n",
       "      <td>Bill Maher says he doesn't need Mueller report...</td>\n",
       "      <td>[Political, External_regulation_and_reputation...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                             frames  \\\n",
       "1   833039623  Political,Crime_and_punishment,External_regula...   \n",
       "2   833032367  Political,Crime_and_punishment,Fairness_and_eq...   \n",
       "3   814777937  Political,Morality,Fairness_and_equality,Exter...   \n",
       "4   821744708  Policy_prescription_and_evaluation,Political,L...   \n",
       "5   833036489  Political,External_regulation_and_reputation,P...   \n",
       "\n",
       "                                             content  \\\n",
       "1  Robert Mueller III Rests His Case—Dems NEVER W...   \n",
       "2  Robert Mueller Not Recommending Any More Indic...   \n",
       "3  The Far Right Is Trying to Co-opt the Yellow V...   \n",
       "4  ‘Special place in hell’ for those who promoted...   \n",
       "5  Bill Maher says he doesn't need Mueller report...   \n",
       "\n",
       "                                         frames_list  Morality  \\\n",
       "1  [Political, Crime_and_punishment, External_reg...         0   \n",
       "2  [Political, Crime_and_punishment, Fairness_and...         0   \n",
       "3  [Political, Morality, Fairness_and_equality, E...         1   \n",
       "4  [Policy_prescription_and_evaluation, Political...         0   \n",
       "5  [Political, External_regulation_and_reputation...         0   \n",
       "\n",
       "   Security_and_defense  Policy_prescription_and_evaluation  \\\n",
       "1                     0                                   1   \n",
       "2                     0                                   0   \n",
       "3                     1                                   0   \n",
       "4                     0                                   1   \n",
       "5                     0                                   1   \n",
       "\n",
       "   Legality_Constitutionality_and_jurisprudence  Economic  Political  \\\n",
       "1                                             1         0          1   \n",
       "2                                             1         0          1   \n",
       "3                                             0         1          1   \n",
       "4                                             1         0          1   \n",
       "5                                             1         0          1   \n",
       "\n",
       "   Crime_and_punishment  External_regulation_and_reputation  Public_opinion  \\\n",
       "1                     1                                   1               1   \n",
       "2                     1                                   1               0   \n",
       "3                     0                                   1               1   \n",
       "4                     0                                   1               0   \n",
       "5                     0                                   1               1   \n",
       "\n",
       "   Fairness_and_equality  Capacity_and_resources  Quality_of_life  \\\n",
       "1                      0                       0                0   \n",
       "2                      1                       0                0   \n",
       "3                      1                       0                0   \n",
       "4                      0                       0                0   \n",
       "5                      0                       0                0   \n",
       "\n",
       "   Cultural_identity  Health_and_safety  \n",
       "1                  0                  0  \n",
       "2                  0                  0  \n",
       "3                  0                  0  \n",
       "4                  0                  0  \n",
       "5                  0                  0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = df[(df['Political'] == 1) | (df['Crime_and_punishment'] == 1)]\n",
    "\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1696624361912,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "FdAZUIg6doG0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df[\"content\"]\n",
    "y = df.drop(columns=[\"article_id\", \"frames\", \"frames_list\", \"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1696624361912,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "HlL31uTzdoG0",
    "outputId": "e732ffad-597e-42c6-f687-62693490fa18",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    How Theresa May Botched\\n\\nThose were the time...\n",
       "1    Robert Mueller III Rests His Case—Dems NEVER W...\n",
       "2    Robert Mueller Not Recommending Any More Indic...\n",
       "3    The Far Right Is Trying to Co-opt the Yellow V...\n",
       "4    ‘Special place in hell’ for those who promoted...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1696624361912,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "cDBgYDTidoG0",
    "outputId": "9c0094db-e682-4e25-bb33-76005f2e7ed5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Morality</th>\n",
       "      <th>Security_and_defense</th>\n",
       "      <th>Policy_prescription_and_evaluation</th>\n",
       "      <th>Legality_Constitutionality_and_jurisprudence</th>\n",
       "      <th>Economic</th>\n",
       "      <th>Political</th>\n",
       "      <th>Crime_and_punishment</th>\n",
       "      <th>External_regulation_and_reputation</th>\n",
       "      <th>Public_opinion</th>\n",
       "      <th>Fairness_and_equality</th>\n",
       "      <th>Capacity_and_resources</th>\n",
       "      <th>Quality_of_life</th>\n",
       "      <th>Cultural_identity</th>\n",
       "      <th>Health_and_safety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Morality  Security_and_defense  Policy_prescription_and_evaluation  \\\n",
       "0         1                     1                                   1   \n",
       "1         0                     0                                   1   \n",
       "2         0                     0                                   0   \n",
       "3         1                     1                                   0   \n",
       "4         0                     0                                   1   \n",
       "\n",
       "   Legality_Constitutionality_and_jurisprudence  Economic  Political  \\\n",
       "0                                             1         1          0   \n",
       "1                                             1         0          1   \n",
       "2                                             1         0          1   \n",
       "3                                             0         1          1   \n",
       "4                                             1         0          1   \n",
       "\n",
       "   Crime_and_punishment  External_regulation_and_reputation  Public_opinion  \\\n",
       "0                     0                                   0               0   \n",
       "1                     1                                   1               1   \n",
       "2                     1                                   1               0   \n",
       "3                     0                                   1               1   \n",
       "4                     0                                   1               0   \n",
       "\n",
       "   Fairness_and_equality  Capacity_and_resources  Quality_of_life  \\\n",
       "0                      0                       0                0   \n",
       "1                      0                       0                0   \n",
       "2                      1                       0                0   \n",
       "3                      1                       0                0   \n",
       "4                      0                       0                0   \n",
       "\n",
       "   Cultural_identity  Health_and_safety  \n",
       "0                  0                  0  \n",
       "1                  0                  0  \n",
       "2                  0                  0  \n",
       "3                  0                  0  \n",
       "4                  0                  0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1696624371765,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "QhcR4ySddoG1",
    "outputId": "1ab79474-cc3b-490e-da77-ae0877c58266",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(432, 432)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.to_csv(\"../notebooks/classifier/y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='transformers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract SRL Embeddings from articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycuda in /usr/local/lib/python3.9/dist-packages (2022.2.2)\n",
      "Requirement already satisfied: mako in /usr/local/lib/python3.9/dist-packages (from pycuda) (1.2.4)\n",
      "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from pycuda) (1.4.4)\n",
      "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.9/dist-packages (from pycuda) (2023.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.9/dist-packages (from pytools>=2011.2->pycuda) (4.4.0)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from pytools>=2011.2->pycuda) (2.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from mako->pycuda) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: allennlp in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
      "Requirement already satisfied: allennlp-models in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
      "Requirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.23.4)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.9.2)\n",
      "Requirement already satisfied: jsonnet>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.20.0)\n",
      "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.1.0)\n",
      "Requirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (5.8.1)\n",
      "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.3.5.1)\n",
      "Requirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.9/dist-packages (from allennlp) (4.64.1)\n",
      "Requirement already satisfied: filelock<3.8,>=3.3 in /usr/local/lib/python3.9/dist-packages (from allennlp) (3.7.1)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (3.20.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.1.2)\n",
      "Requirement already satisfied: lmdb>=1.2.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.4.1)\n",
      "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.4.2)\n",
      "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.9/dist-packages (from allennlp) (7.2.1)\n",
      "Requirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.9/dist-packages (from allennlp) (3.7)\n",
      "Requirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (10.1.0)\n",
      "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.9/dist-packages (from allennlp) (2.6.2.2)\n",
      "Requirement already satisfied: wandb<0.13.0,>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.12.21)\n",
      "Requirement already satisfied: cached-path<1.2.0,>=1.1.3 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.1.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.10.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.1.1)\n",
      "Requirement already satisfied: torchvision<0.14.0,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.13.1+cu116)\n",
      "Requirement already satisfied: spacy<3.4,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (3.3.3)\n",
      "Requirement already satisfied: transformers<4.21,>=4.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (4.20.1)\n",
      "Requirement already satisfied: sentencepiece>=0.1.96 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.1.97)\n",
      "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.9/dist-packages (from allennlp) (2.28.2)\n",
      "Requirement already satisfied: torch<1.13.0,>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.12.1+cu116)\n",
      "Requirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (3.8.0)\n",
      "Requirement already satisfied: base58>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (2.1.1)\n",
      "Requirement already satisfied: fairscale==0.4.6 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.4.6)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.9/dist-packages (from allennlp-models) (6.1.1)\n",
      "Requirement already satisfied: py-rouge==1.1 in /usr/local/lib/python3.9/dist-packages (from allennlp-models) (1.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (from allennlp-models) (2.4.0)\n",
      "Requirement already satisfied: conllu==4.4.2 in /usr/local/lib/python3.9/dist-packages (from allennlp-models) (4.4.2)\n",
      "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.9/dist-packages (from allennlp-models) (1.1)\n",
      "Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.9/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (2.13.0)\n",
      "Requirement already satisfied: rich<13.0,>=12.1 in /usr/local/lib/python3.9/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (12.6.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (1.24.90)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.0.16->allennlp) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.0.16->allennlp) (4.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.0.16->allennlp) (5.4.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk>=3.6.5->allennlp) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk>=3.6.5->allennlp) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk>=3.6.5->allennlp) (8.1.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (23.1.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (1.1.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (1.0.0)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (2.0.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.28->allennlp) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.28->allennlp) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.28->allennlp) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.28->allennlp) (2.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.1->allennlp) (3.1.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.12)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (66.1.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.1.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (8.0.17)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (6.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.8)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (9.2.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<4.21,>=4.1->allennlp) (0.12.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.4.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.9.4)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.3.2)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.1.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.0.11)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (3.1.30)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.14.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.14.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (1.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (0.70.13)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (10.0.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (2023.1.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (3.8.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (3.2.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy->allennlp-models) (0.2.6)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.90 in /usr/local/lib/python3.9/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.27.90)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (1.3.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (4.0.10)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.6.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.23.3 in /usr/local/lib/python3.9/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.23.4)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.9/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.12.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.5.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.14.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (0.9.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->allennlp-models) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->allennlp-models) (2022.7.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (5.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.61.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.2.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.4.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pycuda\n",
    "!pip install allennlp allennlp-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 device(s) found.\n",
      "0 Quadro P5000\n"
     ]
    }
   ],
   "source": [
    "import pycuda\n",
    "from pycuda import compiler\n",
    "import pycuda.driver as drv\n",
    "\n",
    "drv.init()\n",
    "print(\"%d device(s) found.\" % drv.Device.count())\n",
    "           \n",
    "for ordinal in range(drv.Device.count()):\n",
    "    dev = drv.Device(ordinal)\n",
    "    print (ordinal, dev.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "from allennlp_models.structured_prediction.models import srl_bert\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def batched_extract_srl_components(sentences, predictor):\n",
    "    # Prepare the batched input for the predictor\n",
    "    batched_input = [{'sentence': sentence} for sentence in sentences]\n",
    "    batched_srl = predictor.predict_batch_json(batched_input)\n",
    "    \n",
    "    # Extract SRL components from the batched predictions\n",
    "    results = []\n",
    "    for srl in batched_srl:\n",
    "        best_extracted_data = None\n",
    "        second_best_extracted_data = None\n",
    "        for verb_entry in srl['verbs']:\n",
    "            tags = verb_entry['tags']\n",
    "            arg0_indices = [i for i, tag in enumerate(tags) if tag in ['B-ARG0', 'I-ARG0']]\n",
    "            arg1_indices = [i for i, tag in enumerate(tags) if tag in ['B-ARG1', 'I-ARG1']]\n",
    "\n",
    "            if arg0_indices and arg1_indices:\n",
    "                best_extracted_data = {\n",
    "                    'predicate': verb_entry['verb'],\n",
    "                    'ARG0': ' '.join([srl['words'][i] for i in arg0_indices]),\n",
    "                    'ARG1': ' '.join([srl['words'][i] for i in arg1_indices])\n",
    "                }\n",
    "                break\n",
    "            elif (arg0_indices or arg1_indices) and not second_best_extracted_data:\n",
    "                second_best_extracted_data = {\n",
    "                    'predicate': verb_entry['verb'],\n",
    "                    'ARG0': ' '.join([srl['words'][i] for i in arg0_indices]) if arg0_indices else '',\n",
    "                    'ARG1': ' '.join([srl['words'][i] for i in arg1_indices]) if arg1_indices else ''\n",
    "                }\n",
    "\n",
    "        if best_extracted_data:\n",
    "            results.append(best_extracted_data)\n",
    "        elif second_best_extracted_data:\n",
    "            results.append(second_best_extracted_data)\n",
    "            \n",
    "    return results\n",
    "\n",
    "def optimized_extract_srl(X, predictor, batch_size=32):\n",
    "    total_articles = len(X)\n",
    "    processed_articles = 0\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for article in X:\n",
    "        sentences = sent_tokenize(article)\n",
    "        article_srls = []\n",
    "\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batched_sentences = sentences[i:i+batch_size]\n",
    "            article_srls.extend(batched_extract_srl_components(batched_sentences, predictor))\n",
    "\n",
    "        all_results.append(article_srls)\n",
    "        processed_articles += 1\n",
    "        print(f\"Processed article {processed_articles}/{total_articles}\")\n",
    "\n",
    "    return pd.Series(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_X_srl(X, recalculate=False, pickle_path=\"../notebooks/classifier/X_srl_filtered.pkl\"):\n",
    "    \"\"\"\n",
    "    Returns the X_srl either by loading from a pickled file or recalculating.\n",
    "    \"\"\"\n",
    "    if recalculate or not os.path.exists(pickle_path):\n",
    "        print(\"Recalculate SRL\")\n",
    "        # Load predictor\n",
    "        predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\", cuda_device=0)\n",
    "        X_srl = optimized_extract_srl(X, predictor)\n",
    "        with open(pickle_path, 'wb') as f:\n",
    "            pickle.dump(X_srl, f)\n",
    "    else:\n",
    "        print(\"Load SRL from Pickle\")\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            X_srl = pickle.load(f)\n",
    "    return X_srl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def free_gpu():\n",
    "    print(torch.cuda.mem_get_info())\n",
    "    print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/distributed/distributed_c10d.py:181: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def list_gpu_tensors():\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                if obj.is_cuda:\n",
    "                    obj = obj.cpu()\n",
    "                    obj = obj.to(\"cpu\")\n",
    "                    print(type(obj), obj.size())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "list_gpu_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "class ArticleDataset(Dataset):\n",
    "    def __init__(self, X, X_srl, tokenizer, labels=None, max_sentences_per_article=32, max_sentence_length=32, max_arg_length=16):\n",
    "        self.X = X\n",
    "        self.X_srl = X_srl\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sentences_per_article = max_sentences_per_article\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.max_arg_length = max_arg_length\n",
    "        nltk.download('punkt')  # Download the Punkt tokenizer model for sentence splitting\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def _truncate_or_pad(self, lst, target_length, pad_value=0):\n",
    "        \"\"\"\n",
    "        Truncate or pad the input list to match the target length.\n",
    "        \"\"\"\n",
    "        if len(lst) > target_length:\n",
    "            return lst[:target_length]\n",
    "        else:\n",
    "            return lst + [pad_value] * (target_length - len(lst))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        article = self.X.iloc[idx]\n",
    "        srl = self.X_srl.iloc[idx]\n",
    "\n",
    "        # Split the article into sentences\n",
    "        sentences = nltk.sent_tokenize(article)\n",
    "        sentences = sentences[:self.max_sentences_per_article]  # Limit the number of sentences\n",
    "\n",
    "        # Tokenize and pad/truncate the sentences\n",
    "        sentence_ids = [self.tokenizer.encode(sentence, add_special_tokens=True, max_length=self.max_sentence_length, truncation=True, padding='max_length') for sentence in sentences]\n",
    "        while len(sentence_ids) < self.max_sentences_per_article:\n",
    "            sentence_ids.append([0] * self.max_sentence_length)\n",
    "\n",
    "        # Tokenize and pad/truncate the SRL items\n",
    "        predicate_ids = [self.tokenizer.encode(predicate, add_special_tokens=True, max_length=self.max_arg_length, truncation=True, padding='max_length') for predicate in [item['predicate'] for item in srl]]\n",
    "        arg0_ids = [self.tokenizer.encode(arg0, add_special_tokens=True, max_length=self.max_arg_length, truncation=True, padding='max_length') for arg0 in [item.get('arg0', '') for item in srl]]\n",
    "        arg1_ids = [self.tokenizer.encode(arg1, add_special_tokens=True, max_length=self.max_arg_length, truncation=True, padding='max_length') for arg1 in [item.get('arg1', '') for item in srl]]\n",
    "        \n",
    "        predicate_ids = predicate_ids[:self.max_sentences_per_article]\n",
    "        arg0_ids = arg0_ids[:self.max_sentences_per_article]\n",
    "        arg1_ids = arg1_ids[:self.max_sentences_per_article]  \n",
    "        \n",
    "        while len(predicate_ids) < self.max_sentences_per_article:\n",
    "            predicate_ids.append([0] * self.max_arg_length)\n",
    "        while len(arg0_ids) < self.max_sentences_per_article:\n",
    "            arg0_ids.append([0] * self.max_arg_length)\n",
    "        while len(arg1_ids) < self.max_sentences_per_article:\n",
    "            arg1_ids.append([0] * self.max_arg_length)\n",
    "\n",
    "        data = {\n",
    "            'sentence_ids': torch.tensor(sentence_ids, dtype=torch.long),\n",
    "            'predicate_ids': torch.tensor(predicate_ids, dtype=torch.long),\n",
    "            'arg0_ids': torch.tensor(arg0_ids, dtype=torch.long),\n",
    "            'arg1_ids': torch.tensor(arg1_ids, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            data['labels'] = self.labels.iloc[idx]\n",
    "        \n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Extract individual lists from the batch\n",
    "    sentence_ids = [item['sentence_ids'] for item in batch]\n",
    "    predicate_ids = [item['predicate_ids'] for item in batch]\n",
    "    arg0_ids = [item['arg0_ids'] for item in batch]\n",
    "    arg1_ids = [item['arg1_ids'] for item in batch]\n",
    "    \n",
    "    # Pad each list\n",
    "    sentence_ids = torch.nn.utils.rnn.pad_sequence(sentence_ids, batch_first=True, padding_value=0)\n",
    "    predicate_ids = torch.nn.utils.rnn.pad_sequence(predicate_ids, batch_first=True, padding_value=0)\n",
    "    arg0_ids = torch.nn.utils.rnn.pad_sequence(arg0_ids, batch_first=True, padding_value=0)\n",
    "    arg1_ids = torch.nn.utils.rnn.pad_sequence(arg1_ids, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Conditionally extract and add labels\n",
    "    output_dict = {\n",
    "        'sentence_ids': sentence_ids,\n",
    "        'predicate_ids': predicate_ids,\n",
    "        'arg0_ids': arg0_ids,\n",
    "        'arg1_ids': arg1_ids\n",
    "    }\n",
    "    \n",
    "    if 'labels' in batch[0]:\n",
    "        labels = [item['labels'] for item in batch]\n",
    "        output_dict['labels'] = torch.Tensor(labels)\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def get_datasets_dataloaders(X, y, tokenizer, recalculate_srl=False, pickle_path=\"../notebooks/X_srl_filtered.pkl\", batch_size=16, max_sentences_per_article=32, max_sentence_length=32, max_arg_length=16):\n",
    "    # Get X_srl\n",
    "    X_srl = get_X_srl(X, recalculate=recalculate_srl, pickle_path=pickle_path)\n",
    "    \n",
    "    # Assuming X, X_srl, and y are already defined and have the same number of samples\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "    X_srl_train, X_srl_test, _, _ = train_test_split(X_srl, y, test_size=0.20, random_state=42)\n",
    "    \n",
    "    # Create the dataset\n",
    "    train_dataset = ArticleDataset(X_train, X_srl_train, tokenizer, y_train, max_sentences_per_article, max_sentence_length, max_arg_length)\n",
    "    test_dataset = ArticleDataset(X_test, X_srl_test, tokenizer, y_test, max_sentences_per_article, max_sentence_length, max_arg_length)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    print(\"CREATION DONE\")\n",
    "    return train_dataset, test_dataset, train_dataloader, test_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov  7 08:42:58 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro P5000        Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 38%   49C    P8     8W / 180W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_dataloader(article, tokenizer, batch_size=4):\n",
    "    X = pd.Series([article])\n",
    "    y = None  # No labels for this single article\n",
    "    \n",
    "    predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\", cuda_device=0)\n",
    "    # Directly use the optimized_extract_srl function since we don't need to cache for single articles\n",
    "    X_srl = optimized_extract_srl(X, predictor)\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = ArticleDataset(X, X_srl, tokenizer, y)\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Model\n",
    "The Model consist out of various Layers.\n",
    "\n",
    "1. SRL_Embedding\n",
    "2. Autoencoder\n",
    "3. FRISSLoss\n",
    "4. Unsupervised\n",
    "5. Supervised\n",
    "6. FRISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SRL_Embeddings\n",
    "\n",
    "The layer takes tensors of token IDs with the shape [batch_size, max_num_sentences, max_num_tokens] for the sentence, predicates, arg0 and arg1 and returns for each sentence an embedding with shape [batch_size, embedding_dim] for the sentence, predicate, arg0 and arg1. \n",
    "\n",
    "The single embedding for the sentence is extracted by taking the [CLS] token embedding. For the predicate, arg0 and arg1 by taking the mean over all word embeddings in this list of tokens. \n",
    "\n",
    "> Possible improvements: Better way of extracting the single embedding for predicate, arg0 and arg1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12, 768]) torch.Size([2, 12, 768]) torch.Size([2, 12, 768]) torch.Size([2, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class SRL_Embeddings(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"bert-base-uncased\"):\n",
    "        super(SRL_Embeddings, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "        self.embedding_dim = 768  # for bert-base-uncased\n",
    "\n",
    "    def forward(self, sentence_ids, predicate_ids, arg0_ids, arg1_ids):\n",
    "        #with torch.no_grad():\n",
    "        # Extract embeddings directly using BERT\n",
    "        # Adjust dimensions to 2D for BERT input, then reshape back to 3D\n",
    "        sentence_embeddings_3d = self.bert_model(sentence_ids.view(-1, sentence_ids.size(-1)))[0].view(sentence_ids.size(0), sentence_ids.size(1), -1, self.embedding_dim)\n",
    "        predicate_embeddings_3d = self.bert_model(predicate_ids.view(-1, predicate_ids.size(-1)))[0].view(predicate_ids.size(0), predicate_ids.size(1), -1, self.embedding_dim)\n",
    "        arg0_embeddings_3d = self.bert_model(arg0_ids.view(-1, arg0_ids.size(-1)))[0].view(arg0_ids.size(0), arg0_ids.size(1), -1, self.embedding_dim)\n",
    "        arg1_embeddings_3d = self.bert_model(arg1_ids.view(-1, arg1_ids.size(-1)))[0].view(arg1_ids.size(0), arg1_ids.size(1), -1, self.embedding_dim)\n",
    "\n",
    "        # Use [CLS] token embedding for sentences\n",
    "        sentence_embeddings = sentence_embeddings_3d[:, :, 0, :]\n",
    "\n",
    "        # Average token embeddings for predicates, ARG0, and ARG1\n",
    "        predicate_embeddings = predicate_embeddings_3d.mean(dim=2)\n",
    "        arg0_embeddings = arg0_embeddings_3d.mean(dim=2)\n",
    "        arg1_embeddings = arg1_embeddings_3d.mean(dim=2)\n",
    "        \n",
    "        return sentence_embeddings, predicate_embeddings, arg0_embeddings, arg1_embeddings\n",
    "\n",
    "# Generate dummy data for the SRL_Embeddings\n",
    "batch_size = 2\n",
    "num_sentences = 12\n",
    "sentence_length = 8\n",
    "predicate_length = 8\n",
    "arg0_length = 8\n",
    "arg1_length = 8\n",
    "\n",
    "# Dummy data for sentences, predicates, arg0, and arg1\n",
    "sentence_ids = torch.randint(0, 10000, (batch_size, num_sentences, sentence_length))\n",
    "predicate_ids = torch.randint(0, 10000, (batch_size, num_sentences, predicate_length))\n",
    "arg0_ids = torch.randint(0, 10000, (batch_size, num_sentences, arg0_length))\n",
    "arg1_ids = torch.randint(0, 10000, (batch_size, num_sentences, arg1_length))\n",
    "\n",
    "srl_embeddings = SRL_Embeddings()\n",
    "\n",
    "sentence_embeddings, predicate_embeddings, arg0_embeddings, arg1_embeddings = srl_embeddings(sentence_ids, predicate_ids, arg0_ids, arg1_ids)\n",
    "\n",
    "print(sentence_embeddings.shape, predicate_embeddings.shape, arg0_embeddings.shape, arg1_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autoencoder\n",
    "\n",
    "The layer takes tensors for `v` (size: [batch_size, embedding_dim]), `v_sentence` (size: [batch_size, embedding_dim]), `tau` (type: _float_), and `identifier` (type: _str_). Where `v` is the embedding of either predicate, arg0 or arg1 identified by the `identifier` parameter. The `v_sentence` is the sentence embedding and `tau` defined the tau for annealing the gumpel softmax.\n",
    "\n",
    "The forward function returns `vhat` (size: [batch_size, embedding_dim]), `dz` (size: [batch_size, embedding_dim]), `gz` (size: [batch_size, embedding_dim]) and `F` (size: [K, embedding_dim]).\n",
    "\n",
    "- `vhat`: Reconstructed embedding of SRL\n",
    "- `dz`: Descriptor weights\n",
    "- `gz`: Gumbel softmax from logits\n",
    "- `F`: Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shapes:\n",
      "p -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n",
      "a0 -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n",
      "a1 -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CombinedAutoencoder(nn.Module):\n",
    "    def __init__(self, D_w, D_h, K, dropout_prob=0.3):\n",
    "        super(CombinedAutoencoder, self).__init__()\n",
    "        \n",
    "        self.D_h = D_h\n",
    "        self.K = K\n",
    "        \n",
    "        # Shared feed-forward layer for all views\n",
    "        self.feed_forward_shared = nn.Linear(2 * D_w, D_h)\n",
    "        \n",
    "        # Unique feed-forward layers for each view\n",
    "        self.feed_forward_unique = nn.ModuleDict({\n",
    "            'a0': nn.Linear(D_h, K),\n",
    "            'p': nn.Linear(D_h, K),\n",
    "            'a1': nn.Linear(D_h, K),\n",
    "        })\n",
    "\n",
    "        # Initializing F matrices for each view\n",
    "        self.F_matrices = nn.ParameterDict({\n",
    "            'a0': nn.Parameter(torch.randn(K, D_w)),\n",
    "            'p': nn.Parameter(torch.randn(K, D_w)),\n",
    "            'a1': nn.Parameter(torch.randn(K, D_w)),\n",
    "        })\n",
    "\n",
    "        # Additional layers and parameters\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.batch_norm = nn.BatchNorm1d(D_h)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def gumbel_sigmoid(self, logits, tau: float = 1, hard: bool = False, threshold: float = 0.5):\n",
    "        gumbels = (\n",
    "            -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()\n",
    "        )\n",
    "        gumbels = (logits + gumbels) / tau\n",
    "        y_soft = gumbels.sigmoid()\n",
    "\n",
    "        if hard:\n",
    "            indices = (y_soft > threshold).nonzero(as_tuple=True)\n",
    "            y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format)\n",
    "            y_hard[indices[0], indices[1]] = 1.0\n",
    "            ret = y_hard - y_soft.detach() + y_soft\n",
    "        else:\n",
    "            ret = y_soft\n",
    "        return ret\n",
    "    \n",
    "    def forward(self, v_p, v_a0, v_a1, v_sentence, tau):\n",
    "        h_p, h_a0, h_a1 = self.process_through_shared(v_p, v_a0, v_a1, v_sentence)\n",
    "\n",
    "        logits_p = self.feed_forward_unique['p'](h_p)\n",
    "        logits_a0 = self.feed_forward_unique['a0'](h_a0)\n",
    "        logits_a1 = self.feed_forward_unique['a1'](h_a1)\n",
    "        \n",
    "        dz_p = torch.sigmoid(logits_p)\n",
    "        dz_a0 = torch.sigmoid(logits_a0)\n",
    "        dz_a1 = torch.sigmoid(logits_a1)\n",
    "        \n",
    "        gz_p = self.gumbel_sigmoid(dz_p, tau=tau, hard=True)\n",
    "        gz_a0 = self.gumbel_sigmoid(dz_a0, tau=tau, hard=True)\n",
    "        gz_a1 = self.gumbel_sigmoid(dz_a1, tau=tau, hard=True)\n",
    "\n",
    "        vhat_p = torch.matmul(gz_p, self.F_matrices['p'])\n",
    "        vhat_a0 = torch.matmul(gz_a0, self.F_matrices['a0'])\n",
    "        vhat_a1 = torch.matmul(gz_a1, self.F_matrices['a1'])\n",
    "\n",
    "        return {\n",
    "            \"p\": {\"vhat\": vhat_p, \"d\": dz_p, \"g\": gz_p, \"F\": self.F_matrices['p']},\n",
    "            \"a0\": {\"vhat\": vhat_a0, \"d\": dz_a0, \"g\": gz_a0, \"F\": self.F_matrices['a0']},\n",
    "            \"a1\": {\"vhat\": vhat_a1, \"d\": dz_a1, \"g\": gz_a1, \"F\": self.F_matrices['a1']}\n",
    "        }\n",
    "        \n",
    "    def process_through_shared(self, v_p, v_a0, v_a1, v_sentence):\n",
    "        concatenated_p = torch.cat((v_p, v_sentence), dim=-1)\n",
    "        concatenated_a0 = torch.cat((v_a0, v_sentence), dim=-1)\n",
    "        concatenated_a1 = torch.cat((v_a1, v_sentence), dim=-1)\n",
    "        \n",
    "        # Concatenate them along the batch dimension for a single pass through the shared layer\n",
    "        stacked_embeddings = torch.cat([concatenated_p, concatenated_a0, concatenated_a1], dim=0)\n",
    "        \n",
    "        #h_shared = self.dropout(stacked_embeddings)\n",
    "        h_shared = self.feed_forward_shared(stacked_embeddings)\n",
    "        \n",
    "        # Splitting them back to individual embeddings\n",
    "        batch_size = v_p.shape[0]\n",
    "        h_shared = h_shared.view(3, batch_size, self.D_h)\n",
    "        \n",
    "        h_p, h_a0, h_a1 = h_shared[0], h_shared[1], h_shared[2]\n",
    "        return h_p, h_a0, h_a1\n",
    "\n",
    "# Mock Data Preparation\n",
    "D_h = 768\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "K = 20\n",
    "tau = 0.9\n",
    "\n",
    "# Generating mock embeddings for article, predicate, ARG0, ARG1, and their corresponding sentence embeddings\n",
    "article_embedding = torch.randn(batch_size, embedding_dim)\n",
    "v_p = torch.randn(batch_size, embedding_dim)\n",
    "v_a0 = torch.randn(batch_size, embedding_dim)\n",
    "v_a1 = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "# Testing CombinedAutoencoder\n",
    "autoencoder = CombinedAutoencoder(embedding_dim, D_h, K)\n",
    "outputs = autoencoder(v_p, v_a0, v_a1, article_embedding, tau)\n",
    "\n",
    "# Check shapes of the outputs\n",
    "print(\"Output shapes:\")\n",
    "for key, value in outputs.items():\n",
    "    print(f\"{key} -> vhat: {value['vhat'].shape}, d: {value['d'].shape}, g: {value['g'].shape}, F: {value['F'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FRISSLoss\n",
    "\n",
    "The layer calculates the unsupervised loss for predicate, arg0 and arg1. \n",
    "\n",
    "The forward function takes as input 3 dicts with the parameters `v`, `v_hat`, `g` and `F`. Where `v` is the embedding of the predicate, arg0 or arg1. The `v_hat` (size: [batch_size, embedding_dim]) is the reconstructed embedding for the predicate, arg0 and arg1. The `g` is the gumbel softmax result (size: [batch_size, embedding_dim]). The `F` (size: [K, embedding_dim]) which is the descriptor dictionary.\n",
    "\n",
    "The layer returns the loss for each batch. So the output is [batch_size]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRiSSLoss output: tensor([798108.0625, 798107.4375])\n"
     ]
    }
   ],
   "source": [
    "class FRISSLoss(nn.Module):\n",
    "    def __init__(self, lambda_orthogonality, M, t):\n",
    "        super(FRISSLoss, self).__init__()\n",
    "        \n",
    "        self.lambda_orthogonality = lambda_orthogonality\n",
    "        self.M = M\n",
    "        self.t = t\n",
    "        self.triplet_loss = nn.TripletMarginLoss(margin=M)\n",
    "\n",
    "    def contrastive_loss(self, v, vhat, negatives):\n",
    "        batch_size = vhat.size(0)\n",
    "        N = negatives.size(1)\n",
    "        loss = torch.zeros(batch_size, device=v.device)\n",
    "\n",
    "        # Calculate true distance between reconstructed and real embeddings\n",
    "        true_distance = self.l2(vhat, v)\n",
    "\n",
    "        for i in range(N):  # loop over each element in \"negatives\"\n",
    "            # Calculate negative distance for current negative embedding\n",
    "            negative_distance = self.l2(vhat, negatives[:, i])\n",
    "\n",
    "            # Compute loss based on the provided logic: l2(vhat, v) + 1 + l2(vhat, negative) and clamp to 0 if below 0\n",
    "            current_loss = 1 + true_distance - negative_distance\n",
    "            loss += torch.clamp(current_loss, min=0.0)\n",
    "\n",
    "        # Normalize the total loss by N\n",
    "        return loss / N\n",
    "\n",
    "    \n",
    "    def l2(self, u, v):\n",
    "            return torch.sqrt(torch.sum((u - v) ** 2, dim=1))\n",
    "    \n",
    "    def focal_triplet_loss(self, v, vhat_z, g, F):\n",
    "        losses = []\n",
    "        for i in range(F.size(0)):  # Iterate over each negative example\n",
    "            # For each negative, compute the loss against the anchor and positive\n",
    "            loss = self.triplet_loss(vhat_z, v, F[i].unsqueeze(0).expand(v.size(0), -1))\n",
    "            losses.append(loss)\n",
    "\n",
    "        loss_tensor = torch.stack(losses)  # This will be [20, 2]\n",
    "        loss = loss_tensor.mean(dim=0).mean()  # First mean over negatives, then mean over the batch\n",
    "        return loss\n",
    "    \n",
    "    def focal_triplet_loss_old(self, v, vhat_z, g, F):\n",
    "        _, indices = torch.topk(g, self.t, largest=False, dim=1)\n",
    "\n",
    "        F_t = torch.stack([F[indices[i]] for i in range(g.size(0))])\n",
    "        \n",
    "        g_tz = torch.stack([g[i, indices[i]] for i in range(g.size(0))])\n",
    "                    \n",
    "        g_t = g_tz / g_tz.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # if division by zero set all nan values to 0\n",
    "        g_t[torch.isnan(g_t)] = 0\n",
    "        \n",
    "        m_t = self.M * ((1 - g_t)**2)\n",
    "\n",
    "        # Initializing loss\n",
    "        loss = torch.zeros_like(v[:, 0])\n",
    "        \n",
    "        # Iteratively adding to the loss for each negative embedding\n",
    "        for i in range(self.t):\n",
    "            current_v_t = F_t[:, i]\n",
    "            current_m_t = m_t[:, i]\n",
    "            \n",
    "            current_loss = current_m_t + self.l2(vhat_z, v) - self.l2(vhat_z, current_v_t)\n",
    "            \n",
    "            loss += torch.max(torch.zeros_like(current_loss), current_loss)\n",
    "             \n",
    "        # Normalizing\n",
    "        loss = loss / self.t\n",
    "        return loss\n",
    "\n",
    "    def orthogonality_term(self, F, reg=1e-4):\n",
    "        gram_matrix = torch.mm(F, F.T)  # Compute the Gram matrix F * F^T\n",
    "        identity_matrix = torch.eye(gram_matrix.size(0), device=gram_matrix.device)  # Create an identity matrix\n",
    "        ortho_loss = (gram_matrix - identity_matrix).abs().sum()\n",
    "        return ortho_loss\n",
    "\n",
    "\n",
    "    def forward(self, p, a0, a1, p_negatives, a0_negatives, a1_negatives):\n",
    "        # Extract components from dictionary for predicate p\n",
    "        v_p, vhat_p, d_p, g_p, F_p = p[\"v\"], p[\"vhat\"], p[\"d\"], p[\"g\"], p[\"F\"]\n",
    "        \n",
    "        # Extract components from dictionary for ARG0\n",
    "        v_a0, vhat_a0, d_a0, g_a0, F_a0 = a0[\"v\"], a0[\"vhat\"], a0[\"d\"], a0[\"g\"], a0[\"F\"]\n",
    "\n",
    "        # Extract components from dictionary for ARG1\n",
    "        v_a1, vhat_a1, d_a1, g_a1, F_a1 = a1[\"v\"], a1[\"vhat\"], a1[\"d\"], a1[\"g\"], a1[\"F\"]\n",
    "        \n",
    "         # Calculate losses for predicate\n",
    "        Ju_p = self.contrastive_loss(v_p, vhat_p, p_negatives)        \n",
    "        Jt_p = self.focal_triplet_loss(v_p, vhat_p, g_p, F_p)\n",
    "        Jz_p = Ju_p + Jt_p + self.lambda_orthogonality * self.orthogonality_term(F_p) ** 2\n",
    "        #print(Ju_p, Jt_p, self.orthogonality_term(F_p))\n",
    "        # Calculate losses for ARG0\n",
    "        Ju_a0 = self.contrastive_loss(v_a0, vhat_a0, a0_negatives)\n",
    "        Jt_a0 = self.focal_triplet_loss(v_a0, vhat_a0, g_a0, F_a0)\n",
    "        Jz_a0 = Ju_a0 + Jt_a0 + self.lambda_orthogonality * self.orthogonality_term(F_a0) ** 2\n",
    "        \n",
    "        # Calculate losses for ARG1\n",
    "        Ju_a1 = self.contrastive_loss(v_a1, vhat_a1, a1_negatives)\n",
    "        Jt_a1 = self.focal_triplet_loss(v_a1, vhat_a1, g_a1, F_a1)\n",
    "        Jz_a1 = Ju_a1 + Jt_a1 + self.lambda_orthogonality * self.orthogonality_term(F_a1) ** 2\n",
    "        \n",
    "        if torch.isnan(Jz_p).any():\n",
    "            print(\"Jz_p has nan\")\n",
    "            \n",
    "        if torch.isnan(Jz_a0).any():\n",
    "            print(\"Jz_a0 has nan\")\n",
    "            \n",
    "        if torch.isnan(Jz_a1).any():\n",
    "            print(\"Jz_a1 has nan\")\n",
    "        \n",
    "        # Aggregate the losses\n",
    "        loss = Jz_p + Jz_a0 + Jz_a1\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "# Mock Data Preparation\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "K = 15  # Number of frames/descriptors\n",
    "\n",
    "# Generating mock embeddings for article, predicate, ARG0, ARG1 and their reconstructions\n",
    "article_embedding = torch.randn(batch_size, embedding_dim)\n",
    "v_p = torch.randn(batch_size, embedding_dim)\n",
    "vhat_p = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "v_a0 = torch.randn(batch_size, embedding_dim)\n",
    "vhat_a0 = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "v_a1 = torch.randn(batch_size, embedding_dim)\n",
    "vhat_a1 = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "# Generating mock descriptor weights and descriptor matrices for predicate, ARG0, ARG1\n",
    "d_p = torch.randn(batch_size, K)\n",
    "d_a0 = torch.randn(batch_size, K)\n",
    "d_a1 = torch.randn(batch_size, K)\n",
    "\n",
    "F_p = torch.randn(K, embedding_dim)\n",
    "F_a0 = torch.randn(K, embedding_dim)\n",
    "F_a1 = torch.randn(K, embedding_dim)\n",
    "\n",
    "g_p = torch.randn(batch_size, K)\n",
    "g_a0 = torch.randn(batch_size, K)\n",
    "g_a1 = torch.randn(batch_size, K)\n",
    "\n",
    "# Generating some negative samples (let's assume 5 negative samples per batch entry)\n",
    "num_negatives = 8\n",
    "negatives_p = torch.randn(batch_size, num_negatives, embedding_dim)\n",
    "negatives_a0 = torch.randn(batch_size, num_negatives, embedding_dim)\n",
    "negatives_a1 = torch.randn(batch_size, num_negatives, embedding_dim)\n",
    "\n",
    "# Initialize loss function\n",
    "lambda_orthogonality = 1e-3\n",
    "\n",
    "t = 8  # Number of descriptors with smallest weights for negative samples\n",
    "M = t\n",
    "\n",
    "loss_fn = FRISSLoss(lambda_orthogonality, M, t)\n",
    "\n",
    "# Organizing inputs into dictionaries\n",
    "p = {\"v\": v_p, \"vhat\": vhat_p, \"d\": d_p, \"g\": g_p, \"F\": F_p}\n",
    "a0 = {\"v\": v_a0, \"vhat\": vhat_a0, \"d\": d_a0, \"g\": g_a0, \"F\": F_a0}\n",
    "a1 = {\"v\": v_a1, \"vhat\": vhat_a1, \"d\": d_a1, \"g\": g_a1, \"F\": F_a1}\n",
    "\n",
    "loss_fn = FRISSLoss(lambda_orthogonality, M, t)\n",
    "loss = loss_fn(p, a0, a1, negatives_p, negatives_a0, negatives_a1)\n",
    "print(\"FRiSSLoss output:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FRISSUnsupervised\n",
    "\n",
    "The `FRISSUnsupervised` layer integrates multiple autoencoders and the previously described `FRISSLoss` layer to achieve an unsupervised learning process over the predicates and their arguments.\n",
    "\n",
    "### Forward Method:\n",
    "\n",
    "**Inputs**:\n",
    "1. **v_p**: Embedding of the predicate with size: [batch_size, D_w].\n",
    "2. **v_a0**: Embedding of the ARG0 (first argument) with size: [batch_size, D_w].\n",
    "3. **v_a1**: Embedding of the ARG1 (second argument) with size: [batch_size, D_w].\n",
    "4. **v_article**: Embedding of the article with size: [batch_size, D_w].\n",
    "5. **negatives**: Tensor containing negative samples with size: [batch_size, num_negatives, D_w].\n",
    "6. **tau**: A scalar parameter for the Gumbel softmax in the autoencoder.\n",
    "\n",
    "**Outputs**:\n",
    "- A dictionary `results` containing:\n",
    "    - **loss**: A tensor representing the combined unsupervised loss over the batch with size: [batch_size].\n",
    "    - **p**: Dictionary containing components for the predicate, including reconstructed embedding (`vhat`), descriptor weights (`d`), Gumbel softmax result (`g`), and the descriptor matrix (`F`).\n",
    "    - **a0**: Same as `p` but for ARG0.\n",
    "    - **a1**: Same as `p` but for ARG1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results' Shapes:\n",
      "loss: tensor([1.6642e+08, 1.6642e+08], grad_fn=<AddBackward0>)\n",
      "p -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n",
      "a0 -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n",
      "a1 -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming you have already defined CombinedAutoencoder and its methods as provided earlier.\n",
    "\n",
    "class FRISSUnsupervised(nn.Module):\n",
    "    def __init__(self, D_w, D_h, K, num_frames, lambda_orthogonality, M, t, dropout_prob=0.3):\n",
    "        super(FRISSUnsupervised, self).__init__()\n",
    "\n",
    "        self.loss_fn = FRISSLoss(lambda_orthogonality, M, t)      \n",
    "        \n",
    "        # Using the CombinedAutoencoder instead of individual Autoencoders\n",
    "        self.combined_autoencoder = CombinedAutoencoder(D_w, D_h, K, dropout_prob=dropout_prob)\n",
    "\n",
    "    def forward(self, v_p, v_a0, v_a1, v_sentence, p_negatives, a0_negatives, a1_negatives, tau):\n",
    "        outputs = self.combined_autoencoder(v_p, v_a0, v_a1, v_sentence, tau)\n",
    "        \n",
    "        outputs_p = outputs[\"p\"]\n",
    "        outputs_p[\"v\"] = v_p\n",
    "        \n",
    "        outputs_a0 = outputs[\"a0\"]\n",
    "        outputs_a0[\"v\"] = v_a0\n",
    "        \n",
    "        outputs_a1 = outputs[\"a1\"]\n",
    "        outputs_a1[\"v\"] = v_a1\n",
    "        \n",
    "        loss = self.loss_fn(\n",
    "            outputs_p,\n",
    "            outputs_a0, \n",
    "            outputs_a1, \n",
    "            p_negatives, a0_negatives, a1_negatives\n",
    "        )\n",
    "\n",
    "        results = {\n",
    "            \"loss\": loss,\n",
    "            \"p\": outputs[\"p\"],\n",
    "            \"a0\": outputs[\"a0\"],\n",
    "            \"a1\": outputs[\"a1\"]\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Mock Data Preparation\n",
    "D_h = 768\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "K = 20\n",
    "num_frames = 15\n",
    "tau = 0.9\n",
    "lambda_orthogonality = 0.1  # Placeholder value, please replace with your actual value\n",
    "M = 7  # Placeholder value, please replace with your actual value\n",
    "t = 7  # Placeholder value, please replace with your actual value\n",
    "\n",
    "# Generating mock embeddings for article, predicate, ARG0, ARG1, and their corresponding sentence embeddings\n",
    "article_embedding = torch.randn(batch_size, embedding_dim)\n",
    "v_p = torch.randn(batch_size, embedding_dim)\n",
    "v_a0 = torch.randn(batch_size, embedding_dim)\n",
    "v_a1 = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "# Generating some negative samples (let's assume 5 negative samples per batch entry)\n",
    "num_negatives = 10\n",
    "negatives_p = torch.randn(batch_size, num_negatives, embedding_dim)\n",
    "negatives_a0 = torch.randn(batch_size, num_negatives, embedding_dim)\n",
    "negatives_a1 = torch.randn(batch_size, num_negatives, embedding_dim)\n",
    "\n",
    "# Testing FRISSUnsupervised\n",
    "unsupervised_module = FRISSUnsupervised(embedding_dim, D_h, K, num_frames, lambda_orthogonality, M, t)\n",
    "results = unsupervised_module(v_p, v_a0, v_a1, article_embedding, negatives_p, negatives_a0, negatives_a1, tau)\n",
    "\n",
    "# Print the results' shapes for verification\n",
    "print(\"Results' Shapes:\")\n",
    "for key, value in results.items():\n",
    "    if key == \"loss\":\n",
    "        print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(f\"{key} -> vhat: {value['vhat'].shape}, d: {value['d'].shape}, g: {value['g'].shape}, F: {value['F'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. FRISSSupervised\n",
    "\n",
    "The layer takes the embeddings from the args and the sentence and predicts frames. \n",
    "\n",
    "The embeddings for the args are averaged for each arg individually and then averaged on args level. The final embedding is feed into a linear layer and passed through a sigmoid function. \n",
    "\n",
    "The sentence embedding is feed into a linear layer and then into a relu function. After again in a linear function and then averaged. The average embeddung is again feed into a linear layer and lastly in a signoid function. \n",
    "\n",
    "It returns a span and sentence based prediction of shape [batch_size, num_frames]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 15]), torch.Size([2, 15]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FRISSSupervised(nn.Module):\n",
    "    def __init__(self, D_w, num_sentences, K, num_frames, dropout_prob=0.3, sentence_heads=8, srl_heads=8):\n",
    "        super(FRISSSupervised, self).__init__()\n",
    "\n",
    "        self.D_w = D_w\n",
    "        \n",
    "        # Separate MultiheadAttention layers for each SRL type\n",
    "        self.attention_s = nn.MultiheadAttention(embed_dim=D_w, num_heads=sentence_heads, dropout=dropout_prob)\n",
    "        self.attention_p = nn.MultiheadAttention(embed_dim=K, num_heads=srl_heads, dropout=dropout_prob)\n",
    "        self.attention_a0 = nn.MultiheadAttention(embed_dim=K, num_heads=srl_heads, dropout=dropout_prob)\n",
    "        self.attention_a1 = nn.MultiheadAttention(embed_dim=K, num_heads=srl_heads, dropout=dropout_prob)\n",
    "        \n",
    "        self.norm_s = nn.LayerNorm(D_w)\n",
    "        self.norm_p = nn.LayerNorm(K)\n",
    "        self.norm_a0 = nn.LayerNorm(K)\n",
    "        self.norm_a1 = nn.LayerNorm(K)\n",
    "        \n",
    "        # SRL Classifier for each SRL type\n",
    "        self.srl_classifier = nn.Sequential(\n",
    "            nn.Linear(K * 3, 2 * K),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(2 * K, num_frames)\n",
    "        )\n",
    "        \n",
    "        # Sentence-based Classifier\n",
    "        self.Wr = nn.Sequential(\n",
    "            nn.Linear(D_w * num_sentences, 2 * D_w),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(2 * D_w, D_w),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "        self.Wt = nn.Linear(D_w, num_frames)\n",
    "        \n",
    "    def forward(self, d_p, d_a0, d_a1, vs):\n",
    "        batch_size = d_p.shape[0]\n",
    "\n",
    "        # SRL Part\n",
    "        d_p, _ = self.attention_p(d_p, d_p, d_p)\n",
    "        d_a0, _ = self.attention_a0(d_a0, d_a0, d_a0)\n",
    "        d_a1, _ = self.attention_a1(d_a1, d_a1, d_a1)\n",
    "\n",
    "        d_p = self.norm_p(d_p)\n",
    "        d_a0 = self.norm_a0(d_a0)\n",
    "        d_a1 = self.norm_a1(d_a1)\n",
    "\n",
    "        # Concatenate and classify SRL\n",
    "        srl_cat = torch.cat((d_p.mean(dim=1), d_a0.mean(dim=1), d_a1.mean(dim=1)), dim=1)\n",
    "        yu_hat = self.srl_classifier(srl_cat)\n",
    "\n",
    "        # Sentence Part\n",
    "        vs, _ = self.attention_s(vs, vs, vs)\n",
    "        vs = self.norm_s(vs)\n",
    "        vs_flat = vs.view(batch_size, -1)\n",
    "        x = self.Wr(vs_flat)\n",
    "        ys_hat = self.Wt(x)\n",
    "        \n",
    "        return yu_hat, ys_hat\n",
    "\n",
    "# Mock Data Preparation\n",
    "\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "num_frames = 15  # Assuming the number of frames is equal to K for simplicity\n",
    "num_sentences = 32\n",
    "K = 20\n",
    "\n",
    "# Generating mock dsz representations for predicate, ARG0, ARG1\n",
    "d_p = torch.randn(batch_size, num_sentences, K)\n",
    "d_a0 = torch.randn(batch_size, num_sentences, K)\n",
    "d_a1 = torch.randn(batch_size, num_sentences, K) \n",
    "\n",
    "# Adjusting the num_heads parameter\n",
    "srl_heads = 4\n",
    "sentence_heads = 8\n",
    "\n",
    "# Adjust the mock sentence embeddings shape\n",
    "vs = torch.randn(batch_size, num_sentences, embedding_dim)\n",
    "\n",
    "# Initialize and test the supervised module\n",
    "supervised_module = FRISSSupervised(embedding_dim, num_sentences, K, num_frames, sentence_heads=sentence_heads, srl_heads=srl_heads)\n",
    "\n",
    "# Forward pass the mock data\n",
    "yu_hat, ys_hat = supervised_module(d_p, d_a0, d_a1, vs)\n",
    "yu_hat.shape, ys_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FRISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1.3162e+08, grad_fn=<DivBackward0>),\n",
       " torch.Size([2, 14]),\n",
       " torch.Size([2, 14]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FRISS(nn.Module):\n",
    "    def __init__(self, embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob=0.3, sentence_heads=8, srl_heads=4, bert_model_name=\"bert-base-uncased\"):\n",
    "        super(FRISS, self).__init__()\n",
    "        \n",
    "        # Aggregation layer replaced with SRL_Embeddings\n",
    "        self.aggregation = SRL_Embeddings(bert_model_name)\n",
    "        \n",
    "        # Unsupervised training module\n",
    "        self.unsupervised = FRISSUnsupervised(embedding_dim, D_h, K, num_frames, lambda_orthogonality, M, t, dropout_prob=dropout_prob)\n",
    "        \n",
    "        # Supervised training module\n",
    "        self.supervised = FRISSSupervised(embedding_dim, num_sentences, K, num_frames, dropout_prob=dropout_prob, sentence_heads=sentence_heads, srl_heads=srl_heads)\n",
    "        \n",
    "    def negative_sampling(self, embeddings, num_negatives=8):\n",
    "        batch_size, num_sentences, embedding_dim = embeddings.size()\n",
    "        negatives = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Get all the indices which are not padded (assuming padding is represented by all-zero vectors)\n",
    "            non_padded_indices = torch.where(torch.any(embeddings[i] != 0, dim=1))[0]\n",
    "\n",
    "            # Randomly sample negative indices from non-padded embeddings\n",
    "            negative_indices = non_padded_indices[torch.randint(0, len(non_padded_indices), (num_negatives,))]\n",
    "\n",
    "            # If there are fewer non-padded embeddings than required negatives, adjust the negative samples\n",
    "            while len(negative_indices) < num_negatives:\n",
    "                additional_indices = non_padded_indices[torch.randint(0, len(non_padded_indices), (num_negatives - len(negative_indices),))]\n",
    "                negative_indices = torch.cat((negative_indices, additional_indices))\n",
    "\n",
    "            negative_samples = embeddings[i, negative_indices]\n",
    "            negatives.append(negative_samples)\n",
    "\n",
    "        return torch.stack(negatives)    \n",
    "    \n",
    "    def forward(self, sentence_ids, predicate_ids, arg0_ids, arg1_ids, tau):\n",
    "        # Convert input IDs to embeddings\n",
    "        sentence_embeddings, predicate_embeddings, arg0_embeddings, arg1_embeddings = self.aggregation(sentence_ids, predicate_ids, arg0_ids, arg1_ids)\n",
    "        \n",
    "        # Handle multiple spans by averaging predictions\n",
    "        unsupervised_losses = torch.zeros((sentence_embeddings.size(0),), device=sentence_embeddings.device)\n",
    "        \n",
    "        # Creating storage for aggregated d tensors\n",
    "        d_p_list, d_a0_list, d_a1_list = [], [], []\n",
    "        \n",
    "        # Process each span\n",
    "        for span_idx in range(sentence_embeddings.size(1)):\n",
    "            s_sentence_span = sentence_embeddings[:, span_idx, :]\n",
    "            v_p_span = predicate_embeddings[:, span_idx, :]\n",
    "            v_a0_span = arg0_embeddings[:, span_idx, :]\n",
    "            v_a1_span = arg1_embeddings[:, span_idx, :]\n",
    "            \n",
    "            negatives_p = self.negative_sampling(predicate_embeddings)\n",
    "            negatives_a0 = self.negative_sampling(arg0_embeddings)\n",
    "            negatives_a1 = self.negative_sampling(arg1_embeddings)\n",
    " \n",
    "            # Feed the embeddings to the unsupervised module\n",
    "            unsupervised_results = self.unsupervised(v_p_span, v_a0_span, v_a1_span, s_sentence_span, negatives_p, negatives_a0, negatives_a1, tau)                \n",
    "            unsupervised_losses += unsupervised_results[\"loss\"]\n",
    "            \n",
    "            if torch.isnan(unsupervised_results[\"loss\"]).any():\n",
    "                print(\"loss is nan\")\n",
    "            \n",
    "            # Use the vhat (reconstructed embeddings) for supervised predictions\n",
    "            d_p_list.append(unsupervised_results['p']['d'])\n",
    "            d_a0_list.append(unsupervised_results['a0']['d'])\n",
    "            d_a1_list.append(unsupervised_results['a1']['d'])        \n",
    "        \n",
    "        # Aggregating across all spans\n",
    "        d_p_aggregated = torch.stack(d_p_list, dim=1)\n",
    "        d_a0_aggregated = torch.stack(d_a0_list, dim=1)\n",
    "        d_a1_aggregated = torch.stack(d_a1_list, dim=1)\n",
    "        \n",
    "        span_pred, sentence_pred = self.supervised(d_p_aggregated, d_a0_aggregated, d_a1_aggregated, sentence_embeddings)\n",
    "        \n",
    "        if torch.isnan(span_pred).any():\n",
    "            print(\"span_pred has nan:\", span_pred)\n",
    "        \n",
    "        if torch.isnan(sentence_pred).any():\n",
    "            print(\"sentence_pred has nan:\", sentence_pred)\n",
    "        \n",
    "        # Identify valid (non-nan) losses\n",
    "        valid_losses = ~torch.isnan(unsupervised_losses)\n",
    "\n",
    "        # Sum only the valid losses\n",
    "        #unsupervised_loss = unsupervised_losses[valid_losses].sum()\n",
    "        \n",
    "        # Take average by summing the valid losses and dividing by num sentences so that padded sentences are also taken in equation\n",
    "        unsupervised_loss = unsupervised_losses[valid_losses].sum() / sentence_embeddings.shape[1]\n",
    "        \n",
    "        return unsupervised_loss, span_pred, sentence_pred\n",
    "\n",
    "\n",
    "# Set the necessary parameters\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "K = 14  # Number of frames/descriptors\n",
    "num_frames = 14  # Assuming the number of frames is equal to K for simplicity\n",
    "D_h = 512  # Dimension of the hidden representation\n",
    "lambda_orthogonality = 0.1\n",
    "M = 8\n",
    "t = 8\n",
    "tau = 1.0\n",
    "\n",
    "# Define some mock token IDs data parameters\n",
    "max_sentences_per_article = 8\n",
    "max_sentence_length = 10\n",
    "num_sentences = max_sentences_per_article\n",
    "\n",
    "# Generating mock token IDs for predicate, ARG0, ARG1, and their corresponding sentences\n",
    "# We assume a vocab size of 30522 (standard BERT vocab size) for simplicity.\n",
    "vocab_size = 30522\n",
    "\n",
    "sentence_ids = torch.randint(0, vocab_size, (batch_size, max_sentences_per_article, max_sentence_length))\n",
    "predicate_ids = torch.randint(0, vocab_size, (batch_size, max_sentences_per_article, max_sentence_length))\n",
    "arg0_ids = torch.randint(0, vocab_size, (batch_size, max_sentences_per_article, max_sentence_length))\n",
    "arg1_ids = torch.randint(0, vocab_size, (batch_size, max_sentences_per_article, max_sentence_length))\n",
    "\n",
    "sentence_embeddings = torch.randn(batch_size, max_sentences_per_article, embedding_dim)\n",
    "predicate_embeddings = torch.randn(batch_size, max_sentences_per_article, embedding_dim)\n",
    "arg0_embeddings = torch.randn(batch_size, max_sentences_per_article, embedding_dim)\n",
    "arg1_embeddings = torch.randn(batch_size, max_sentences_per_article, embedding_dim)\n",
    "\n",
    "srl_heads = 7\n",
    "sentence_heads = 8\n",
    "\n",
    "# Initialize the FRISS model\n",
    "friss_model = FRISS(embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K=K, num_frames=num_frames, srl_heads=srl_heads)\n",
    "\n",
    "# Forward pass the mock data\n",
    "unsupervised_loss, span_pred, sentence_pred = friss_model(sentence_ids, predicate_ids, arg0_ids, arg1_ids, 1)\n",
    "unsupervised_loss, span_pred.shape, sentence_pred.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1-Score (micro-averaged) and Average Precision Score are chosen as primary metrics for evaluating the multi-label classification task due to the following reasons:\n",
    "\n",
    "1. **F1-Score (Micro)**:\n",
    "    - The micro-averaged F1-score computes global counts of true positives, false negatives, and false positives. \n",
    "    - It provides a balance between precision (the number of correct positive results divided by the number of all positive results) and recall (the number of correct positive results divided by the number of positive results that should have been returned).\n",
    "    - Given the imbalance in the label distribution observed in the dataset, the micro-averaged F1-score is robust against this imbalance, making it a suitable metric for optimization.\n",
    "\n",
    "2. **Average Precision Score**:\n",
    "    - This metric summarizes the precision-recall curve, giving a single value that represents the average of precision values at different recall levels.\n",
    "    - It's especially valuable when class imbalances exist, as it gives more weight to the positive class (the rarer class in an imbalanced dataset).\n",
    "\n",
    "Using these metrics will ensure that the model is optimized for a balanced performance across all labels, even if some labels are rarer than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "from math import exp\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, optimizer, loss_function, batch_size=4, alpha=0.5, num_epochs=10, tau_min=1, tau_decay=0.95, device='cuda', save_path='../notebooks/', save=False):\n",
    "    tau = 1\n",
    "    \n",
    "    metrics = {\n",
    "        'f1_span_micro': [],\n",
    "        'f1_sentence_micro': [],\n",
    "        'avg_span_precision': [],\n",
    "        'avg_sentence_precision': []\n",
    "    }\n",
    "\n",
    "    iteration = 0\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        supervised_total_loss = 0\n",
    "        unsupervised_total_loss = 0\n",
    "        batch_progress = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=\"Batches\", leave=False)\n",
    "        \n",
    "        for batch_idx, batch in batch_progress:            \n",
    "            iteration = iteration + 1\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            sentence_ids = batch['sentence_ids'].to(device)\n",
    "            predicate_ids = batch['predicate_ids'].to(device)\n",
    "            arg0_ids = batch['arg0_ids'].to(device)\n",
    "            arg1_ids = batch['arg1_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            assert torch.isnan(sentence_ids).sum() == 0\n",
    "            assert torch.isnan(predicate_ids).sum() == 0\n",
    "            assert torch.isnan(arg0_ids).sum() == 0\n",
    "            assert torch.isnan(arg1_ids).sum() == 0\n",
    "            \n",
    "            if sentence_ids.shape[0] != batch_size:\n",
    "                print(\"Wrong Batch Size\")\n",
    "                # IDK why but sometimes I ended up with a tensor of batch size 1 which doesn't work with the batch norm layer\n",
    "                continue   \n",
    "            \n",
    "            unsupervised_loss, span_logits, sentence_logits = model(sentence_ids, predicate_ids, arg0_ids, arg1_ids, tau)\n",
    "            \n",
    "            span_loss = loss_function(span_logits, labels.float())            \n",
    "            sentence_loss = loss_function(sentence_logits, labels.float())\n",
    "            \n",
    "            supervised_loss = span_loss + sentence_loss\n",
    "            \n",
    "            # alpha = unsupervised_loss / (supervised_loss + unsupervised_loss) # try dynamic alpha\n",
    "            \n",
    "            combined_loss = alpha * supervised_loss + (1-alpha) * unsupervised_loss\n",
    "            \n",
    "            # combined_loss = supervised_loss * unsupervised_loss\n",
    "            \n",
    "            if torch.isnan(combined_loss):\n",
    "                print(f\"NaN loss detected at epoch {epoch+1}, batch {batch_idx+1}. Stopping...\")\n",
    "                return\n",
    "        \n",
    "            combined_loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # After the backward pass\n",
    "            if any(p.grad is not None and torch.isnan(p.grad).any() for p in model.parameters()):\n",
    "                print(f\"NaN gradients detected at epoch {epoch+1}, batch {batch_idx+1}. Stopping...\")\n",
    "                return\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += combined_loss.item()\n",
    "            supervised_total_loss += supervised_loss.item()\n",
    "            unsupervised_total_loss += unsupervised_loss.item()\n",
    "\n",
    "            batch_progress.set_description(f\"Epoch {epoch+1} ({iteration}) Combined Loss: {combined_loss.item():.3f}, SRLs: {span_loss:.3f}, Sentence: {sentence_loss:.3f}, Unsupervised: {unsupervised_loss.item():.3f}\")\n",
    "                        \n",
    "            if save:\n",
    "                # Log metrics to CSV\n",
    "                with open(save_path + 'training_metrics.csv', 'a') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow([batch_idx, epoch+1, total_loss/len(train_dataloader), supervised_total_loss/len(train_dataloader), unsupervised_total_loss/len(train_dataloader)])\n",
    "\n",
    "            # Explicitly delete tensors to free up memory\n",
    "            del sentence_ids, predicate_ids, arg0_ids, arg1_ids, labels, unsupervised_loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Combined Loss: {total_loss/len(train_dataloader)}, Supervised Loss: {supervised_total_loss/len(train_dataloader)}, Unsupervised Loss: {unsupervised_total_loss/len(train_dataloader)}\")\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        span_preds = []\n",
    "        sentence_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                sentence_ids = batch['sentence_ids'].to(device)\n",
    "                predicate_ids = batch['predicate_ids'].to(device)\n",
    "                arg0_ids = batch['arg0_ids'].to(device)\n",
    "                arg1_ids = batch['arg1_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                _, span_logits, sentence_logits = model(sentence_ids, predicate_ids, arg0_ids, arg1_ids, tau)\n",
    "                span_pred = (torch.sigmoid(span_logits) > 0.5).float()\n",
    "                sentence_pred = (torch.sigmoid(sentence_logits) > 0.5).float()\n",
    "\n",
    "                print(\"SRL:\", span_pred)\n",
    "                print(\"Sentence:\", sentence_pred)\n",
    "                \n",
    "                span_preds.append(span_pred.cpu().numpy())\n",
    "                sentence_preds.append(sentence_pred.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "                # Explicitly delete tensors to free up memory\n",
    "                del sentence_ids, predicate_ids, arg0_ids, arg1_ids, labels, span_logits, sentence_logits, span_pred, sentence_pred\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        all_span_preds = np.vstack(span_preds)\n",
    "        all_sentence_preds = np.vstack(sentence_preds)\n",
    "        all_labels = np.vstack(all_labels)\n",
    "\n",
    "        f1_span_micro = f1_score(all_labels, all_span_preds, average='micro')\n",
    "        f1_sentence_micro = f1_score(all_labels, all_sentence_preds, average='micro')\n",
    "        \n",
    "        avg_span_precision = average_precision_score(all_labels, all_span_preds)\n",
    "        avg_sentence_precision = average_precision_score(all_labels, all_sentence_preds)\n",
    "\n",
    "        metrics['f1_span_micro'].append(f1_span_micro)\n",
    "        metrics['f1_sentence_micro'].append(f1_sentence_micro)\n",
    "        \n",
    "        metrics['avg_span_precision'].append(avg_span_precision)\n",
    "        metrics['avg_sentence_precision'].append(avg_sentence_precision)\n",
    "\n",
    "        print(f\"Validation Metrics - F1 Score Span, Sentence (Micro): {f1_span_micro:.4f}, {f1_sentence_micro:.4f} Average Span, Sentence Precision: {avg_span_precision:.4f}, {avg_sentence_precision:.4f}\")\n",
    "        \n",
    "        # Anneal tau at the end of the epoch\n",
    "        tau = max(tau_min, exp(-tau_decay * iteration))\n",
    "    \n",
    "    if save:\n",
    "        model_save_path = os.path.join(save_path, 'model1.pth')\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "    \n",
    "        with open('../notebooks/metrics.json', 'w') as f:\n",
    "            json.dump(metrics, f)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load SRL from Pickle\n",
      "CREATION DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 32\n",
    "batch_size = 6\n",
    "\n",
    "train_dataset, test_dataset, train_dataloader, test_dataloader = get_datasets_dataloaders(X, y, tokenizer, recalculate_srl=False, batch_size=batch_size, max_sentences_per_article=num_sentences, max_sentence_length=32, max_arg_length=12, pickle_path=\"../notebooks/X_srl_full.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0b5b83d71843fa866b65a41cbdb7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Batch Size\n",
      "Epoch 1/40, Combined Loss: 11580503.103448275, Supervised Loss: 22.466761128655797, Unsupervised Loss: 23160983.79310345\n",
      "SRL: tensor([[0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "Validation Metrics - F1 Score Span, Sentence (Micro): 0.2608, 0.4948 Average Span, Sentence Precision: 0.2619, 0.2619\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Batch Size\n",
      "Epoch 2/40, Combined Loss: 8879477.465517242, Supervised Loss: 20.835500947360334, Unsupervised Loss: 17758934.03448276\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "Validation Metrics - F1 Score Span, Sentence (Micro): 0.3412, 0.3567 Average Span, Sentence Precision: 0.2619, 0.2619\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Batch Size\n",
      "Epoch 3/40, Combined Loss: 6930350.568965517, Supervised Loss: 17.320610260141308, Unsupervised Loss: 13860683.793103449\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.]],\n",
      "       device='cuda:0')\n",
      "Validation Metrics - F1 Score Span, Sentence (Micro): 0.2854, 0.3319 Average Span, Sentence Precision: 0.2619, 0.2619\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Batch Size\n",
      "Epoch 4/40, Combined Loss: 5576497.370689655, Supervised Loss: 17.072803859052986, Unsupervised Loss: 11152977.75862069\n",
      "SRL: tensor([[0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "SRL: tensor([[0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "Sentence: tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.]],\n",
      "       device='cuda:0')\n",
      "Validation Metrics - F1 Score Span, Sentence (Micro): 0.2331, 0.2966 Average Span, Sentence Precision: 0.2619, 0.2619\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c24d71860b143c691b7d256d9685090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.set_printoptions(profile=\"full\")\n",
    "\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 768\n",
    "num_frames = 14\n",
    "\n",
    "D_h = 768\n",
    "lambda_orthogonality = 1e-3\n",
    "\n",
    "K = 28\n",
    "t = 28\n",
    "M = 7\n",
    "tau_min = 0.5\n",
    "tau_decay = 5e-4\n",
    "\n",
    "dropout_prob = 0.3\n",
    "\n",
    "sentence_heads = 8\n",
    "srl_heads = 7\n",
    "\n",
    "# Model instantiation\n",
    "model = FRISS(embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob=dropout_prob, sentence_heads=sentence_heads, srl_heads=srl_heads)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# LOSS\n",
    "\n",
    "# Compute the `weight` parameter for each label\n",
    "label_frequencies = y.mean()\n",
    "weights = 1 / (label_frequencies + 1e-10)  # Adding a small value to avoid division by zero\n",
    "\n",
    "# Compute the `pos_weight` parameter\n",
    "pos_weights = (1 - label_frequencies) / (label_frequencies + 1e-10)\n",
    "\n",
    "# Convert the computed weights and pos_weights to PyTorch tensors\n",
    "weights_tensor = torch.tensor(weights.values, dtype=torch.float32).to(device)\n",
    "pos_weights_tensor = torch.tensor(pos_weights.values, dtype=torch.float32).to(device)\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss(weight=weights_tensor, pos_weight=pos_weights_tensor, reduction=\"mean\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Train the model\n",
    "alpha_value = 0.5\n",
    "num_epochs_value = 40\n",
    "metrics = train(model, train_dataloader, test_dataloader, optimizer, loss_function, batch_size=batch_size, tau_min=tau_min, tau_decay=tau_decay, alpha=alpha_value, num_epochs=num_epochs_value, device=device, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 768\n",
    "num_frames = 2\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def grid_search(train_dataloader, test_dataloader, search_space, num_epochs=5):\n",
    "    # Store the results for each hyperparameter combination\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize the file to write metrics\n",
    "    with open(\"../notebooks/08-model/grid_search_metrics.csv\", \"w\", newline='') as csvfile:\n",
    "        fieldnames = ['alpha', 'lr', 'D_h', 'lambda_orthogonality', 'M', 't', 'tau_min', 'tau_decay', 'epoch', 'f1_span_micro', 'f1_sentence_micro', 'avg_span_precision', 'avg_sentence_precision']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Calculate the total number of combinations\n",
    "        total_combinations = 1\n",
    "        for key, values in search_space.items():\n",
    "            total_combinations *= len(values)\n",
    "        \n",
    "        # Loop through all combinations\n",
    "        for idx, combination in enumerate(product(*search_space.values())):\n",
    "            print(f\"Training combination {idx + 1}/{total_combinations}: {combination}\")\n",
    "            \n",
    "            # Extract hyperparameters from the current combination\n",
    "            alpha, lr, tau_min, tau_decay, t, D_h, lambda_orthogonality, M = combination\n",
    "            \n",
    "            # Initialize the model with current hyperparameters\n",
    "            model = FRISS(embedding_dim, D_h, lambda_orthogonality, M, t, num_frames)\n",
    "            model.to(device)\n",
    "            \n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            \n",
    "            # Train the model with the current hyperparameters\n",
    "            epoch_metrics = train(model, train_dataloader, test_dataloader, optimizer, tau_min=tau_min, tau_decay=tau_decay, alpha=alpha, num_epochs=num_epochs, device=device)\n",
    "            \n",
    "            # Write the metrics to the CSV file\n",
    "            for epoch, f1_span_micro in enumerate(epoch_metrics['f1_span_micro']):\n",
    "                f1_sentence_micro = epoch_metrics['f1_sentence_micro'][epoch]\n",
    "                avg_span_precision = epoch_metrics['avg_span_precision'][epoch]\n",
    "                avg_sentence_precision = epoch_metrics['avg_sentence_precision'][epoch]\n",
    "                row = {\n",
    "                    'alpha': alpha,\n",
    "                    'lr': lr,\n",
    "                    'D_h': D_h,\n",
    "                    'lambda_orthogonality': lambda_orthogonality,\n",
    "                    'M': M,\n",
    "                    't': t,\n",
    "                    'tau_min': tau_min,\n",
    "                    'tau_decay': tau_decay,\n",
    "                    'epoch': epoch + 1,\n",
    "                    'f1_span_micro': f1_span_micro,\n",
    "                    'f1_sentence_micro': f1_span_micro,\n",
    "                    'avg_span_precision': avg_span_precision,\n",
    "                    'avg_sentence_precision': avg_sentence_precision\n",
    "                }\n",
    "                writer.writerow(row)\n",
    "                csvfile.flush()\n",
    "    \n",
    "    return results\n",
    "\n",
    "search_space = {\n",
    "    'alpha': [1],#[0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'lr': [1e-5, 2e-5, 5e-4, 1e-3],\n",
    "    'tau_min': [0.1, 0.5],\n",
    "    'tau_decay': [1e-3, 5e-4, 1e-4],\n",
    "    't': [1,2],\n",
    "    'D_h': [256, 512],\n",
    "    'lambda_orthogonality': [1e-4, 5e-4, 1e-3, 5e-3, 1e-2],\n",
    "    'M': [1,2]\n",
    "}\n",
    "\n",
    "# Call the grid search function\n",
    "results = grid_search(train_dataloader, test_dataloader, search_space, num_epochs=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_path(model_class, path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads the weights into an instance of the model class from the given path.\n",
    "    \n",
    "    Args:\n",
    "    - model_class (torch.nn.Module): The class of the model (uninitialized).\n",
    "    - path (str): Path to the saved weights.\n",
    "    - device (str): Device to load the model on ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - model (torch.nn.Module): Model with weights loaded.\n",
    "    \"\"\"\n",
    "    model = model_class().to(device)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model_from_path(FRISS, '../notebooks/classifier/new_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, y_columns, device='cuda'):\n",
    "    \"\"\"\n",
    "    Make predictions with the given model and dataloader.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): The model to make predictions with.\n",
    "    - dataloader (DataLoader): DataLoader for the dataset to predict on.\n",
    "    - y_columns (pandas.Index): Column names from the y dataframe which correspond to labels.\n",
    "    - device (str): Device to make predictions on ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - predicted_labels (list of lists): List containing the predicted labels for each instance.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds_span = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move data to device\n",
    "            sentence_ids = batch['sentence_ids'].to(device)\n",
    "            predicate_ids = batch['predicate_ids'].to(device)\n",
    "            arg0_ids = batch['arg0_ids'].to(device)\n",
    "            arg1_ids = batch['arg1_ids'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits_span, _ = model(sentence_ids, predicate_ids, arg0_ids, arg1_ids)\n",
    "            preds_span = (torch.sigmoid(logits_span) > 0.5).float()\n",
    "\n",
    "            all_preds_span.append(preds_span.cpu().numpy())\n",
    "                \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    predictions = np.vstack(all_preds_span)\n",
    "    \n",
    "    # Convert boolean predictions to labels\n",
    "    predicted_labels = []\n",
    "    for pred in predictions:\n",
    "        labels = list(y_columns[pred.astype(bool)])\n",
    "        predicted_labels.append(labels)\n",
    "    \n",
    "    return predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# article813452859\n",
    "article = \"\"\"EU Profits From Trading With UK While London Loses Money – Political Campaigner\n",
    "\n",
    "With the Parliamentary vote on British Prime Minister Theresa May’s Brexit plan set to be held next month; President of the European Commission Jean Claude Juncker has criticised the UK’s preparations for their departure from the EU.\n",
    "But is there any chance that May's deal will make it through parliament and if it fails, how could this ongoing political deadlock finally come to an end?\n",
    "Sputnik spoke with political campaigner Michael Swadling for more…\n",
    "Sputnik: Does Theresa May have any chance of getting her deal through Parliament on the 14th January?\n",
    "Michael Swadling: I guess her only chance is if Labour decides that they want to dishonour democracy and effectively keep us in the EU.\n",
    "© AP Photo / Pablo Martinez Monsivais UK 'In Need of Leadership', May's Brexit Deal Unwelcome to Trump - US Ambassador\n",
    "There is a chance; as unfortunately there are many MPs who don't respect the vote and may just turn on it, but short of that I don't see any way the Conservatives would vote for it, and the majority is slender as it is, as the DUP is bitterly against it, and I can't see the Lib Dems voting for it, so it will only be if there are enough, what I can describe as remoaner MPs, that the deal won't be dead in the water.\n",
    "Sputnik: What could be a solution to the political chaos if the Prime Minister's deal is not approved?\n",
    "Michael Swadling: The EU withdrawal act is in place; we'll leave and revert to WTO terms and that works, that's fine.\n",
    "I often use the example of an iPhone to people; that's a piece of technology which is manufactured in China, uses American technology and these are two countries we deal with on WTO terms, this isn't a fantasy, stuck in a port somewhere, there isn't a massive tariff, this is the world that really exists today.\n",
    "When we exit the EU on WTO terms; that will be fine for whatever trading we do with the EU, just as well as it does for our trade in China.READ MORE: UK Finance Chief Bashed for Failing to Unlock Money for No-Deal Brexit — Reports\n",
    "Sputnik: Do you think that the EU needs the UK more than the UK needs the EU?\n",
    "Michael Swadling: The EU makes a profit on its trade with the UK; the UK makes a loss on its trade with the EU.\n",
    "They have a financial incentive to ensure that good trading relations continue far more than we do.\n",
    "© REUTERS / Toby Melville UK Trade Minister Says '50-50' Chance Brexit Will Not Happen – Reports\n",
    "The lifeblood and cash flow that keeps manufacturing in Europe going, comes from the city of London.\n",
    "If someone in a city in Germany wants to do a deal with someone in Japan; the financial services of that are probably going through the city of London, they're not going through Frankfurt and Paris.\n",
    "Views and opinions, expressed in the article are those of Michael Swadling and do not necessarily reflect those of Sputnik\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "test_article = get_article_dataloader(article, tokenizer)\n",
    "predict(model, test_article, y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "free_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05c4f9fa65704ea5ba7f80e879e08510": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f54181f857a4110bf9976a56c97de97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fbfe002ccd541f9b6ab62ed4eebef35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1048f064e69b46d497b65cb9b88d0142": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc871ed94e764fe884ffdafca1445bfe",
       "IPY_MODEL_b4e7e77911e5408b84d89ccaff134708",
       "IPY_MODEL_c322e99ad9504207b7cfb9ae2ca9d6b1"
      ],
      "layout": "IPY_MODEL_dd92db5892be4357a6446146d9e9a0dd"
     }
    },
    "145c8616bab245358c8052beac5bd2bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20d6b378a69744e4a327d929c391b475": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "364a4257f8a641e18b7bde39e63a618d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "409a73804fc34a63a619a42b9468be46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "475020323c07410e87d366a6f553aafb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_145c8616bab245358c8052beac5bd2bc",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca901348dcd1428ebecc55d659fa40d1",
      "value": 28
     }
    },
    "4ddbb3fa3b924a9e9650f51204580f8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a5458551f974a94b61ff658ae59fa70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b48b8b79d6a4ff1b0655a6bfcf7a422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c06d626b5ee4fe8bdb9be7fb879aae0",
      "placeholder": "​",
      "style": "IPY_MODEL_779202125255413cb719a43fe5d14ce7",
      "value": "Downloading (…)solve/main/vocab.txt: 100%"
     }
    },
    "5d40c6e034fd4b278969dd928cc07dff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f4b55ead6884f3790a3acfa43232fc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87907508c4eb43f3a27858f1d397ca99",
      "placeholder": "​",
      "style": "IPY_MODEL_d21ed2e527464a0ca0cb2b3b4fad928a",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "650f8e0f82e849f58a47507291b15500": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbb672669b9f4c36873048cfa1d2daf6",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cf42b39e3be94c90b3a63d05783481a3",
      "value": 231508
     }
    },
    "67ea6145091b440da6b4c5d5f8695aa9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ac495a816994aaeb8926c6c97d103ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ddbb3fa3b924a9e9650f51204580f8d",
      "placeholder": "​",
      "style": "IPY_MODEL_a115ed03534a4c64a17fa55d93a0f93a",
      "value": " 570/570 [00:00&lt;00:00, 47.0kB/s]"
     }
    },
    "71add6bf108545af8db6aeb392793759": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5f4b55ead6884f3790a3acfa43232fc9",
       "IPY_MODEL_475020323c07410e87d366a6f553aafb",
       "IPY_MODEL_e2454ec2f5904e0bacde56ae7d4089a9"
      ],
      "layout": "IPY_MODEL_0f54181f857a4110bf9976a56c97de97"
     }
    },
    "779202125255413cb719a43fe5d14ce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87907508c4eb43f3a27858f1d397ca99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c06d626b5ee4fe8bdb9be7fb879aae0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e13dccd922946ab873cc287fcca5baf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93fa55d130db463c8ddffc947f2e99cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a6d16261d5b4693b09acf83dcb38920": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0265a21ae504485ab59207228c1f6d6",
      "placeholder": "​",
      "style": "IPY_MODEL_8e13dccd922946ab873cc287fcca5baf",
      "value": " 232k/232k [00:00&lt;00:00, 9.51MB/s]"
     }
    },
    "a0265a21ae504485ab59207228c1f6d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a115ed03534a4c64a17fa55d93a0f93a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a74bacc051494d1ea0f4cbd656505cfe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4e7e77911e5408b84d89ccaff134708": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67ea6145091b440da6b4c5d5f8695aa9",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bef37b47c8ee49778464c05adcffa30d",
      "value": 466062
     }
    },
    "b83493088cd24629a04ec612aa522ed4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c222d2a5e4664f9b9aefed7f093d4a4e",
       "IPY_MODEL_fa94f90b735f418fb2b502f7877b5306",
       "IPY_MODEL_6ac495a816994aaeb8926c6c97d103ae"
      ],
      "layout": "IPY_MODEL_409a73804fc34a63a619a42b9468be46"
     }
    },
    "bef37b47c8ee49778464c05adcffa30d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c222d2a5e4664f9b9aefed7f093d4a4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_364a4257f8a641e18b7bde39e63a618d",
      "placeholder": "​",
      "style": "IPY_MODEL_93fa55d130db463c8ddffc947f2e99cd",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "c322e99ad9504207b7cfb9ae2ca9d6b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d40c6e034fd4b278969dd928cc07dff",
      "placeholder": "​",
      "style": "IPY_MODEL_f6b3c67d61114db5810bd78339a218d9",
      "value": " 466k/466k [00:00&lt;00:00, 1.88MB/s]"
     }
    },
    "ca901348dcd1428ebecc55d659fa40d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cf42b39e3be94c90b3a63d05783481a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d21ed2e527464a0ca0cb2b3b4fad928a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d568d9b894694b6fb432456031cee230": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbb672669b9f4c36873048cfa1d2daf6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd92db5892be4357a6446146d9e9a0dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2454ec2f5904e0bacde56ae7d4089a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20d6b378a69744e4a327d929c391b475",
      "placeholder": "​",
      "style": "IPY_MODEL_05c4f9fa65704ea5ba7f80e879e08510",
      "value": " 28.0/28.0 [00:00&lt;00:00, 1.97kB/s]"
     }
    },
    "e9ee9acc5e414c65ad10b8864cc07b07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f19c187b1e8c4bbe9fda366f24f2b79e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5b48b8b79d6a4ff1b0655a6bfcf7a422",
       "IPY_MODEL_650f8e0f82e849f58a47507291b15500",
       "IPY_MODEL_9a6d16261d5b4693b09acf83dcb38920"
      ],
      "layout": "IPY_MODEL_d568d9b894694b6fb432456031cee230"
     }
    },
    "f6b3c67d61114db5810bd78339a218d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa94f90b735f418fb2b502f7877b5306": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a74bacc051494d1ea0f4cbd656505cfe",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0fbfe002ccd541f9b6ab62ed4eebef35",
      "value": 570
     }
    },
    "fc871ed94e764fe884ffdafca1445bfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a5458551f974a94b61ff658ae59fa70",
      "placeholder": "​",
      "style": "IPY_MODEL_e9ee9acc5e414c65ad10b8864cc07b07",
      "value": "Downloading (…)/main/tokenizer.json: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
