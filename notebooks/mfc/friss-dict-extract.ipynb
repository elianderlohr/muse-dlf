{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Waumhn_ldoGu"
   },
   "source": [
    "## FRISS with MFC\n",
    "\n",
    "Implementation of the FRISS using the Media Frames Corpus (MFC) from Card et al. (2015). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FRISS_srl.pkl',\n",
       " 'README.md',\n",
       " 'logs',\n",
       " 'used_labels_a1.npy',\n",
       " 'notebooks',\n",
       " 'FRISS_SRL_unlabeled.pkl',\n",
       " 'chunks.pkl',\n",
       " 'grid_search_metrics.csv',\n",
       " 'predicted_labels.npy',\n",
       " '.git',\n",
       " 'used_labels_p.npy',\n",
       " 'results',\n",
       " 'assets',\n",
       " 'friss',\n",
       " 'models',\n",
       " 'used_labels_a0.npy',\n",
       " '.ipynb_checkpoints',\n",
       " 'data',\n",
       " '.gitignore',\n",
       " 'frameaxis']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_path = \"data/mfc/immigration_labeled.json\"\n",
    "unlabeld_path = \"data/mfc/immigration_unlabeled.json\"\n",
    "codes_path = \"data/mfc/codes.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from path \n",
    "import json\n",
    "\n",
    "with open(labeled_path) as f:\n",
    "    labeled = json.load(f)\n",
    "\n",
    "with open(unlabeld_path) as f:\n",
    "    unlabeld = json.load(f)\n",
    "\n",
    "with open(codes_path) as f:\n",
    "    codes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled_data(labeled, codes):\n",
    "    # articles list\n",
    "    articles_list = []\n",
    "\n",
    "    # Iterate through the data to fill the DataFrame\n",
    "    for article_id, article_data in labeled.items():\n",
    "        annotations_data = article_data['annotations']\n",
    "\n",
    "        irrelevant_dict = annotations_data['irrelevant']\n",
    "\n",
    "        text = article_data['text']\n",
    "        irrelevant = article_data['irrelevant']\n",
    "\n",
    "        # if primary_frame is none set to 15.0\n",
    "        if article_data['primary_frame'] is not None:\n",
    "            primary_frame = str(article_data['primary_frame']).split(\".\")[0] + \".0\"\n",
    "        else:\n",
    "            primary_frame = \"15.0\"\n",
    "\n",
    "        # get primary frame from code\n",
    "        primary_frame = str(codes[primary_frame])\n",
    "\n",
    "        # split text into sentences using nltk library\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        # iterate through sentences\n",
    "        for sentence in sentences:\n",
    "            article = {\n",
    "                'article_id': article_id,\n",
    "                'irrelevant': irrelevant,\n",
    "                'text': sentence,\n",
    "                'document_frame': primary_frame\n",
    "            }\n",
    "\n",
    "            articles_list.append(article)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    df = pd.DataFrame(articles_list, columns=['article_id', 'irrelevant', 'text', 'document_frame'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unlabeled_data(unlabeled):\n",
    "    # articles list\n",
    "    articles_list = []\n",
    "\n",
    "    for idx, article in enumerate(unlabeled):\n",
    "        article_id = f\"unlabeled_{idx}\" \n",
    "        text = article['text']\n",
    "\n",
    "        # split text into sentences using nltk library\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        # iterate through sentences\n",
    "        for sentence in sentences:\n",
    "            article = {\n",
    "                'article_id': article_id,\n",
    "                'text': sentence\n",
    "            }\n",
    "\n",
    "            articles_list.append(article)\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    df = pd.DataFrame(articles_list, columns=['article_id', 'text'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get labeled and unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled Count:  74468\n",
      "Unlabeled Count:  460535\n"
     ]
    }
   ],
   "source": [
    "df_labeled = get_labeled_data(labeled, codes)\n",
    "df_unlabeled = get_unlabeled_data(unlabeld)\n",
    "\n",
    "print(\"Labeled Count: \", len(df_labeled))\n",
    "print(\"Unlabeled Count: \", len(df_unlabeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labeled_df(df):\n",
    "    df = df[df[\"irrelevant\"] == False][[\"article_id\", \"text\", \"document_frame\"]]\n",
    "\n",
    "    # create for each code a col and fill with 1 if code is in code col\n",
    "    df = pd.concat([df, pd.get_dummies(df['document_frame'])], axis=1)  \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1296
    },
    "executionInfo": {
     "elapsed": 3772,
     "status": "ok",
     "timestamp": 1696624002536,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "DG_Xix7gdoGy",
    "outputId": "d6fad26e-e6f7-4c20-f4bb-c7b01d51eb33",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_labeled = preprocess_labeled_df(df_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>text</th>\n",
       "      <th>document_frame</th>\n",
       "      <th>Capacity and Resources</th>\n",
       "      <th>Crime and Punishment</th>\n",
       "      <th>Cultural Identity</th>\n",
       "      <th>Economic</th>\n",
       "      <th>External Regulation and Reputation</th>\n",
       "      <th>Fairness and Equality</th>\n",
       "      <th>Health and Safety</th>\n",
       "      <th>Legality, Constitutionality, Jurisdiction</th>\n",
       "      <th>Morality</th>\n",
       "      <th>Other</th>\n",
       "      <th>Policy Prescription and Evaluation</th>\n",
       "      <th>Political</th>\n",
       "      <th>Public Sentiment</th>\n",
       "      <th>Quality of Life</th>\n",
       "      <th>Security and Defense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>IMM-10005\\n\\nPRIMARY\\n\\nImmigrants without HOP...</td>\n",
       "      <td>Quality of Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>It mounted as students went around the room te...</td>\n",
       "      <td>Quality of Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>Georgia Tech.</td>\n",
       "      <td>Quality of Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>University of Georgia.</td>\n",
       "      <td>Quality of Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>\"All I could say was, 'I'm planning to see if ...</td>\n",
       "      <td>Quality of Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             article_id                                               text  \\\n",
       "0  Immigration1.0-10005  IMM-10005\\n\\nPRIMARY\\n\\nImmigrants without HOP...   \n",
       "1  Immigration1.0-10005  It mounted as students went around the room te...   \n",
       "2  Immigration1.0-10005                                      Georgia Tech.   \n",
       "3  Immigration1.0-10005                             University of Georgia.   \n",
       "4  Immigration1.0-10005  \"All I could say was, 'I'm planning to see if ...   \n",
       "\n",
       "    document_frame  Capacity and Resources  Crime and Punishment  \\\n",
       "0  Quality of Life                       0                     0   \n",
       "1  Quality of Life                       0                     0   \n",
       "2  Quality of Life                       0                     0   \n",
       "3  Quality of Life                       0                     0   \n",
       "4  Quality of Life                       0                     0   \n",
       "\n",
       "   Cultural Identity  Economic  External Regulation and Reputation  \\\n",
       "0                  0         0                                   0   \n",
       "1                  0         0                                   0   \n",
       "2                  0         0                                   0   \n",
       "3                  0         0                                   0   \n",
       "4                  0         0                                   0   \n",
       "\n",
       "   Fairness and Equality  Health and Safety  \\\n",
       "0                      0                  0   \n",
       "1                      0                  0   \n",
       "2                      0                  0   \n",
       "3                      0                  0   \n",
       "4                      0                  0   \n",
       "\n",
       "   Legality, Constitutionality, Jurisdiction  Morality  Other  \\\n",
       "0                                          0         0      0   \n",
       "1                                          0         0      0   \n",
       "2                                          0         0      0   \n",
       "3                                          0         0      0   \n",
       "4                                          0         0      0   \n",
       "\n",
       "   Policy Prescription and Evaluation  Political  Public Sentiment  \\\n",
       "0                                   0          0                 0   \n",
       "1                                   0          0                 0   \n",
       "2                                   0          0                 0   \n",
       "3                                   0          0                 0   \n",
       "4                                   0          0                 0   \n",
       "\n",
       "   Quality of Life  Security and Defense  \n",
       "0                1                     0  \n",
       "1                1                     0  \n",
       "2                1                     0  \n",
       "3                1                     0  \n",
       "4                1                     0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unlabeled_0</td>\n",
       "      <td>IMM-10000\\n\\nPRIMARY\\n\\nMetro Briefing New Yor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unlabeled_0</td>\n",
       "      <td>As part of the scheme, Ms. Holzer convinced Sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unlabeled_0</td>\n",
       "      <td>Katherine E. Finkelstein (NYT)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unlabeled_1</td>\n",
       "      <td>IMM-10003\\n\\nPRIMARY\\n\\nAmnesty Works for Amer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unlabeled_1</td>\n",
       "      <td>All working families would benefit from immigr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    article_id                                               text\n",
       "0  unlabeled_0  IMM-10000\\n\\nPRIMARY\\n\\nMetro Briefing New Yor...\n",
       "1  unlabeled_0  As part of the scheme, Ms. Holzer convinced Sp...\n",
       "2  unlabeled_0                     Katherine E. Finkelstein (NYT)\n",
       "3  unlabeled_1  IMM-10003\\n\\nPRIMARY\\n\\nAmnesty Works for Amer...\n",
       "4  unlabeled_1  All working families would benefit from immigr..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unlabeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67480, 18), (460535, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled.shape, df_unlabeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,\n",
       " Index(['Capacity and Resources', 'Crime and Punishment', 'Cultural Identity',\n",
       "        'Economic', 'External Regulation and Reputation',\n",
       "        'Fairness and Equality', 'Health and Safety',\n",
       "        'Legality, Constitutionality, Jurisdiction', 'Morality', 'Other',\n",
       "        'Policy Prescription and Evaluation', 'Political', 'Public Sentiment',\n",
       "        'Quality of Life', 'Security and Defense'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_columns = df_labeled.columns[3:]\n",
    "len(y_columns), y_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract SRL Embeddings from articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycuda in /usr/local/lib/python3.9/dist-packages (2023.1)\n",
      "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from pycuda) (1.4.4)\n",
      "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.9/dist-packages (from pycuda) (2023.1.1)\n",
      "Requirement already satisfied: mako in /usr/local/lib/python3.9/dist-packages (from pycuda) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.9/dist-packages (from pytools>=2011.2->pycuda) (4.4.0)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from pytools>=2011.2->pycuda) (2.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from mako->pycuda) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: allennlp in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
      "Requirement already satisfied: allennlp-models in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
      "Requirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.9/dist-packages (from allennlp) (3.7)\n",
      "Requirement already satisfied: lmdb>=1.2.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.4.1)\n",
      "Requirement already satisfied: jsonnet>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.20.0)\n",
      "Requirement already satisfied: base58>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (2.1.1)\n",
      "Requirement already satisfied: torch<1.13.0,>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.12.1+cu116)\n",
      "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.9/dist-packages (from allennlp) (2.28.2)\n",
      "Requirement already satisfied: torchvision<0.14.0,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.13.1+cu116)\n",
      "Requirement already satisfied: wandb<0.13.0,>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.12.21)\n",
      "Requirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (10.1.0)\n",
      "Requirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.23.4)\n",
      "Requirement already satisfied: fairscale==0.4.6 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.4.6)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.1.1)\n",
      "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.9/dist-packages (from allennlp) (7.2.1)\n",
      "Requirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (5.8.1)\n",
      "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.3.5.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.1.2)\n",
      "Requirement already satisfied: spacy<3.4,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (3.3.3)\n",
      "Requirement already satisfied: transformers<4.21,>=4.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (4.20.1)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.10.1)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.9.2)\n",
      "Requirement already satisfied: sentencepiece>=0.1.96 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.1.97)\n",
      "Requirement already satisfied: cached-path<1.2.0,>=1.1.3 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.1.6)\n",
      "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.1.0)\n",
      "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.9/dist-packages (from allennlp) (4.64.1)\n",
      "Requirement already satisfied: filelock<3.8,>=3.3 in /usr/local/lib/python3.9/dist-packages (from allennlp) (3.7.1)\n",
      "Requirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (3.8.0)\n",
      "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.9/dist-packages (from allennlp) (2.6.2.2)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.9/dist-packages (from allennlp-models) (6.1.3)\n",
      "Requirement already satisfied: py-rouge==1.1 in /usr/local/lib/python3.9/dist-packages (from allennlp-models) (1.1)\n",
      "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.9/dist-packages (from allennlp-models) (1.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (from allennlp-models) (2.4.0)\n",
      "Requirement already satisfied: conllu==4.4.2 in /usr/local/lib/python3.9/dist-packages (from allennlp-models) (4.4.2)\n",
      "Requirement already satisfied: rich<13.0,>=12.1 in /usr/local/lib/python3.9/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (12.6.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (1.24.90)\n",
      "Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.9/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (2.13.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.0.16->allennlp) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.0.16->allennlp) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.0.16->allennlp) (4.4.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk>=3.6.5->allennlp) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk>=3.6.5->allennlp) (2022.10.31)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk>=3.6.5->allennlp) (8.1.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (1.1.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (1.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (23.1.0)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (2.0.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.28->allennlp) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.28->allennlp) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.28->allennlp) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.28->allennlp) (2.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.1->allennlp) (3.1.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (6.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.8)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (66.1.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.9)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (8.0.17)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (9.2.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<4.21,>=4.1->allennlp) (0.12.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.14.0)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.1.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.0.11)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.14.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.4.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (3.1.30)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.9.4)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.3.2)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (10.0.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (3.8.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (2023.1.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (0.70.13)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (1.5.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (3.2.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (0.18.0)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.9/dist-packages (from ftfy->allennlp-models) (0.2.12)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.90 in /usr/local/lib/python3.9/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.27.90)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (6.0.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (4.0.10)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.4.1)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.23.3 in /usr/local/lib/python3.9/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.25.2)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.6.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.5.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.9/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.15.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.14.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (0.9.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->allennlp-models) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->allennlp-models) (2.8.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (5.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.62.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.2.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.4.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pycuda\n",
    "!pip install allennlp allennlp-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadro P5000\n"
     ]
    }
   ],
   "source": [
    "# get name / id of cuda device\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "cuda.init()\n",
    "device = cuda.Device(0)\n",
    "print(device.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def batched_extract_srl_components(batched_sentences, predictor):\n",
    "    # Convert each sentence into the required format for the predictor\n",
    "    batched_sentences = [{'sentence': sentence} for sentence in batched_sentences]\n",
    "\n",
    "    # Prepare the batched input for the predictor\n",
    "    batched_srl = predictor.predict_batch_json(batched_sentences)\n",
    "\n",
    "    # Extract SRL components from the batched predictions\n",
    "    results = []\n",
    "    for index, srl in enumerate(batched_srl):\n",
    "        sentence_results = []\n",
    "        for verb_entry in srl['verbs']:\n",
    "            arg_components = {'ARG0': [], 'ARG1': []}\n",
    "            for i, tag in enumerate(verb_entry['tags']):\n",
    "                if 'ARG0' in tag:\n",
    "                    arg_components['ARG0'].append(srl['words'][i])\n",
    "                elif 'ARG1' in tag:\n",
    "                    arg_components['ARG1'].append(srl['words'][i])\n",
    "\n",
    "            if arg_components['ARG0'] or arg_components['ARG1']:\n",
    "                sentence_results.append({\n",
    "                    'predicate': verb_entry['verb'],\n",
    "                    'ARG0': ' '.join(arg_components['ARG0']),\n",
    "                    'ARG1': ' '.join(arg_components['ARG1'])\n",
    "                })\n",
    "\n",
    "        if sentence_results:\n",
    "            # add empty dict if predicate, arg0 or arg1 is empty\n",
    "            if not sentence_results[0]['predicate']:\n",
    "                results.append({'predicate': '', 'ARG0': '', 'ARG1': ''})\n",
    "            elif not sentence_results[0]['ARG0']:\n",
    "                results.append({'predicate': '', 'ARG0': '', 'ARG1': ''})\n",
    "            elif not sentence_results[0]['ARG1']:\n",
    "                results.append({'predicate': '', 'ARG0': '', 'ARG1': ''})\n",
    "            else:\n",
    "                results.append(sentence_results)    \n",
    "        else:\n",
    "            results.append([{'predicate': '', 'ARG0': '', 'ARG1': ''}])\n",
    "\n",
    "    return results\n",
    "\n",
    "def optimized_extract_srl(X, predictor, batch_size=32):\n",
    "    all_results = []\n",
    "\n",
    "    # Process sentences in batches\n",
    "    for i in tqdm(range(0, len(X), batch_size), desc=\"Processing Batches\"):\n",
    "        batched_sentences = X[i:i+batch_size]\n",
    "\n",
    "        batch_results = batched_extract_srl_components(batched_sentences, predictor)\n",
    "\n",
    "        all_results.extend(batch_results)\n",
    "\n",
    "    return pd.Series(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_X_srl(X, recalculate=False, pickle_path=\"../notebooks/classifier/X_srl_filtered.pkl\", _save = True):\n",
    "    \"\"\"\n",
    "    Returns the X_srl either by loading from a pickled file or recalculating.\n",
    "    \"\"\"\n",
    "    if recalculate or not os.path.exists(pickle_path):\n",
    "        print(\"Recalculate SRL\")\n",
    "        # Load predictor\n",
    "        predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\", cuda_device=0)\n",
    "\n",
    "        # make sentences max 480 chars long\n",
    "        X = X.apply(lambda x: x[:480])\n",
    "\n",
    "        X_srl = optimized_extract_srl(X, predictor, batch_size=32)\n",
    "\n",
    "        if _save:\n",
    "            print(\"Save SRL to Pickle\")\n",
    "            with open(pickle_path, 'wb') as f:\n",
    "                pickle.dump(X_srl, f)\n",
    "    else:\n",
    "        print(\"Load SRL from Pickle\")\n",
    "        with tqdm(total=os.path.getsize(pickle_path)) as pbar:\n",
    "            with open(pickle_path, 'rb') as f:\n",
    "                X_srl = pickle.load(f)\n",
    "                pbar.update(os.path.getsize(pickle_path))\n",
    "                \n",
    "    return X_srl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_X_srl(df_unlabeled[\"text\"], recalculate=True, pickle_path=\"../notebooks/FRISS_SRL_unlabeled.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def free_gpu():\n",
    "    print(torch.cuda.mem_get_info())\n",
    "    print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/distributed/distributed_c10d.py:181: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def list_gpu_tensors():\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                if obj.is_cuda:\n",
    "                    obj = obj.cpu()\n",
    "                    obj = obj.to(\"cpu\")\n",
    "                    print(type(obj), obj.size())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "list_gpu_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "class ArticleDataset(Dataset):\n",
    "    def __init__(self, X, X_srl, tokenizer, labels=None, max_sentences_per_article=32, max_sentence_length=32, max_args_per_sentence=10, max_arg_length=16, _ignore_y=False):\n",
    "        self.X = X  # DataFrame where each row has multiple sentences\n",
    "        self.X_srl = X_srl  # DataFrame where each row has multiple dictionaries for SRL\n",
    "        self.labels = labels  # DataFrame where each row has a list of lists of integers\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sentences_per_article = max_sentences_per_article\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.max_args_per_sentence = max_args_per_sentence\n",
    "        self.max_arg_length = max_arg_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentences = self.X.iloc[idx]\n",
    "        srl_data = self.X_srl.iloc[idx]\n",
    "\n",
    "        labels = None\n",
    "        if self.labels is False:\n",
    "            labels = self.labels.iloc[idx]\n",
    "        else:\n",
    "            # placeholder for labels\n",
    "            labels = [[0] * 15]\n",
    "\n",
    "        # Tokenize sentences and get attention masks\n",
    "        sentence_ids, sentence_attention_masks = [], []\n",
    "        for sentence in sentences:\n",
    "            encoded = self.tokenizer(sentence, add_special_tokens=True, max_length=self.max_sentence_length, truncation=True, padding='max_length', return_attention_mask=True)\n",
    "            sentence_ids.append(encoded['input_ids'])\n",
    "            sentence_attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "        # Padding for sentences if necessary\n",
    "        while len(sentence_ids) < self.max_sentences_per_article:\n",
    "            sentence_ids.append([0] * self.max_sentence_length)\n",
    "            sentence_attention_masks.append([0] * self.max_sentence_length)\n",
    "\n",
    "        sentence_ids = sentence_ids[:self.max_sentences_per_article]\n",
    "        sentence_attention_masks = sentence_attention_masks[:self.max_sentences_per_article]\n",
    "\n",
    "        # Process SRL data\n",
    "        predicates, arg0s, arg1s = [], [], []\n",
    "        predicate_attention_masks, arg0_attention_masks, arg1_attention_masks = [], [], []\n",
    "        for srl_items in srl_data:\n",
    "            sentence_predicates, sentence_arg0s, sentence_arg1s = [], [], []\n",
    "            sentence_predicate_masks, sentence_arg0_masks, sentence_arg1_masks = [], [], []\n",
    "\n",
    "            if not isinstance(srl_items, list):\n",
    "                srl_items = [srl_items]\n",
    "\n",
    "            for item in srl_items:\n",
    "                encoded_predicate = self.tokenizer(item[\"predicate\"], add_special_tokens=True, max_length=self.max_arg_length, truncation=True, padding='max_length', return_attention_mask=True)\n",
    "                encoded_arg0 = self.tokenizer(item[\"ARG0\"], add_special_tokens=True, max_length=self.max_arg_length, truncation=True, padding='max_length', return_attention_mask=True)\n",
    "                encoded_arg1 = self.tokenizer(item[\"ARG1\"], add_special_tokens=True, max_length=self.max_arg_length, truncation=True, padding='max_length', return_attention_mask=True)\n",
    "\n",
    "                sentence_predicates.append(encoded_predicate['input_ids'])\n",
    "                sentence_arg0s.append(encoded_arg0['input_ids'])\n",
    "                sentence_arg1s.append(encoded_arg1['input_ids'])\n",
    "\n",
    "                sentence_predicate_masks.append(encoded_predicate['attention_mask'])\n",
    "                sentence_arg0_masks.append(encoded_arg0['attention_mask'])\n",
    "                sentence_arg1_masks.append(encoded_arg1['attention_mask'])\n",
    "\n",
    "            # Padding for SRL elements\n",
    "            for _ in range(self.max_args_per_sentence):\n",
    "                sentence_predicates.append([0] * self.max_arg_length)\n",
    "                sentence_arg0s.append([0] * self.max_arg_length)\n",
    "                sentence_arg1s.append([0] * self.max_arg_length)\n",
    "\n",
    "                sentence_predicate_masks.append([0] * self.max_arg_length)\n",
    "                sentence_arg0_masks.append([0] * self.max_arg_length)\n",
    "                sentence_arg1_masks.append([0] * self.max_arg_length)\n",
    "\n",
    "            sentence_predicates = sentence_predicates[:self.max_args_per_sentence]\n",
    "            sentence_arg0s = sentence_arg0s[:self.max_args_per_sentence]\n",
    "            sentence_arg1s = sentence_arg1s[:self.max_args_per_sentence]\n",
    "\n",
    "            sentence_predicate_masks = sentence_predicate_masks[:self.max_args_per_sentence]\n",
    "            sentence_arg0_masks = sentence_arg0_masks[:self.max_args_per_sentence]\n",
    "            sentence_arg1_masks = sentence_arg1_masks[:self.max_args_per_sentence]\n",
    "\n",
    "            predicates.append(sentence_predicates)\n",
    "            arg0s.append(sentence_arg0s)\n",
    "            arg1s.append(sentence_arg1s)\n",
    "\n",
    "            predicate_attention_masks.append(sentence_predicate_masks)\n",
    "            arg0_attention_masks.append(sentence_arg0_masks)\n",
    "            arg1_attention_masks.append(sentence_arg1_masks)\n",
    "\n",
    "        # Padding for SRL data\n",
    "        srl_padding = [[0] * self.max_arg_length] * self.max_args_per_sentence\n",
    "        mask_padding = [[0] * self.max_arg_length] * self.max_args_per_sentence\n",
    "\n",
    "        predicates = (predicates + [srl_padding] * self.max_sentences_per_article)[:self.max_sentences_per_article]\n",
    "        arg0s = (arg0s + [srl_padding] * self.max_sentences_per_article)[:self.max_sentences_per_article]\n",
    "        arg1s = (arg1s + [srl_padding] * self.max_sentences_per_article)[:self.max_sentences_per_article]\n",
    "\n",
    "        predicate_attention_masks = (predicate_attention_masks + [mask_padding] * self.max_sentences_per_article)[:self.max_sentences_per_article]\n",
    "        arg0_attention_masks = (arg0_attention_masks + [mask_padding] * self.max_sentences_per_article)[:self.max_sentences_per_article]\n",
    "        arg1_attention_masks = (arg1_attention_masks + [mask_padding] * self.max_sentences_per_article)[:self.max_sentences_per_article]\n",
    "\n",
    "        data = {\n",
    "            'sentence_ids': torch.tensor(sentence_ids, dtype=torch.long),\n",
    "            'sentence_attention_masks': torch.tensor(sentence_attention_masks, dtype=torch.long),\n",
    "            'predicate_ids': torch.tensor(predicates, dtype=torch.long),\n",
    "            'predicate_attention_masks': torch.tensor(predicate_attention_masks, dtype=torch.long),\n",
    "            'arg0_ids': torch.tensor(arg0s, dtype=torch.long),\n",
    "            'arg0_attention_masks': torch.tensor(arg0_attention_masks, dtype=torch.long),\n",
    "            'arg1_ids': torch.tensor(arg1s, dtype=torch.long),\n",
    "            'arg1_attention_masks': torch.tensor(arg1_attention_masks, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels[0], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Extract individual lists from the batch\n",
    "    sentence_ids = [item['sentence_ids'] for item in batch]\n",
    "    sentence_attention_masks = [item['sentence_attention_masks'] for item in batch]\n",
    "    predicate_ids = [item['predicate_ids'] for item in batch]\n",
    "    predicate_attention_masks = [item['predicate_attention_masks'] for item in batch]\n",
    "    arg0_ids = [item['arg0_ids'] for item in batch]\n",
    "    arg0_attention_masks = [item['arg0_attention_masks'] for item in batch]\n",
    "    arg1_ids = [item['arg1_ids'] for item in batch]\n",
    "    arg1_attention_masks = [item['arg1_attention_masks'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Pad each list\n",
    "    sentence_ids = torch.nn.utils.rnn.pad_sequence(sentence_ids, batch_first=True, padding_value=0)\n",
    "    sentence_attention_masks = torch.nn.utils.rnn.pad_sequence(sentence_attention_masks, batch_first=True, padding_value=0)\n",
    "    predicate_ids = torch.nn.utils.rnn.pad_sequence(predicate_ids, batch_first=True, padding_value=0)\n",
    "    predicate_attention_masks = torch.nn.utils.rnn.pad_sequence(predicate_attention_masks, batch_first=True, padding_value=0)\n",
    "    arg0_ids = torch.nn.utils.rnn.pad_sequence(arg0_ids, batch_first=True, padding_value=0)\n",
    "    arg0_attention_masks = torch.nn.utils.rnn.pad_sequence(arg0_attention_masks, batch_first=True, padding_value=0)\n",
    "    arg1_ids = torch.nn.utils.rnn.pad_sequence(arg1_ids, batch_first=True, padding_value=0)\n",
    "    arg1_attention_masks = torch.nn.utils.rnn.pad_sequence(arg1_attention_masks, batch_first=True, padding_value=0)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Create the output dictionary\n",
    "    output_dict = {\n",
    "        'sentence_ids': sentence_ids,\n",
    "        'sentence_attention_masks': sentence_attention_masks,\n",
    "        'predicate_ids': predicate_ids,\n",
    "        'predicate_attention_masks': predicate_attention_masks,\n",
    "        'arg0_ids': arg0_ids,\n",
    "        'arg0_attention_masks': arg0_attention_masks,\n",
    "        'arg1_ids': arg1_ids,\n",
    "        'arg1_attention_masks': arg1_attention_masks,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def preprocess_df(df, recalculate_srl=False, pickle_path=\"../notebooks/FRISS_srl.pkl\", _ignore_y=False, _save = False):\n",
    "    # reset index of df\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Get X_srl\n",
    "    X_srl = get_X_srl(df[\"text\"], recalculate=recalculate_srl, pickle_path=pickle_path, _save = _save)\n",
    "\n",
    "    # reset index of X_srl\n",
    "    X_srl = X_srl.reset_index(drop=True)\n",
    "\n",
    "    # Aggregating 'text' column in df into a list of strings for each article_id\n",
    "    X_subset = df.groupby('article_id')['text'].apply(list).reset_index(name='text')\n",
    "    X_subset = X_subset['text']\n",
    "\n",
    "    # Assuming X_srl follows the same index order as df\n",
    "    X_srl_subset = X_srl.groupby(df['article_id']).apply(lambda x: x.values.tolist()).reset_index(name='srl_values')\n",
    "    X_srl_subset = X_srl_subset['srl_values']\n",
    "\n",
    "    if not _ignore_y:\n",
    "        # Columns to be one-hot encoded in y_subset\n",
    "        y_cols = ['Capacity and Resources', 'Crime and Punishment', 'Cultural Identity', \n",
    "                'Economic', 'External Regulation and Reputation', 'Fairness and Equality', \n",
    "                'Health and Safety', 'Legality, Constitutionality, Jurisdiction', \n",
    "                'Morality', 'Other', 'Policy Prescription and Evaluation', 'Political', \n",
    "                'Public Sentiment', 'Quality of Life', 'Security and Defense']\n",
    "\n",
    "        # Creating y_subset\n",
    "        y_subset = df.groupby('article_id')[y_cols].apply(lambda x: x.values.tolist()).reset_index(name='encoded_values')\n",
    "        y_subset = y_subset['encoded_values']\n",
    "    else:\n",
    "        # create empty y_subset\n",
    "        y_subset = [0] * len(X_subset)\n",
    "\n",
    "    return X_subset, X_srl_subset, y_subset\n",
    "\n",
    "def get_training_datasets(df, tokenizer, recalculate_srl=False, pickle_path=\"../notebooks/FRISS_srl.pkl\", batch_size=16, max_sentences_per_article=32, max_sentence_length=32,  max_args_per_sentence=10,  max_arg_length=16, test_size=0.1, _ignore_y=False):\n",
    "    print(\"IGNORE Y:\", _ignore_y)\n",
    "    X_subset, X_srl_subset, y_subset = preprocess_df(df, recalculate_srl=recalculate_srl, pickle_path=pickle_path, _ignore_y=_ignore_y)\n",
    "\n",
    "    # Len\n",
    "    print(\"X:\", len(X_subset))\n",
    "    print(\"X_srl:\", len(X_srl_subset))\n",
    "    print(\"y:\", len(y_subset))\n",
    "\n",
    "    print(\"CREATING DATASETS\")\n",
    "    \n",
    "    # Assuming X, X_srl, and y are already defined and have the same number of samples\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_subset, y_subset, test_size=test_size, random_state=42)\n",
    "    \n",
    "    print(\"TRAIN TEST SPLIT DONE\")\n",
    "    \n",
    "    X_srl_train, X_srl_test, _, _ = train_test_split(X_srl_subset, y_subset, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Create the dataset\n",
    "    train_dataset = ArticleDataset(X_train, X_srl_train, tokenizer, y_train, max_sentences_per_article, max_sentence_length,  max_args_per_sentence, max_arg_length)\n",
    "    test_dataset = ArticleDataset(X_test, X_srl_test, tokenizer, y_test, max_sentences_per_article, max_sentence_length, max_args_per_sentence, max_arg_length)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "    \n",
    "    print(\"CREATION DONE\")\n",
    "    return train_dataset, test_dataset , train_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "def get_testing_dataset(df, tokenizer, recalculate_srl=False, pickle_path=\"../notebooks/FRISS_srl.pkl\", batch_size=16, max_sentences_per_article=32, max_sentence_length=32,  max_args_per_sentence=10, max_arg_length=16, sample_size=1000):\n",
    "    \"\"\"GET Datasets to test dict extraction\"\"\"\n",
    "    print(\"Get Testing Datasets for testing trained model\")\n",
    "    print()\n",
    "    X_subset, X_srl_subset, y_subset = preprocess_df(df, recalculate_srl=recalculate_srl, pickle_path=pickle_path, _ignore_y=True)\n",
    "\n",
    "    # take first sample_size samples\n",
    "    X_subset = X_subset[:sample_size]\n",
    "    X_srl_subset = X_srl_subset[:sample_size]\n",
    "    y_subset = y_subset[:sample_size]\n",
    "\n",
    "    # Len\n",
    "    print(\"X:\", len(X_subset))\n",
    "    print(\"X_srl:\", len(X_srl_subset))\n",
    "    print(\"y:\", len(y_subset))\n",
    "\n",
    "    print(\"CREATING DATASETS\")\n",
    "    \n",
    "    dataset = ArticleDataset(X_subset, X_srl_subset, tokenizer, y_subset, max_sentences_per_article, max_sentence_length,  max_args_per_sentence, max_arg_length)\n",
    "\n",
    "    # Create dataloaders\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "    \n",
    "    print(\"CREATION DONE\")\n",
    "    return dataset, dataloader\n",
    "\n",
    "# get dataset from string\n",
    "def get_dataset_from_string(string, tokenizer, max_sentences_per_article=32, max_sentence_length=32,  max_args_per_sentence=10, max_arg_length=16):\n",
    "    \"\"\"GET Datasets to test dict extraction\"\"\"\n",
    "    print(\"Get Testing Datasets for testing trained model\")\n",
    "    print()\n",
    "\n",
    "    # split text into sentences using nltk library\n",
    "    sentences = sent_tokenize(string)\n",
    "\n",
    "    # sentences to df and create article_id\n",
    "    df = pd.DataFrame(sentences, columns=['text'])\n",
    "    df['article_id'] = \"mock_article_id\"\n",
    "\n",
    "    X_subset, X_srl_subset, y_subset = preprocess_df(df, recalculate_srl=True, _ignore_y=True, _save = False)\n",
    "\n",
    "    # Len\n",
    "    print(\"X:\", len(X_subset))\n",
    "    print(\"X_srl:\", len(X_srl_subset))\n",
    "    print(\"y:\", len(y_subset))\n",
    "\n",
    "    print(\"CREATING DATASETS\")\n",
    "    \n",
    "    dataset = ArticleDataset(X_subset, X_srl_subset, tokenizer, y_subset, max_sentences_per_article, max_sentence_length,  max_args_per_sentence, max_arg_length)\n",
    "\n",
    "    # Create dataloaders\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    print(\"CREATION DONE\")\n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Model\n",
    "The Model consist out of various Layers.\n",
    "\n",
    "1. SRL_Embedding\n",
    "2. Autoencoder\n",
    "3. FRISSLoss\n",
    "4. Unsupervised\n",
    "5. Supervised\n",
    "6. FRISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SRL_Embeddings\n",
    "\n",
    "The layer takes tensors of token IDs with the shape [batch_size, max_num_sentences, max_num_tokens] for the sentence, predicates, arg0 and arg1 and returns for each sentence an embedding with shape [batch_size, embedding_dim] for the sentence, predicate, arg0 and arg1. \n",
    "\n",
    "The single embedding for the sentence is extracted by taking the [CLS] token embedding. For the predicate, arg0 and arg1 by taking the mean over all word embeddings in this list of tokens. \n",
    "\n",
    "> Possible improvements: Better way of extracting the single embedding for predicate, arg0 and arg1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shapes:  torch.Size([2, 12, 8]) torch.Size([2, 12, 9, 8]) torch.Size([2, 12, 9, 8]) torch.Size([2, 12, 9, 8])\n",
      "Outputs shapes:  torch.Size([2, 12, 768]) torch.Size([2, 12, 9, 768]) torch.Size([2, 12, 9, 768]) torch.Size([2, 12, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class SRL_Embeddings(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"bert-base-uncased\"):\n",
    "        super(SRL_Embeddings, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "        self.embedding_dim = 768  # for bert-base-uncased\n",
    "\n",
    "    def forward(self, sentence_ids, sentence_attention_masks, predicate_ids, predicate_attention_masks, arg0_ids, arg0_attention_masks, arg1_ids, arg1_attention_masks):\n",
    "        with torch.no_grad():\n",
    "            # Sentence embeddings\n",
    "            sentence_embeddings = self.bert_model(input_ids=sentence_ids.view(-1, sentence_ids.size(-1)), \n",
    "                                                  attention_mask=sentence_attention_masks.view(-1, sentence_attention_masks.size(-1)))[0]\n",
    "            sentence_embeddings = sentence_embeddings.view(sentence_ids.size(0), sentence_ids.size(1), -1, self.embedding_dim)\n",
    "            sentence_embeddings = sentence_embeddings.mean(dim=2)\n",
    "\n",
    "            # Predicate embeddings\n",
    "            predicate_embeddings = self.bert_model(input_ids=predicate_ids.view(-1, predicate_ids.size(-1)), \n",
    "                                                   attention_mask=predicate_attention_masks.view(-1, predicate_attention_masks.size(-1)))[0]\n",
    "            predicate_embeddings = predicate_embeddings.view(predicate_ids.size(0), predicate_ids.size(1), predicate_ids.size(2), -1, self.embedding_dim)\n",
    "            predicate_embeddings = predicate_embeddings.mean(dim=3)\n",
    "\n",
    "            # ARG0 embeddings\n",
    "            arg0_embeddings = self.bert_model(input_ids=arg0_ids.view(-1, arg0_ids.size(-1)), \n",
    "                                              attention_mask=arg0_attention_masks.view(-1, arg0_attention_masks.size(-1)))[0]\n",
    "            arg0_embeddings = arg0_embeddings.view(arg0_ids.size(0), arg0_ids.size(1), arg0_ids.size(2), -1, self.embedding_dim)\n",
    "            arg0_embeddings = arg0_embeddings.mean(dim=3)\n",
    "\n",
    "            # ARG1 embeddings\n",
    "            arg1_embeddings = self.bert_model(input_ids=arg1_ids.view(-1, arg1_ids.size(-1)), \n",
    "                                              attention_mask=arg1_attention_masks.view(-1, arg1_attention_masks.size(-1)))[0]\n",
    "            arg1_embeddings = arg1_embeddings.view(arg1_ids.size(0), arg1_ids.size(1), arg1_ids.size(2), -1, self.embedding_dim)\n",
    "            arg1_embeddings = arg1_embeddings.mean(dim=3)\n",
    "\n",
    "        return sentence_embeddings, predicate_embeddings, arg0_embeddings, arg1_embeddings\n",
    "\n",
    "# Generate dummy data for the SRL_Embeddings\n",
    "batch_size = 2\n",
    "num_sentences = 12\n",
    "sentence_length = 8\n",
    "num_args = 9\n",
    "predicate_length = 8\n",
    "arg0_length = 8\n",
    "arg1_length = 8\n",
    "\n",
    "# Dummy data for sentences, predicates, arg0, and arg1\n",
    "sentence_ids = torch.randint(0, 10000, (batch_size, num_sentences, sentence_length))\n",
    "predicate_ids = torch.randint(0, 10000, (batch_size, num_sentences, num_args, predicate_length))\n",
    "arg0_ids = torch.randint(0, 10000, (batch_size, num_sentences, num_args, arg0_length))\n",
    "arg1_ids = torch.randint(0, 10000, (batch_size, num_sentences, num_args, arg1_length))\n",
    "\n",
    "# Mock attention masks\n",
    "sentence_attention_masks = torch.randint(0, 2, (batch_size, num_sentences, sentence_length))\n",
    "predicate_attention_masks = torch.randint(0, 2, (batch_size, num_sentences, num_args, predicate_length))\n",
    "arg0_attention_masks = torch.randint(0, 2, (batch_size, num_sentences, num_args, arg0_length))\n",
    "arg1_attention_masks = torch.randint(0, 2, (batch_size, num_sentences, num_args, arg1_length))\n",
    "\n",
    "srl_embeddings = SRL_Embeddings()\n",
    "\n",
    "sentence_embeddings, predicate_embeddings, arg0_embeddings, arg1_embeddings = srl_embeddings(\n",
    "    sentence_ids, sentence_attention_masks, \n",
    "    predicate_ids, predicate_attention_masks, \n",
    "    arg0_ids, arg0_attention_masks, \n",
    "    arg1_ids, arg1_attention_masks\n",
    ")\n",
    "\n",
    "print(\"Inputs shapes: \", sentence_ids.shape, predicate_ids.shape, arg0_ids.shape, arg1_ids.shape)\n",
    "print(\"Outputs shapes: \", sentence_embeddings.shape, predicate_embeddings.shape, arg0_embeddings.shape, arg1_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shapes:\n",
      "p -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n",
      "a0 -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n",
      "a1 -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n",
      "NaN values:\n",
      "p -> vhat: False, d: False, g: False, F: False\n",
      "d:  tensor([[0.0459, 0.1011, 0.0656, 0.0344, 0.0282, 0.0329, 0.0380, 0.0472, 0.0161,\n",
      "         0.1816, 0.0237, 0.0328, 0.0856, 0.0332, 0.0435, 0.0413, 0.0268, 0.0288,\n",
      "         0.0418, 0.0514],\n",
      "        [0.0213, 0.0762, 0.0423, 0.0312, 0.0450, 0.0216, 0.0702, 0.0532, 0.0249,\n",
      "         0.1776, 0.0835, 0.0561, 0.0212, 0.0262, 0.0567, 0.0609, 0.0323, 0.0418,\n",
      "         0.0286, 0.0293]], grad_fn=<SoftmaxBackward0>)\n",
      "a0 -> vhat: False, d: False, g: False, F: False\n",
      "d:  tensor([[0.0614, 0.0236, 0.0396, 0.0762, 0.0743, 0.0829, 0.1025, 0.0713, 0.0202,\n",
      "         0.0540, 0.0536, 0.0471, 0.0220, 0.0525, 0.0386, 0.0578, 0.0342, 0.0274,\n",
      "         0.0093, 0.0515],\n",
      "        [0.0618, 0.0443, 0.0441, 0.0241, 0.0449, 0.1851, 0.1217, 0.0752, 0.0367,\n",
      "         0.0254, 0.0367, 0.0612, 0.0433, 0.0505, 0.0220, 0.0453, 0.0191, 0.0090,\n",
      "         0.0363, 0.0133]], grad_fn=<SoftmaxBackward0>)\n",
      "a1 -> vhat: False, d: False, g: False, F: False\n",
      "d:  tensor([[0.1268, 0.0396, 0.0306, 0.0414, 0.0388, 0.0399, 0.0499, 0.0844, 0.0363,\n",
      "         0.0243, 0.0119, 0.0179, 0.0702, 0.0500, 0.0217, 0.0714, 0.0770, 0.0328,\n",
      "         0.0558, 0.0794],\n",
      "        [0.0321, 0.0539, 0.0708, 0.0529, 0.0662, 0.0290, 0.0453, 0.0482, 0.0397,\n",
      "         0.0559, 0.0512, 0.0720, 0.0709, 0.0945, 0.0444, 0.0317, 0.0273, 0.0390,\n",
      "         0.0509, 0.0242]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import log_softmax, softmax\n",
    "\n",
    "class CombinedAutoencoder(nn.Module):\n",
    "    def __init__(self, D_w, D_h, K, dropout_prob=0.3):\n",
    "        super(CombinedAutoencoder, self).__init__()\n",
    "        \n",
    "        self.D_h = D_h\n",
    "        self.K = K\n",
    "        \n",
    "        # Shared feed-forward layer for all views\n",
    "        self.feed_forward_shared = nn.Linear(2 * D_w, D_h)\n",
    "        \n",
    "        # Unique feed-forward layers for each view\n",
    "        self.feed_forward_unique = nn.ModuleDict({\n",
    "            'a0': nn.Linear(D_h, K),\n",
    "            'p': nn.Linear(D_h, K),\n",
    "            'a1': nn.Linear(D_h, K),\n",
    "        })\n",
    "\n",
    "        # Initializing F matrices for each view\n",
    "        self.F_matrices = nn.ParameterDict({\n",
    "            'a0': nn.Parameter(torch.Tensor(K, D_w)),\n",
    "            'p': nn.Parameter(torch.Tensor(K, D_w)),\n",
    "            'a1': nn.Parameter(torch.Tensor(K, D_w)),\n",
    "        })\n",
    "\n",
    "        # init F matrices with xavier_uniform and nn.init.calculate_gain('relu')\n",
    "        for _, value in self.F_matrices.items():\n",
    "            nn.init.xavier_uniform_(value.data, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        # Additional layers and parameters\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        self.batch_norm = nn.BatchNorm1d(D_h)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.activation2 = nn.Sigmoid()\n",
    "\n",
    "    def sample_gumbel(self, shape, eps=1e-20, device='cpu'):\n",
    "        \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "        U = torch.rand(shape, device=device)\n",
    "        return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "    def gumbel_softmax_sample(self, logits, t):\n",
    "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "        y = logits + self.sample_gumbel(logits.size(), device=logits.device)\n",
    "        return softmax(y / t, dim=-1)\n",
    "\n",
    "\n",
    "    def gumbel_logsoftmax_sample(self, logits, t):\n",
    "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "        y = logits + self.sample_gumbel(logits.size(), device=logits.device)\n",
    "        return log_softmax(y / t, dim=-1)\n",
    "\n",
    "\n",
    "    def custom_gumbel_softmax(self, logits, tau, hard=False, log=False):\n",
    "        \"\"\"Sample from the Gumbel-Softmax distribution and optionally discretize.\n",
    "        Args:\n",
    "        logits: [batch_size, n_class] unnormalized log-probs\n",
    "        tau: non-negative scalar\n",
    "        hard: if True, take argmax, but differentiate w.r.t. soft sample y\n",
    "        Returns:\n",
    "        [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n",
    "        If hard=True, then the returned sample will be one-hot, otherwise it will\n",
    "        be a probabilitiy distribution that sums to 1 across classes\n",
    "        \"\"\"\n",
    "        if log:\n",
    "            y = self.gumbel_logsoftmax_sample(logits, tau)\n",
    "        else:\n",
    "            y = self.gumbel_softmax_sample(logits, tau)\n",
    "        if hard:\n",
    "            shape = y.size()\n",
    "            _, ind = y.max(dim=-1)\n",
    "            y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "            y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "            y_hard = y_hard.view(*shape)\n",
    "            # Set gradients w.r.t. y_hard gradients w.r.t. y\n",
    "            y_hard = (y_hard - y).detach() + y\n",
    "            return y_hard\n",
    "        return y\n",
    "\n",
    "    def forward(self, v_p, v_a0, v_a1, v_sentence, tau):\n",
    "        h_p = self.process_through_shared(v_p, v_sentence)\n",
    "        h_a0 = self.process_through_shared(v_a0, v_sentence)\n",
    "        h_a1 = self.process_through_shared(v_a1, v_sentence)\n",
    "\n",
    "        logits_p = self.feed_forward_unique['p'](h_p)\n",
    "        logits_a0 = self.feed_forward_unique['a0'](h_a0)\n",
    "        logits_a1 = self.feed_forward_unique['a1'](h_a1) \n",
    "\n",
    "        d_p = torch.softmax(logits_p, dim=1)\n",
    "        d_a0 = torch.softmax(logits_a0, dim=1)\n",
    "        d_a1 = torch.softmax(logits_a1, dim=1)\n",
    "\n",
    "        # TODO - Paper said we pass the output of softmax into the Gumbel-Softmax but code passes the logits\n",
    "\n",
    "        #g_p = self.custom_gumbel_softmax(d_p, tau=tau, hard=False, log=True)\n",
    "        #g_a0 = self.custom_gumbel_softmax(d_a0, tau=tau, hard=False, log=True)\n",
    "        #g_a1 = self.custom_gumbel_softmax(d_a1, tau=tau, hard=False, log=True)\n",
    "\n",
    "        g_p = self.custom_gumbel_softmax(logits_p, tau=tau, hard=False, log=False)\n",
    "        g_a0 = self.custom_gumbel_softmax(logits_a0, tau=tau, hard=False, log=False)\n",
    "        g_a1 = self.custom_gumbel_softmax(logits_a1, tau=tau, hard=False, log=False)\n",
    "\n",
    "        vhat_p = torch.matmul(g_p, self.F_matrices['p'])\n",
    "        vhat_a0 = torch.matmul(g_a0, self.F_matrices['a0'])\n",
    "        vhat_a1 = torch.matmul(g_a1, self.F_matrices['a1'])\n",
    "\n",
    "        return {\n",
    "            \"p\": {\"vhat\": vhat_p, \"d\": d_p, \"g\": g_p, \"F\": self.F_matrices['p']},\n",
    "            \"a0\": {\"vhat\": vhat_a0, \"d\": d_a0, \"g\": g_a0, \"F\": self.F_matrices['a0']},\n",
    "            \"a1\": {\"vhat\": vhat_a1, \"d\": d_a1, \"g\": g_a1, \"F\": self.F_matrices['a1']}\n",
    "        }\n",
    "        \n",
    "    def process_through_shared(self, v_z, v_sentence):\n",
    "        # Concatenating v_z with the sentence embedding\n",
    "        concatenated = torch.cat((v_z, v_sentence), dim=-1)\n",
    "        \n",
    "        # Applying dropout\n",
    "        dropped = self.dropout1(concatenated)\n",
    "\n",
    "        # Passing through the shared linear layer\n",
    "        h_shared = self.feed_forward_shared(dropped)\n",
    "\n",
    "        # Applying batch normalization and ReLU activation\n",
    "        h_shared = self.batch_norm(h_shared)\n",
    "        h_shared = self.activation(h_shared)\n",
    "\n",
    "        # Applying dropout again\n",
    "        h_shared = self.dropout2(h_shared)\n",
    "\n",
    "        return h_shared\n",
    "\n",
    "# Mock Data Preparation\n",
    "D_h = 768\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "K = 20\n",
    "tau = 0.9\n",
    "\n",
    "# Generating mock embeddings for article, predicate, ARG0, ARG1, and their corresponding sentence embeddings\n",
    "article_embedding = torch.randn(batch_size, embedding_dim)\n",
    "v_p = torch.randn(batch_size, embedding_dim)\n",
    "v_a0 = torch.randn(batch_size, embedding_dim)\n",
    "v_a1 = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "# Testing CombinedAutoencoder\n",
    "autoencoder = CombinedAutoencoder(embedding_dim, D_h, K)\n",
    "outputs = autoencoder(v_p, v_a0, v_a1, article_embedding, tau)\n",
    "\n",
    "# Check shapes of the outputs\n",
    "print(\"Output shapes:\")\n",
    "for key, value in outputs.items():\n",
    "    print(f\"{key} -> vhat: {value['vhat'].shape}, d: {value['d'].shape}, g: {value['g'].shape}, F: {value['F'].shape}\")\n",
    "\n",
    "# check if tensor have nan values\n",
    "def check_nan(tensor):\n",
    "    # if tensor has any nan values, return True\n",
    "    if torch.isnan(tensor).any():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Check if any of the outputs have NaN values\n",
    "print(\"NaN values:\")\n",
    "for key, value in outputs.items():\n",
    "    print(f\"{key} -> vhat: {check_nan(value['vhat'])}, d: {check_nan(value['d'])}, g: {check_nan(value['g'])}, F: {check_nan(value['F'])}\")\n",
    "    print(f\"d: \", value['d'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FRISSLoss\n",
    "\n",
    "The layer calculates the unsupervised loss for predicate, arg0 and arg1. \n",
    "\n",
    "The forward function takes as input 3 dicts with the parameters `v`, `v_hat`, `g` and `F`. Where `v` is the embedding of the predicate, arg0 or arg1. The `v_hat` (size: [batch_size, embedding_dim]) is the reconstructed embedding for the predicate, arg0 and arg1. The `g` is the gumbel softmax result (size: [batch_size, embedding_dim]). The `F` (size: [K, embedding_dim]) which is the descriptor dictionary.\n",
    "\n",
    "The layer returns the loss for each batch. So the output is [batch_size]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRiSSLoss output: tensor([755281.5000, 755281.5000])\n"
     ]
    }
   ],
   "source": [
    "class FRISSLoss(nn.Module):\n",
    "    def __init__(self, lambda_orthogonality, M, t):\n",
    "        super(FRISSLoss, self).__init__()\n",
    "        \n",
    "        self.lambda_orthogonality = lambda_orthogonality\n",
    "        self.M = M\n",
    "        self.t = t\n",
    "        self.triplet_loss = nn.TripletMarginLoss(margin=M)\n",
    "\n",
    "    def contrastive_loss(self, v, vhat, negatives):\n",
    "        batch_size = vhat.size(0)\n",
    "        N = negatives.size(0)\n",
    "        loss = torch.zeros(batch_size, device=v.device)\n",
    "\n",
    "        # Calculate true distance between reconstructed and real embeddings\n",
    "        true_distance = self.l2(vhat, v)\n",
    "\n",
    "        for i in range(N):  # loop over each element in \"negatives\"\n",
    "            \n",
    "            # Tranform negative from [embedding dim] to [batch size, embedding_dim] \n",
    "            negative = negatives[i, :].expand(v.size(0), -1)\n",
    "\n",
    "            # Calculate negative distance for current negative embedding\n",
    "            negative_distance = self.l2(vhat, negative)\n",
    "\n",
    "            # Compute loss based on the provided logic: l2(vhat, v) + 1 + l2(vhat, negative) and clamp to 0 if below 0\n",
    "            current_loss = 1 + true_distance - negative_distance\n",
    "            loss += torch.clamp(current_loss, min=0.0)\n",
    "\n",
    "        # Normalize the total loss by N\n",
    "        return loss / N\n",
    "\n",
    "    \n",
    "    def l2(self, u, v):\n",
    "        return torch.sqrt(torch.sum((u - v) ** 2, dim=1))\n",
    "    \n",
    "    def focal_triplet_loss_WRONG(self, v, vhat_z, g, F):\n",
    "        losses = []\n",
    "        for i in range(F.size(0)):  # Iterate over each negative example\n",
    "            # For each negative, compute the loss against the anchor and positive\n",
    "            loss = self.triplet_loss(vhat_z, v, F[i].unsqueeze(0).expand(v.size(0), -1))\n",
    "            losses.append(loss)\n",
    "\n",
    "        loss_tensor = torch.stack(losses) \n",
    "        loss = loss_tensor.mean(dim=0).mean()\n",
    "        return loss\n",
    "    \n",
    "    def focal_triplet_loss(self, v, vhat_z, g, F):\n",
    "        _, indices = torch.topk(g, self.t, largest=False, dim=1)\n",
    "\n",
    "        F_t = torch.stack([F[indices[i]] for i in range(g.size(0))])\n",
    "\n",
    "        g_tz = torch.stack([g[i, indices[i]] for i in range(g.size(0))])\n",
    "                    \n",
    "        g_t = g_tz / g_tz.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # if division by zero set all nan values to 0\n",
    "        g_t[torch.isnan(g_t)] = 0\n",
    "        \n",
    "        m_t = self.M * ((1 - g_t)**2)\n",
    "\n",
    "        # Initializing loss\n",
    "        loss = torch.zeros_like(v[:, 0])\n",
    "        \n",
    "        # Iteratively adding to the loss for each negative embedding\n",
    "        for i in range(self.t):\n",
    "            current_v_t = F_t[:, i]\n",
    "            current_m_t = m_t[:, i]\n",
    "            \n",
    "            current_loss = current_m_t + self.l2(vhat_z, v) - self.l2(vhat_z, current_v_t)\n",
    "            \n",
    "            loss += torch.max(torch.zeros_like(current_loss), current_loss)\n",
    "             \n",
    "        # Normalizing\n",
    "        loss = loss / self.t\n",
    "        return loss\n",
    "\n",
    "    def orthogonality_term(self, F, reg=1e-4):\n",
    "        gram_matrix = torch.mm(F, F.T)  # Compute the Gram matrix F * F^T\n",
    "        identity_matrix = torch.eye(gram_matrix.size(0), device=gram_matrix.device)  # Create an identity matrix\n",
    "        ortho_loss = (gram_matrix - identity_matrix).abs().sum()\n",
    "        return ortho_loss\n",
    "\n",
    "\n",
    "    def forward(self, p, a0, a1, p_negatives, a0_negatives, a1_negatives):\n",
    "        # Extract components from dictionary for predicate p\n",
    "        v_p, vhat_p, d_p, g_p, F_p = p[\"v\"], p[\"vhat\"], p[\"d\"], p[\"g\"], p[\"F\"]\n",
    "        \n",
    "        # Extract components from dictionary for ARG0\n",
    "        v_a0, vhat_a0, d_a0, g_a0, F_a0 = a0[\"v\"], a0[\"vhat\"], a0[\"d\"], a0[\"g\"], a0[\"F\"]\n",
    "\n",
    "        # Extract components from dictionary for ARG1\n",
    "        v_a1, vhat_a1, d_a1, g_a1, F_a1 = a1[\"v\"], a1[\"vhat\"], a1[\"d\"], a1[\"g\"], a1[\"F\"]\n",
    "        \n",
    "         # Calculate losses for predicate\n",
    "        Ju_p = self.contrastive_loss(v_p, vhat_p, p_negatives)        \n",
    "        Jt_p = self.focal_triplet_loss(v_p, vhat_p, g_p, F_p)        \n",
    "        Jz_p = Ju_p + Jt_p + self.lambda_orthogonality * self.orthogonality_term(F_p) ** 2\n",
    "        \n",
    "        # Calculate losses for ARG0\n",
    "        Ju_a0 = self.contrastive_loss(v_a0, vhat_a0, a0_negatives)\n",
    "        Jt_a0 = self.focal_triplet_loss(v_a0, vhat_a0, g_a0, F_a0)\n",
    "        Jz_a0 = Ju_a0 + Jt_a0 + self.lambda_orthogonality * self.orthogonality_term(F_a0) ** 2\n",
    "        \n",
    "        # Calculate losses for ARG1\n",
    "        Ju_a1 = self.contrastive_loss(v_a1, vhat_a1, a1_negatives)\n",
    "        Jt_a1 = self.focal_triplet_loss(v_a1, vhat_a1, g_a1, F_a1)\n",
    "        Jz_a1 = Ju_a1 + Jt_a1 + self.lambda_orthogonality * self.orthogonality_term(F_a1) ** 2\n",
    "        \n",
    "        if torch.isnan(Jz_p).any():\n",
    "            print(\"Jz_p has nan\")\n",
    "            \n",
    "        if torch.isnan(Jz_a0).any():\n",
    "            print(\"Jz_a0 has nan\")\n",
    "            \n",
    "        if torch.isnan(Jz_a1).any():\n",
    "            print(\"Jz_a1 has nan\")\n",
    "        \n",
    "        # Aggregate the losses\n",
    "        loss = Jz_p + Jz_a0 + Jz_a1\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "# Mock Data Preparation\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "K = 15  # Number of frames/descriptors\n",
    "\n",
    "# Generating mock embeddings for article, predicate, ARG0, ARG1 and their reconstructions\n",
    "article_embedding = torch.randn(batch_size, embedding_dim)\n",
    "v_p = torch.randn(batch_size, embedding_dim)\n",
    "vhat_p = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "v_a0 = torch.randn(batch_size, embedding_dim)\n",
    "vhat_a0 = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "v_a1 = torch.randn(batch_size, embedding_dim)\n",
    "vhat_a1 = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "# Generating mock descriptor weights and descriptor matrices for predicate, ARG0, ARG1\n",
    "d_p = torch.randn(batch_size, K)\n",
    "d_a0 = torch.randn(batch_size, K)\n",
    "d_a1 = torch.randn(batch_size, K)\n",
    "\n",
    "F_p = torch.randn(K, embedding_dim)\n",
    "F_a0 = torch.randn(K, embedding_dim)\n",
    "F_a1 = torch.randn(K, embedding_dim)\n",
    "\n",
    "g_p = torch.randn(batch_size, K)\n",
    "g_a0 = torch.randn(batch_size, K)\n",
    "g_a1 = torch.randn(batch_size, K)\n",
    "\n",
    "# Generating some negative samples (let's assume 5 negative samples per batch entry)\n",
    "num_negatives = 8\n",
    "negatives_p = torch.randn(num_negatives, embedding_dim)\n",
    "negatives_a0 = torch.randn(num_negatives, embedding_dim)\n",
    "negatives_a1 = torch.randn(num_negatives, embedding_dim)\n",
    "\n",
    "# Initialize loss function\n",
    "lambda_orthogonality = 1e-3\n",
    "\n",
    "t = 8  # Number of descriptors with smallest weights for negative samples\n",
    "M = t\n",
    "\n",
    "loss_fn = FRISSLoss(lambda_orthogonality, M, t)\n",
    "\n",
    "# Organizing inputs into dictionaries\n",
    "p = {\"v\": v_p, \"vhat\": vhat_p, \"d\": d_p, \"g\": g_p, \"F\": F_p}\n",
    "a0 = {\"v\": v_a0, \"vhat\": vhat_a0, \"d\": d_a0, \"g\": g_a0, \"F\": F_a0}\n",
    "a1 = {\"v\": v_a1, \"vhat\": vhat_a1, \"d\": d_a1, \"g\": g_a1, \"F\": F_a1}\n",
    "\n",
    "loss_fn = FRISSLoss(lambda_orthogonality, M, t)\n",
    "loss = loss_fn(p, a0, a1, negatives_p, negatives_a0, negatives_a1)\n",
    "print(\"FRiSSLoss output:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FRISSUnsupervised\n",
    "\n",
    "The `FRISSUnsupervised` layer integrates multiple autoencoders and the previously described `FRISSLoss` layer to achieve an unsupervised learning process over the predicates and their arguments.\n",
    "\n",
    "### Forward Method:\n",
    "\n",
    "**Inputs**:\n",
    "1. **v_p**: Embedding of the predicate with size: [batch_size, D_w].\n",
    "2. **v_a0**: Embedding of the ARG0 (first argument) with size: [batch_size, D_w].\n",
    "3. **v_a1**: Embedding of the ARG1 (second argument) with size: [batch_size, D_w].\n",
    "4. **v_article**: Embedding of the article with size: [batch_size, D_w].\n",
    "5. **negatives**: Tensor containing negative samples with size: [batch_size, num_negatives, D_w].\n",
    "6. **tau**: A scalar parameter for the Gumbel softmax in the autoencoder.\n",
    "\n",
    "**Outputs**:\n",
    "- A dictionary `results` containing:\n",
    "    - **loss**: A tensor representing the combined unsupervised loss over the batch with size: [batch_size].\n",
    "    - **p**: Dictionary containing components for the predicate, including reconstructed embedding (`vhat`), descriptor weights (`d`), Gumbel softmax result (`g`), and the descriptor matrix (`F`).\n",
    "    - **a0**: Same as `p` but for ARG0.\n",
    "    - **a1**: Same as `p` but for ARG1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results' Shapes:\n",
      "loss: tensor([3215.3774, 3216.5034], grad_fn=<AddBackward0>)\n",
      "p -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n",
      "a0 -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n",
      "a1 -> vhat: torch.Size([2, 768]), d: torch.Size([2, 20]), g: torch.Size([2, 20]), F: torch.Size([20, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming you have already defined CombinedAutoencoder and its methods as provided earlier.\n",
    "\n",
    "class FRISSUnsupervised(nn.Module):\n",
    "    def __init__(self, D_w, D_h, K, num_frames, lambda_orthogonality, M, t, dropout_prob=0.3):\n",
    "        super(FRISSUnsupervised, self).__init__()\n",
    "        \n",
    "        self.loss_fn = FRISSLoss(lambda_orthogonality, M, t)      \n",
    "        \n",
    "        # Using the CombinedAutoencoder instead of individual Autoencoders\n",
    "        self.combined_autoencoder = CombinedAutoencoder(D_w, D_h, K, dropout_prob=dropout_prob)\n",
    "\n",
    "    def forward(self, v_p, v_a0, v_a1, v_sentence, p_negatives, a0_negatives, a1_negatives, tau):\n",
    "        outputs = self.combined_autoencoder(v_p, v_a0, v_a1, v_sentence, tau)\n",
    "\n",
    "        outputs_p = outputs[\"p\"]\n",
    "        outputs_p[\"v\"] = v_p\n",
    "        \n",
    "        outputs_a0 = outputs[\"a0\"]\n",
    "        outputs_a0[\"v\"] = v_a0\n",
    "        \n",
    "        outputs_a1 = outputs[\"a1\"]\n",
    "        outputs_a1[\"v\"] = v_a1\n",
    "        \n",
    "        loss = self.loss_fn(\n",
    "            outputs_p,\n",
    "            outputs_a0, \n",
    "            outputs_a1, \n",
    "            p_negatives, a0_negatives, a1_negatives\n",
    "        )\n",
    "\n",
    "        results = {\n",
    "            \"loss\": loss,\n",
    "            \"p\": outputs[\"p\"],\n",
    "            \"a0\": outputs[\"a0\"],\n",
    "            \"a1\": outputs[\"a1\"]\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Mock Data Preparation\n",
    "D_h = 768\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "K = 20\n",
    "num_frames = 15\n",
    "tau = 0.9\n",
    "lambda_orthogonality = 0.1  # Placeholder value, please replace with your actual value\n",
    "M = 7  # Placeholder value, please replace with your actual value\n",
    "t = 7  # Placeholder value, please replace with your actual value\n",
    "\n",
    "# Generating mock embeddings for article, predicate, ARG0, ARG1, and their corresponding sentence embeddings\n",
    "article_embedding = torch.randn(batch_size, embedding_dim)\n",
    "v_p = torch.randn(batch_size, embedding_dim)\n",
    "v_a0 = torch.randn(batch_size, embedding_dim)\n",
    "v_a1 = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "# Generating some negative samples (let's assume 5 negative samples per batch entry)\n",
    "num_negatives = 10\n",
    "negatives_p = torch.randn(num_negatives, embedding_dim)\n",
    "negatives_a0 = torch.randn(num_negatives, embedding_dim)\n",
    "negatives_a1 = torch.randn(num_negatives, embedding_dim)\n",
    "\n",
    "# Testing FRISSUnsupervised\n",
    "unsupervised_module = FRISSUnsupervised(embedding_dim, D_h, K, num_frames, lambda_orthogonality, M, t)\n",
    "results = unsupervised_module(v_p, v_a0, v_a1, article_embedding, negatives_p, negatives_a0, negatives_a1, tau)\n",
    "\n",
    "# Print the results' shapes for verification\n",
    "print(\"Results' Shapes:\")\n",
    "for key, value in results.items():\n",
    "    if key == \"loss\":\n",
    "        print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(f\"{key} -> vhat: {value['vhat'].shape}, d: {value['d'].shape}, g: {value['g'].shape}, F: {value['F'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. FRISSSupervised\n",
    "\n",
    "The layer takes the embeddings from the args and the sentence and predicts frames. \n",
    "\n",
    "The embeddings for the args are averaged for each arg individually and then averaged on args level. The final embedding is feed into a linear layer and passed through a sigmoid function. \n",
    "\n",
    "The sentence embedding is feed into a linear layer and then into a relu function. After again in a linear function and then averaged. The average embeddung is again feed into a linear layer and lastly in a signoid function. \n",
    "\n",
    "It returns a span and sentence based prediction of shape [batch_size, num_frames]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 15]), torch.Size([2, 15]), torch.Size([2, 15]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FRISSSupervised(nn.Module):\n",
    "    def __init__(self, D_w, K, num_frames, dropout_prob=0.3):\n",
    "        super(FRISSSupervised, self).__init__()\n",
    "\n",
    "        self.D_w = D_w\n",
    "                \n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.feed_forward_sentence1 = nn.Linear(D_w, D_w)\n",
    "        self.feed_forward_sentence2 = nn.Linear(D_w, num_frames)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Adding two dropout layers\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "    def forward(self, d_p, d_a0, d_a1, vs):\n",
    "        # Span-based Classification   \n",
    "\n",
    "        # Aggregate the SRL descriptors to have one descriptor per sentence\n",
    "        d_p = d_p.mean(dim=2)\n",
    "        d_a0 = d_a0.mean(dim=2)\n",
    "        d_a1 = d_a1.mean(dim=2)\n",
    "\n",
    "        # Take the mean over descriptors\n",
    "        w_u = (d_p + d_a0 + d_a1) / 3\n",
    "\n",
    "        w_u = w_u.sum(dim=1)\n",
    "\n",
    "        # Sentence-based Classification\n",
    "\n",
    "        # Apply the first dropout to vs\n",
    "        vs = self.dropout1(vs)\n",
    "\n",
    "        ws = self.relu(self.feed_forward_sentence1(vs))\n",
    "\n",
    "        # Mean over sentences and apply the second dropout\n",
    "        ws = self.dropout2(ws.mean(dim=1))\n",
    "\n",
    "        # Pass through the second feed forward network\n",
    "        ws = self.feed_forward_sentence2(ws)\n",
    "\n",
    "        # The softmax layer is commented out as it is not used with CrossEntropyLoss\n",
    "        # ys_hat = self.softmax(ws)\n",
    "\n",
    "        # combined pred = sum of span-based and sentence-based predictions\n",
    "        combined = w_u + ws\n",
    "\n",
    "        return w_u, ws, combined\n",
    "\n",
    "\n",
    "# Mock Data Preparation\n",
    "\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "num_frames = 15  # Assuming the number of frames is equal to K for simplicity\n",
    "num_sentences = 32\n",
    "K = 15\n",
    "num_args = 9\n",
    "\n",
    "# Generating mock dsz representations for predicate, ARG0, ARG1\n",
    "d_p = torch.randn(batch_size, num_sentences, num_args, K)\n",
    "d_a0 = torch.randn(batch_size, num_sentences, num_args, K)\n",
    "d_a1 = torch.randn(batch_size, num_sentences, num_args, K) \n",
    "\n",
    "# Adjusting the num_heads parameter\n",
    "srl_heads = 4\n",
    "sentence_heads = 8\n",
    "\n",
    "# Adjust the mock sentence embeddings shape\n",
    "vs = torch.randn(batch_size, num_sentences, embedding_dim)\n",
    "\n",
    "# Initialize and test the supervised module\n",
    "supervised_module = FRISSSupervised(embedding_dim, K, num_frames)\n",
    "\n",
    "# Forward pass the mock data\n",
    "yu_hat, ys_hat, combined_pred = supervised_module(d_p, d_a0, d_a1, vs)\n",
    "yu_hat.shape, ys_hat.shape, combined_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FRISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(9.0970, grad_fn=<DivBackward0>),\n",
       " torch.Size([2, 14]),\n",
       " torch.Size([2, 14]),\n",
       " torch.Size([2, 14]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FRISS(nn.Module):\n",
    "    def __init__(self, embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob=0.3, bert_model_name=\"bert-base-uncased\"):\n",
    "        super(FRISS, self).__init__()\n",
    "        \n",
    "        # Aggregation layer replaced with SRL_Embeddings\n",
    "        self.aggregation = SRL_Embeddings(bert_model_name)\n",
    "        \n",
    "        # Unsupervised training module\n",
    "        self.unsupervised = FRISSUnsupervised(embedding_dim, D_h, K, num_frames, lambda_orthogonality, M, t, dropout_prob=dropout_prob)\n",
    "        \n",
    "        # Supervised training module\n",
    "        self.supervised = FRISSSupervised(embedding_dim, K, num_frames, dropout_prob=dropout_prob)\n",
    "        \n",
    "    def negative_sampling(self, embeddings, num_negatives=8):\n",
    "        batch_size, num_sentences, num_args, embedding_dim = embeddings.size()\n",
    "        all_negatives = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(num_sentences):\n",
    "                # Flatten the arguments dimension to sample across all arguments in the sentence\n",
    "                flattened_embeddings = embeddings[i, j].view(-1, embedding_dim)\n",
    "                \n",
    "                # Get indices of non-padded embeddings (assuming padding is represented by all-zero vectors)\n",
    "                non_padded_indices = torch.where(torch.any(flattened_embeddings != 0, dim=1))[0]\n",
    "\n",
    "                # Randomly sample negative indices from non-padded embeddings\n",
    "                if len(non_padded_indices) > 0:\n",
    "                    negative_indices = non_padded_indices[torch.randint(0, len(non_padded_indices), (num_negatives,))]\n",
    "                else:\n",
    "                    # If no non-padded embeddings, use zeros\n",
    "                    negative_indices = torch.zeros(num_negatives, dtype=torch.long)\n",
    "\n",
    "                negative_samples = flattened_embeddings[negative_indices, :]\n",
    "                all_negatives.append(negative_samples)\n",
    "\n",
    "        # Concatenate all negative samples into a single tensor\n",
    "        all_negatives = torch.cat(all_negatives, dim=0)\n",
    "\n",
    "        # If more samples than required, randomly select 'num_negatives' samples\n",
    "        if all_negatives.size(0) > num_negatives:\n",
    "            indices = torch.randperm(all_negatives.size(0))[:num_negatives]\n",
    "            all_negatives = all_negatives[indices]\n",
    "\n",
    "        return all_negatives\n",
    "    \n",
    "    def forward(self, sentence_ids, sentence_attention_masks, predicate_ids, predicate_attention_masks, arg0_ids, arg0_attention_masks, arg1_ids, arg1_attention_masks, tau):\n",
    "        # Convert input IDs to embeddings\n",
    "        sentence_embeddings, predicate_embeddings, arg0_embeddings, arg1_embeddings = self.aggregation(sentence_ids, sentence_attention_masks, predicate_ids, predicate_attention_masks, arg0_ids, arg0_attention_masks, arg1_ids, arg1_attention_masks)\n",
    "        \n",
    "        # Handle multiple spans by averaging predictions\n",
    "        unsupervised_losses = torch.zeros((sentence_embeddings.size(0),), device=sentence_embeddings.device)\n",
    "        \n",
    "        # Creating storage for aggregated d tensors\n",
    "        d_p_list, d_a0_list, d_a1_list = [], [], []\n",
    "        \n",
    "        negatives_p = self.negative_sampling(predicate_embeddings)\n",
    "        negatives_a0 = self.negative_sampling(arg0_embeddings)\n",
    "        negatives_a1 = self.negative_sampling(arg1_embeddings)\n",
    "\n",
    "        # Process each sentence \n",
    "        for sentence_idx in range(sentence_embeddings.size(1)):\n",
    "            s_sentence_span = sentence_embeddings[:, sentence_idx, :]\n",
    "\n",
    "            d_p_sentence_list = []\n",
    "            d_a0_sentence_list = []\n",
    "            d_a1_sentence_list = []\n",
    "\n",
    "            # Process each span\n",
    "            for span_idx in range(predicate_embeddings.size(2)):                \n",
    "                v_p_span = predicate_embeddings[:, sentence_idx, span_idx, :]\n",
    "                v_a0_span = arg0_embeddings[:, sentence_idx, span_idx, :]\n",
    "                v_a1_span = arg1_embeddings[:, sentence_idx, span_idx, :]\n",
    "\n",
    "                # Feed the embeddings to the unsupervised module\n",
    "                unsupervised_results = self.unsupervised(v_p_span, v_a0_span, v_a1_span, s_sentence_span, negatives_p, negatives_a0, negatives_a1, tau)                \n",
    "                unsupervised_losses += unsupervised_results[\"loss\"]\n",
    "                \n",
    "                if torch.isnan(unsupervised_results[\"loss\"]).any():\n",
    "                    print(\"loss is nan\")\n",
    "                \n",
    "                # Use the vhat (reconstructed embeddings) for supervised predictions\n",
    "                d_p_sentence_list.append(unsupervised_results['p']['d'])\n",
    "                d_a0_sentence_list.append(unsupervised_results['a0']['d'])\n",
    "                d_a1_sentence_list.append(unsupervised_results['a1']['d'])        \n",
    "\n",
    "\n",
    "            # Aggregating across all spans\n",
    "            d_p_sentence = torch.stack(d_p_sentence_list, dim=1)\n",
    "            d_a0_sentence = torch.stack(d_a0_sentence_list, dim=1)\n",
    "            d_a1_sentence = torch.stack(d_a1_sentence_list, dim=1)\n",
    "\n",
    "            d_p_list.append(d_p_sentence)\n",
    "            d_a0_list.append(d_a0_sentence)\n",
    "            d_a1_list.append(d_a1_sentence)\n",
    "\n",
    "        # Aggregating across all spans\n",
    "        d_p_aggregated = torch.stack(d_p_list, dim=1)\n",
    "        d_a0_aggregated = torch.stack(d_a0_list, dim=1)\n",
    "        d_a1_aggregated = torch.stack(d_a1_list, dim=1)\n",
    "        \n",
    "        # Supervised predictions\n",
    "        span_pred, sentence_pred, combined_pred = self.supervised(d_p_aggregated, d_a0_aggregated, d_a1_aggregated, sentence_embeddings)\n",
    "    \n",
    "        # Identify valid (non-nan) losses\n",
    "        valid_losses = ~torch.isnan(unsupervised_losses)\n",
    "\n",
    "        # Take average by summing the valid losses and dividing by num sentences so that padded sentences are also taken in equation\n",
    "        unsupervised_loss = unsupervised_losses[valid_losses].sum() / (sentence_embeddings.shape[1] * sentence_embeddings.shape[2])\n",
    "        \n",
    "        return unsupervised_loss, span_pred, sentence_pred, combined_pred\n",
    "\n",
    "\n",
    "# Set the necessary parameters\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "K = 14  # Number of frames/descriptors\n",
    "num_frames = 14  # Assuming the number of frames is equal to K for simplicity\n",
    "D_h = 512  # Dimension of the hidden representation\n",
    "lambda_orthogonality = 0.1\n",
    "M = 8\n",
    "t = 8\n",
    "tau = 1.0\n",
    "\n",
    "# Define some mock token IDs data parameters\n",
    "max_sentences_per_article = 8\n",
    "max_sentence_length = 10\n",
    "num_sentences = max_sentences_per_article\n",
    "max_args_per_sentence = 3\n",
    "\n",
    "# Generating mock token IDs for predicate, ARG0, ARG1, and their corresponding sentences\n",
    "# We assume a vocab size of 30522 (standard BERT vocab size) for simplicity.\n",
    "vocab_size = 30522\n",
    "\n",
    "sentence_ids = torch.randint(0, vocab_size, (batch_size, max_sentences_per_article, max_sentence_length))\n",
    "predicate_ids = torch.randint(0, vocab_size, (batch_size, max_sentences_per_article, max_args_per_sentence, max_sentence_length))\n",
    "arg0_ids = torch.randint(0, vocab_size, (batch_size, max_sentences_per_article, max_args_per_sentence, max_sentence_length))\n",
    "arg1_ids = torch.randint(0, vocab_size, (batch_size, max_sentences_per_article, max_args_per_sentence, max_sentence_length))\n",
    "\n",
    "sentence_embeddings = torch.randn(batch_size, max_sentences_per_article, embedding_dim)\n",
    "predicate_embeddings = torch.randn(batch_size, max_sentences_per_article, max_args_per_sentence, embedding_dim)\n",
    "arg0_embeddings = torch.randn(batch_size, max_sentences_per_article, max_args_per_sentence, embedding_dim)\n",
    "arg1_embeddings = torch.randn(batch_size, max_sentences_per_article, max_args_per_sentence, embedding_dim)\n",
    "\n",
    "# Mock attention masks\n",
    "sentence_attention_masks = torch.randint(0, 2, (batch_size, max_sentences_per_article, max_sentence_length))\n",
    "predicate_attention_masks = torch.randint(0, 2, (batch_size, max_sentences_per_article, max_args_per_sentence, max_sentence_length))\n",
    "arg0_attention_masks = torch.randint(0, 2, (batch_size, max_sentences_per_article, max_args_per_sentence, max_sentence_length))\n",
    "arg1_attention_masks = torch.randint(0, 2, (batch_size, max_sentences_per_article, max_args_per_sentence, max_sentence_length))\n",
    "\n",
    "# Initialize the FRISS model\n",
    "friss_model = FRISS(embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K=K, num_frames=num_frames)\n",
    "\n",
    "# Forward pass the mock data\n",
    "unsupervised_loss, span_pred, sentence_pred, combined_pred = friss_model(sentence_ids, sentence_attention_masks, predicate_ids, predicate_attention_masks, arg0_ids, arg0_attention_masks, arg1_ids, arg1_attention_masks, 1)\n",
    "unsupervised_loss, span_pred.shape, sentence_pred.shape, combined_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Testing Datasets for testing trained model\n",
      "\n",
      "Load SRL from Pickle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f203f89de994f4db944b4178f77cd08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67865808 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 500\n",
      "X_srl: 500\n",
      "y: 500\n",
      "CREATING DATASETS\n",
      "CREATION DONE\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "max_sentences_per_article = 24\n",
    "max_sentence_length = 32\n",
    "\n",
    "max_args_per_sentence = 10\n",
    "max_arg_length = 8\n",
    "\n",
    "sample_size = 500\n",
    "\n",
    "dataset, dataloader = get_testing_dataset(df_unlabeled, tokenizer, recalculate_srl=False, batch_size=batch_size, max_sentences_per_article=max_sentences_per_article, max_sentence_length=max_sentence_length, max_arg_length=max_arg_length, sample_size=sample_size, pickle_path=\"../notebooks/FRISS_SRL_unlabeled.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Testing Datasets for testing trained model\n",
      "\n",
      "Recalculate SRL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40ea8890f444cd6badde064c0338e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 1\n",
      "X_srl: 1\n",
      "y: 1\n",
      "CREATING DATASETS\n",
      "CREATION DONE\n"
     ]
    }
   ],
   "source": [
    "example_article = \"\"\"\n",
    "BILL ON IMMIGRANT WORKERS DIES. Legislation to allow nearly twice as many computer-savvy foreigners and other high-skilled immigrants into the country next year apparently has died in Congress. The House passed the compromise measure last month, 288-133, but Sen. Tom Harkin, D-Iowa, had blocked a vote when in the Senate. The proposal, backed by high-tech companies, would raise the limit of so- called H-1B visas granted each year to skilled workers from abroad. Only 65,000 visas are now granted each year; the bill would raise the annual cap to 115,500 for the next two years and to 107,500 in 2001. The ceiling would return to 65,000 in 2002.\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# get dataset from string\n",
    "example_dataset, example_dataloader = get_dataset_from_string(example_article, tokenizer, max_sentences_per_article=32, max_sentence_length=32,  max_args_per_sentence=10, max_arg_length=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_path(path, embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob, bert_model_name, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads the weights into an instance of the model class from the given path.\n",
    "    \n",
    "    Args:\n",
    "    - model_class (torch.nn.Module): The class of the model (uninitialized).\n",
    "    - path (str): Path to the saved weights.\n",
    "    - device (str): Device to load the model on ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - model (torch.nn.Module): Model with weights loaded.\n",
    "    \"\"\"\n",
    "\n",
    "    # Model instantiation\n",
    "    model = FRISS(embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob=dropout_prob, bert_model_name=bert_model_name)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    \n",
    "    #model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 768\n",
    "num_frames = 15\n",
    "\n",
    "D_h = 768\n",
    "lambda_orthogonality = 1e-3\n",
    "\n",
    "K = num_frames\n",
    "t = 8\n",
    "M = 8\n",
    "tau_min = 0.5\n",
    "tau_decay = 5e-4\n",
    "\n",
    "dropout_prob = 0.3\n",
    "\n",
    "friss_model_path = \"bert-base-uncased\"\n",
    "bert_model_path = \"bert-base-uncased\"\n",
    "\n",
    "model = load_model_from_path('models/friss-new-v2/model_checkpoint_epoch_4.pth', \n",
    "                             embedding_dim, \n",
    "                             D_h, \n",
    "                             lambda_orthogonality, \n",
    "                             M, \n",
    "                             t, \n",
    "                             max_sentences_per_article, \n",
    "                             K, \n",
    "                             num_frames, \n",
    "                             dropout_prob, \n",
    "                             bert_model_path, \n",
    "                             device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def inspect(model, dataloader, device='cuda'):\n",
    "    \"\"\"\n",
    "    Make predictions with the given model and dataloader.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): The model to make predictions with.\n",
    "    - dataloader (DataLoader): DataLoader for the dataset to predict on.\n",
    "    - device (str): Device to make predictions on ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - predicted_labels (list of lists): List containing the predicted labels for each instance.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # dim\n",
    "    batch_size = dataloader.batch_size\n",
    "    num_sentences = dataloader.dataset.max_sentences_per_article\n",
    "    max_args_per_sentence = dataloader.dataset.max_args_per_sentence   \n",
    "    K = 15\n",
    "\n",
    "    print(\"num_sentences\", num_sentences)\n",
    "    print(\"max_args_per_sentence\", max_args_per_sentence)\n",
    "\n",
    "    all_preds_span = []\n",
    "    \n",
    "    # Initialize usage lists for each label\n",
    "    all_used_labels_p = []\n",
    "    all_used_labels_a0 = []\n",
    "    all_used_labels_a1 = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Wrap the dataloader with tqdm for batch progress\n",
    "        for batch in tqdm(dataloader, desc=\"Processing Batches\"):\n",
    "            used_labels_p = []\n",
    "            used_labels_a0 = []\n",
    "            used_labels_a1 = []\n",
    "    \n",
    "            sentence_ids = batch['sentence_ids'].to(device)\n",
    "            sentence_attention_masks = batch['sentence_attention_masks'].to(device)\n",
    "\n",
    "            predicate_ids = batch['predicate_ids'].to(device)\n",
    "            predicate_attention_masks = batch['predicate_attention_masks'].to(device)\n",
    "            \n",
    "            arg0_ids = batch['arg0_ids'].to(device)\n",
    "            arg0_attention_masks = batch['arg0_attention_masks'].to(device)\n",
    "\n",
    "            arg1_ids = batch['arg1_ids'].to(device)\n",
    "            arg1_attention_masks = batch['arg1_attention_masks'].to(device)\t\n",
    "\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            sentence_embeddings, predicate_embeddings, arg0_embeddings, arg1_embeddings = model.aggregation(sentence_ids, sentence_attention_masks, predicate_ids, predicate_attention_masks, arg0_ids, arg0_attention_masks, arg1_ids, arg1_attention_masks)\n",
    "            \n",
    "            # Process each span\n",
    "            for sentence_idx in range(sentence_embeddings.size(1)):\n",
    "                used_sentence_labels_p = []\n",
    "                used_sentence_labels_a0 = []\n",
    "                used_sentence_labels_a1 = []\n",
    "\n",
    "                s_sentence_span = sentence_embeddings[:, sentence_idx, :]\n",
    "\n",
    "                for span_idx in range(predicate_embeddings.size(2)):                    \n",
    "                    v_p_span = predicate_embeddings[:, sentence_idx, span_idx, :]\n",
    "                    v_a0_span = arg0_embeddings[:, sentence_idx, span_idx, :]\n",
    "                    v_a1_span = arg1_embeddings[:, sentence_idx, span_idx, :]\n",
    "\n",
    "                    #unsupervised.combined_autoencoder v_p, v_a0, v_a1, v_sentence, tau\n",
    "                    output = model.unsupervised.combined_autoencoder(v_p_span, v_a0_span, v_a1_span, s_sentence_span, 0.6)\n",
    "                    \n",
    "                    used_sentence_labels_p.append(output[\"p\"][\"g\"].cpu().numpy())\n",
    "                    used_sentence_labels_a0.append(output[\"a0\"][\"g\"].cpu().numpy())\n",
    "                    used_sentence_labels_a1.append(output[\"a1\"][\"g\"].cpu().numpy())\n",
    "                \n",
    "                used_labels_p.append(used_sentence_labels_p)\n",
    "                used_labels_a0.append(used_sentence_labels_a0)\n",
    "                used_labels_a1.append(used_sentence_labels_a1)\n",
    "            \n",
    "            # Forward pass\n",
    "            _, span_logits, sentence_logits, combined_logits = model(sentence_ids, sentence_attention_masks, predicate_ids, predicate_attention_masks, arg0_ids, arg0_attention_masks, arg1_ids, arg1_attention_masks, 0.5)\n",
    "            combined_pred = (torch.softmax(combined_logits, dim=-1) > 0.5).float()\n",
    "\n",
    "            all_preds_span.append(combined_pred.cpu().numpy())\n",
    "                \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            all_used_labels_p.append(used_labels_p)\n",
    "            all_used_labels_a0.append(used_labels_a0)\n",
    "            all_used_labels_a1.append(used_labels_a1)\n",
    "\n",
    "    predictions = np.vstack(all_preds_span)\n",
    "    \n",
    "    all_used_labels_p = np.vstack(all_used_labels_p)\n",
    "    all_used_labels_a0 = np.vstack(all_used_labels_a0)\n",
    "    all_used_labels_a1 = np.vstack(all_used_labels_a1)\n",
    "\n",
    "    # reshape from (iterator (1), num sentences 24, num spans 10, batch size 64, classes 15) to (batch size 64, num sentences 24, num spans 10, classes 15)\n",
    "    all_used_labels_p = all_used_labels_p.reshape(len(dataloader), batch_size, num_sentences, max_args_per_sentence, K)\n",
    "    all_used_labels_a0 = all_used_labels_a0.reshape(len(dataloader), batch_size, num_sentences, max_args_per_sentence, K)\n",
    "    all_used_labels_a1 = all_used_labels_a1.reshape(len(dataloader), batch_size, num_sentences, max_args_per_sentence, K)\n",
    "\n",
    "    return predictions, all_used_labels_p, all_used_labels_a0, all_used_labels_a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_sentences 24\n",
      "max_args_per_sentence 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0bc7972b844764a342f7ffe915a1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "predicted_labels, used_labels_p, used_labels_a0, used_labels_a1 = inspect(model, dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_lists_p = {category: [] for category in y_columns}\n",
    "category_lists_a1 = {category: [] for category in y_columns}\n",
    "category_lists_a0 = {category: [] for category in y_columns}\n",
    "\n",
    "batch_size = len(dataloader)\n",
    "\n",
    "boundary = 0.5\n",
    "\n",
    "for batch_idx in range(batch_size):\n",
    "\n",
    "    # Iterate over each sentence\n",
    "    ds = dataloader.dataset[batch_idx]\n",
    "\n",
    "    for elem_idx in range(used_labels_a0.shape[2]):\n",
    "    \n",
    "        for sentence_idx in range(len(ds[\"predicate_ids\"][elem_idx])):\n",
    "            # loop over spans\n",
    "            for span_idx in range(len(ds[\"predicate_ids\"][elem_idx][sentence_idx])):            \n",
    "                # Update the lists for each category\n",
    "                for cat_idx, category in enumerate(y_columns):\n",
    "                    if used_labels_p[batch_idx][elem_idx][sentence_idx][span_idx][cat_idx] > boundary:\n",
    "                        category_lists_p[category].append(ds[\"predicate_ids\"][sentence_idx][span_idx].int().numpy())\n",
    "                    \n",
    "                    if used_labels_a0[batch_idx][elem_idx][sentence_idx][span_idx][cat_idx] > boundary:\n",
    "                        category_lists_a0[category].append(ds[\"arg0_ids\"][sentence_idx][span_idx].int().numpy())\n",
    "                        \n",
    "                    if used_labels_a1[batch_idx][elem_idx][sentence_idx][span_idx][cat_idx] > boundary:\n",
    "                        category_lists_a1[category].append(ds[\"arg1_ids\"][sentence_idx][span_idx].int().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure you have downloaded the necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def decode_tokens(token_dict, tokenizer, remove_stopwords=False, lemmatize=False):\n",
    "    decoded_data = {}\n",
    "    stop_words = set(stopwords.words('english')) if remove_stopwords else set()\n",
    "    lemmatizer = WordNetLemmatizer() if lemmatize else None\n",
    "\n",
    "    for category, token_lists in token_dict.items():\n",
    "        decoded_data[category] = []\n",
    "        for tokens in token_lists:\n",
    "            if np.any(tokens > 0):\n",
    "                # Convert tokens to a list if it's a tensor or numpy array\n",
    "                if isinstance(tokens, torch.Tensor):\n",
    "                    tokens = tokens.tolist()\n",
    "                elif isinstance(tokens, np.ndarray):\n",
    "                    tokens = tokens.tolist()\n",
    "\n",
    "                # Decode the tokens\n",
    "                decoded_text = tokenizer.decode(tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "                # Remove non-alphabetic characters (but keep spaces)\n",
    "                decoded_text = re.sub(r'[^A-Za-z ]', '', decoded_text)\n",
    "\n",
    "                # Tokenize, optionally lemmatize, and remove stop words\n",
    "                words = word_tokenize(decoded_text)\n",
    "                processed_words = [lemmatizer.lemmatize(word.lower()) if lemmatizer else word.lower() for word in words if word.lower() not in stop_words]\n",
    "\n",
    "                # Join the words back into a string and ensure it's not empty\n",
    "                processed_text = ' '.join(processed_words)\n",
    "                if processed_text:\n",
    "                    decoded_data[category].append(processed_text)\n",
    "\n",
    "    return decoded_data\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# Decode the token IDs for each ARG\n",
    "decoded_predicate = decode_tokens(category_lists_p, tokenizer, remove_stopwords=True, lemmatize=True)\n",
    "decoded_arg0 = decode_tokens(category_lists_a0, tokenizer, remove_stopwords=True, lemmatize=True)\n",
    "decoded_arg1 = decode_tokens(category_lists_a1, tokenizer, remove_stopwords=True, lemmatize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_idf_per_category(data_dict):\n",
    "    \"\"\"\n",
    "    Calculate the inverse document frequency (IDF) for each word in each category in the data_dict.\n",
    "    Returns a dictionary with the same keys as the input, where the values are sorted lists of tuples (word, IDF score).\n",
    "    \"\"\"\n",
    "    category_idfs = {}\n",
    "\n",
    "    for category, documents in data_dict.items():\n",
    "        word_counts = {}\n",
    "        # Total number of documents in the current category\n",
    "        N = len(documents)\n",
    "\n",
    "        # Count the frequency of each word in the current category\n",
    "        for doc in documents:\n",
    "            unique_words = set(doc.split())\n",
    "            for word in unique_words:\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "        # Calculate IDF for each word in the current category\n",
    "        idfs = {word: math.log(N / (1 + count)) for word, count in word_counts.items()}\n",
    "\n",
    "        # Sort the idfs by score\n",
    "        sorted_idfs = sorted(idfs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        category_idfs[category] = sorted_idfs\n",
    "\n",
    "    return category_idfs\n",
    "\n",
    "\n",
    "# Calculate IDF scores for each category\n",
    "idf_predicate = calculate_idf_per_category(decoded_predicate)\n",
    "idf_arg0 = calculate_idf_per_category(decoded_arg0)\n",
    "idf_arg1 = calculate_idf_per_category(decoded_arg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_481/3975524772.py:34: FutureWarning: this method is deprecated in favour of `Styler.hide(axis=\"index\")`\n",
      "  df_full_table.style.hide_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f3bf1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_f3bf1_level0_col0\" class=\"col_heading level0 col0\" >Frame</th>\n",
       "      <th id=\"T_f3bf1_level0_col1\" class=\"col_heading level0 col1\" >Predicate</th>\n",
       "      <th id=\"T_f3bf1_level0_col2\" class=\"col_heading level0 col2\" >ARG0</th>\n",
       "      <th id=\"T_f3bf1_level0_col3\" class=\"col_heading level0 col3\" >ARG1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_f3bf1_row0_col0\" class=\"data row0 col0\" >Health and Safety</td>\n",
       "      <td id=\"T_f3bf1_row0_col1\" class=\"data row0 col1\" >arrested, gained, serve, care, sends</td>\n",
       "      <td id=\"T_f3bf1_row0_col2\" class=\"data row0 col2\" >immigrant, m holzer, many worker, ex producer, immigration agent</td>\n",
       "      <td id=\"T_f3bf1_row0_col3\" class=\"data row0 col3\" >number, nomination former senate sergeant, billboard, spanish speaking immigrant, step</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f3bf1_row1_col0\" class=\"data row1 col0\" >Capacity and Resources</td>\n",
       "      <td id=\"T_f3bf1_row1_col1\" class=\"data row1 col1\" >approved, help, estimate, show, paid</td>\n",
       "      <td id=\"T_f3bf1_row1_col2\" class=\"data row1 col2\" >immigrant, m holzer, many worker, ex producer, immigration agent</td>\n",
       "      <td id=\"T_f3bf1_row1_col3\" class=\"data row1 col3\" >standing ovation, nomination former senate sergeant, economy, offensive, in track foreign student five college</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f3bf1_row2_col0\" class=\"data row2 col0\" >External Regulation and Reputation</td>\n",
       "      <td id=\"T_f3bf1_row2_col1\" class=\"data row2 col1\" >depends, like, provoking, characterized</td>\n",
       "      <td id=\"T_f3bf1_row2_col2\" class=\"data row2 col2\" >hamburger chain, sept editorial, m holzer, sort xenophobia, intelligence</td>\n",
       "      <td id=\"T_f3bf1_row2_col3\" class=\"data row2 col3\" >immigrant worker, nation continuing debate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f3bf1_row3_col0\" class=\"data row3 col0\" >Legality, Constitutionality, Jurisdiction</td>\n",
       "      <td id=\"T_f3bf1_row3_col1\" class=\"data row3 col1\" >get, care, make, began, living</td>\n",
       "      <td id=\"T_f3bf1_row3_col2\" class=\"data row3 col2\" >million illegal alien, southeastern school, sort xenophobia, country, foreign student</td>\n",
       "      <td id=\"T_f3bf1_row3_col3\" class=\"data row3 col3\" >one serve, essential service, billboard, provide living wage, computer system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f3bf1_row4_col0\" class=\"data row4 col0\" >Crime and Punishment</td>\n",
       "      <td id=\"T_f3bf1_row4_col1\" class=\"data row4 col1\" >arrested, gained, serve, care, make</td>\n",
       "      <td id=\"T_f3bf1_row4_col2\" class=\"data row4 col2\" >immigrant, m holzer, immigration agent, step, senate majority leader thomas dasch</td>\n",
       "      <td id=\"T_f3bf1_row4_col3\" class=\"data row4 col3\" >standing ovation, managing director painew, step, economy, provide living wage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f3bf1_row5_col0\" class=\"data row5 col0\" >Cultural Identity</td>\n",
       "      <td id=\"T_f3bf1_row5_col1\" class=\"data row5 col1\" >arrested, gained, serve, sends, make</td>\n",
       "      <td id=\"T_f3bf1_row5_col2\" class=\"data row5 col2\" >immigrant, m holzer, many worker, ex producer, immigration agent</td>\n",
       "      <td id=\"T_f3bf1_row5_col3\" class=\"data row5 col3\" >standing ovation, in, number, nomination former senate sergeant, immigrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f3bf1_row6_col0\" class=\"data row6 col0\" >Economic</td>\n",
       "      <td id=\"T_f3bf1_row6_col1\" class=\"data row6 col1\" >gained, serve, drop, asked, show</td>\n",
       "      <td id=\"T_f3bf1_row6_col2\" class=\"data row6 col2\" >imm primary, sept editorial, immigration naturalization service, computer system, ziglar</td>\n",
       "      <td id=\"T_f3bf1_row6_col3\" class=\"data row6 col3\" >number, nomination former senate sergeant, immigrant, billboard, spanish speaking immigrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f3bf1_row7_col0\" class=\"data row7 col0\" >Quality of Life</td>\n",
       "      <td id=\"T_f3bf1_row7_col1\" class=\"data row7 col1\" >approved, serve, would, start, working</td>\n",
       "      <td id=\"T_f3bf1_row7_col2\" class=\"data row7 col2\" >sort xenophobia, engineer, one</td>\n",
       "      <td id=\"T_f3bf1_row7_col3\" class=\"data row7 col3\" >essential service, billboard, computer system, five college georgia, spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f3bf1_row8_col0\" class=\"data row8 col0\" >Fairness and Equality</td>\n",
       "      <td id=\"T_f3bf1_row8_col1\" class=\"data row8 col1\" >help, estimate, drop, working, clean</td>\n",
       "      <td id=\"T_f3bf1_row8_col2\" class=\"data row8 col2\" >juan escarfuller, mr lonegans</td>\n",
       "      <td id=\"T_f3bf1_row8_col3\" class=\"data row8 col3\" >in, thought, record success editor, managing director painew, assistant secretary interior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f3bf1_row9_col0\" class=\"data row9 col0\" >Security and Defense</td>\n",
       "      <td id=\"T_f3bf1_row9_col1\" class=\"data row9 col1\" >serve, care, stopped, sends, make</td>\n",
       "      <td id=\"T_f3bf1_row9_col2\" class=\"data row9 col2\" >step, native pascago, one, immigration naturalization service, immigrant worker</td>\n",
       "      <td id=\"T_f3bf1_row9_col3\" class=\"data row9 col3\" >one serve, essential service, billboard, provide living wage, computer system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f3bf1_row10_col0\" class=\"data row10 col0\" >Policy Prescription and Evaluation</td>\n",
       "      <td id=\"T_f3bf1_row10_col1\" class=\"data row10 col1\" >gained, serve, drop, asked, show</td>\n",
       "      <td id=\"T_f3bf1_row10_col2\" class=\"data row10 col2\" >immigrant, sept editorial, engineer, everyone undocum, one</td>\n",
       "      <td id=\"T_f3bf1_row10_col3\" class=\"data row10 col3\" >essential service, improvement nation, none thought pro, tracking system, threat deportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f3bf1_row11_col0\" class=\"data row11 col0\" >Morality</td>\n",
       "      <td id=\"T_f3bf1_row11_col1\" class=\"data row11 col1\" >approved, like, hold, stopped, run</td>\n",
       "      <td id=\"T_f3bf1_row11_col2\" class=\"data row11 col2\" >imm primary, private immigration relief bill, million illegal alien, everyone undocum, primary senate ok bush</td>\n",
       "      <td id=\"T_f3bf1_row11_col3\" class=\"data row11 col3\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f3bf1_row12_col0\" class=\"data row12 col0\" >Political</td>\n",
       "      <td id=\"T_f3bf1_row12_col1\" class=\"data row12 col1\" >arrested, gained, serve, care, stopped</td>\n",
       "      <td id=\"T_f3bf1_row12_col2\" class=\"data row12 col2\" >immigrant, m holzer, many worker, ex producer, immigration agent</td>\n",
       "      <td id=\"T_f3bf1_row12_col3\" class=\"data row12 col3\" >one serve, essential service, billboard, provide living wage, computer system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f3bf1_row13_col0\" class=\"data row13 col0\" >Public Sentiment</td>\n",
       "      <td id=\"T_f3bf1_row13_col1\" class=\"data row13 col1\" >arrested, serve, stopped, make, standing</td>\n",
       "      <td id=\"T_f3bf1_row13_col2\" class=\"data row13 col2\" >imm primary, importance part, private immigration relief bill, immigrant, million illegal alien</td>\n",
       "      <td id=\"T_f3bf1_row13_col3\" class=\"data row13 col3\" >standing ovation, in, number, one serve, nomination former senate sergeant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f3bf1_row14_col0\" class=\"data row14 col0\" >Other</td>\n",
       "      <td id=\"T_f3bf1_row14_col1\" class=\"data row14 col1\" ></td>\n",
       "      <td id=\"T_f3bf1_row14_col2\" class=\"data row14 col2\" >immigrant, many worker, ex producer, step, senate majority leader thomas dasch</td>\n",
       "      <td id=\"T_f3bf1_row14_col3\" class=\"data row14 col3\" >defraud immigrant, assistant secretary interior, offensive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f0e4ebaf670>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataframe_with_limit(decoded_predicate, decoded_arg0, decoded_arg1, word_limit):\n",
    "    # Initialize a list to collect DataFrame rows\n",
    "    rows = []\n",
    "\n",
    "    # Populate the list with rows\n",
    "    for frame in set(decoded_predicate) | set(decoded_arg0) | set(decoded_arg1):\n",
    "        # Get the lists, limiting the number of words and joining them with a comma\n",
    "        pred_words = ', '.join([s.strip() for s in list(set(decoded_predicate.get(frame, [])))[:word_limit] if s])\n",
    "        arg0_words = ', '.join(list(set(decoded_arg0.get(frame, [])))[:word_limit])\n",
    "        arg1_words = ', '.join(list(set(decoded_arg1.get(frame, [])))[:word_limit])\n",
    "\n",
    "        # Create a dictionary for the row\n",
    "        row = {\n",
    "            \"Frame\": frame,\n",
    "            \"Predicate\": pred_words,\n",
    "            \"ARG0\": arg0_words,\n",
    "            \"ARG1\": arg1_words\n",
    "        }\n",
    "        \n",
    "        # Append the row dictionary to the rows list\n",
    "        rows.append(row)\n",
    "\n",
    "    # Convert the list of rows to a DataFrame\n",
    "    df_full_table = pd.DataFrame(rows)\n",
    "\n",
    "    return df_full_table\n",
    "\n",
    "# Example usage\n",
    "# Assuming decoded_predicate, decoded_arg0, decoded_arg1 are already defined\n",
    "word_limit = 5  # Set your word limit here\n",
    "df_full_table = create_dataframe_with_limit(decoded_predicate, decoded_arg0, decoded_arg1, word_limit)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_full_table.style.hide_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_sentences 32\n",
      "max_args_per_sentence 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0870ca8135149209bae1cbbb0453884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "predicted_labels, used_labels_p, used_labels_a0, used_labels_a1 = inspect(model, example_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Capacity and Resources', 'Crime and Punishment', 'Cultural Identity',\n",
       "       'Economic', 'External Regulation and Reputation',\n",
       "       'Fairness and Equality', 'Health and Safety',\n",
       "       'Legality, Constitutionality, Jurisdiction', 'Morality', 'Other',\n",
       "       'Policy Prescription and Evaluation', 'Political', 'Public Sentiment',\n",
       "       'Quality of Life', 'Security and Defense'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>Predicted Frame: <span style='color: #F7CAC9;'>Economic</span><br></div>\n",
       "BILL ON IMMIGRANT WORKERS DIES. (p: , a0: , a1: )<br>Legislation to allow nearly twice as many computer-savvy foreigners and other high-skilled immigrants into the country next year apparently has died in Congress. (p: <span style=\"color: #45B8AC;\">allow</span>, a0: <span style=\"color: #FF6F61;\">legislation</span>, a1: <span style=\"color: #F7CAC9;\">legislation</span> <span style=\"color: #F7CAC9;\">to</span> <span style=\"color: #F7CAC9;\">allow</span> <span style=\"color: #F7CAC9;\">nearly</span> <span style=\"color: #F7CAC9;\">twice</span> <span style=\"color: #F7CAC9;\">as</span> <span style=\"color: #F7CAC9;\">many</span> <span style=\"color: #F7CAC9;\">computer</span> <span style=\"color: #F7CAC9;\">-</span> <span style=\"color: #F7CAC9;\">savvy</span> <span style=\"color: #F7CAC9;\">foreigners</span> <span style=\"color: #F7CAC9;\">and</span>)<br>The House passed the compromise measure last month, 288-133, but Sen. Tom Harkin, D-Iowa, had blocked a vote when in the Senate. (p: <span style=\"color: #EFC050;\">blocked</span>, a0: <span style=\"color: #EFC050;\">the</span> <span style=\"color: #EFC050;\">house</span>, <span style=\"color: #EFC050;\">sen.</span> <span style=\"color: #EFC050;\">tom</span> <span style=\"color: #EFC050;\">harkin,</span> <span style=\"color: #EFC050;\">d</span> <span style=\"color: #EFC050;\">-</span> <span style=\"color: #EFC050;\">iowa,</span>, a1: <span style=\"color: #EFC050;\">the</span> <span style=\"color: #EFC050;\">compromise</span> <span style=\"color: #EFC050;\">measure</span>, <span style=\"color: #EFC050;\">a</span> <span style=\"color: #EFC050;\">vote</span>)<br>The proposal, backed by high-tech companies, would raise the limit of so- called H-1B visas granted each year to skilled workers from abroad. (p: <span style=\"color: #45B8AC;\">backed</span>, <span style=\"color: #45B8AC;\">raise</span>, <span style=\"color: #45B8AC;\">granted</span>, a0: <span style=\"color: #FF6F61;\">by</span> <span style=\"color: #FF6F61;\">high</span> <span style=\"color: #FF6F61;\">-</span> <span style=\"color: #FF6F61;\">tech</span> <span style=\"color: #FF6F61;\">companies</span>, <span style=\"color: #FF6F61;\">the</span> <span style=\"color: #FF6F61;\">proposal,</span> <span style=\"color: #FF6F61;\">backed</span> <span style=\"color: #FF6F61;\">by</span> <span style=\"color: #FF6F61;\">high</span> <span style=\"color: #FF6F61;\">-</span> <span style=\"color: #FF6F61;\">tech</span> <span style=\"color: #FF6F61;\">companies,</span>, a1: )<br>Only 65,000 visas are now granted each year; the bill would raise the annual cap to 115,500 for the next two years and to 107,500 in 2001. (p: , a0: , a1: )<br>The ceiling would return to 65,000 in 2002. (p: , a0: , a1: )<br><br><br><div>Color Legend:<br>Policy Prescription and Evaluation: <span style='color: #45B8AC;'></span><br>Economic: <span style='color: #F7CAC9;'></span><br>Political: <span style='color: #EFC050;'></span><br>Capacity and Resources: <span style='color: #FF6F61;'></span><br></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Placeholder functions - you'll need to replace these with your actual functions\n",
    "def decode_token_ids(token_ids):\n",
    "    # Replace with your actual tokenizer decode call\n",
    "    return tokenizer.decode(token_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "category_colors = {\n",
    "    'Capacity and Resources': '#FF6F61',  # a vibrant coral\n",
    "    'Crime and Punishment': '#6B5B95',  # a deep violet\n",
    "    'Cultural Identity': '#88B04B',  # a leafy green\n",
    "    'Economic': '#F7CAC9',  # a soft pink\n",
    "    'External Regulation and Reputation': '#92A8D1',  # a light dusky blue\n",
    "    'Fairness and Equality': '#955251',  # a warm terra cotta\n",
    "    'Health and Safety': '#B565A7',  # a muted magenta\n",
    "    'Legality, Constitutionality, Jurisdiction': '#009B77',  # a medium teal\n",
    "    'Morality': '#DD4124',  # a bright red-orange\n",
    "    'Other': '#D2C29D',  # a neutral beige\n",
    "    'Policy Prescription and Evaluation': '#45B8AC',  # a seafoam green\n",
    "    'Political': '#EFC050',  # a golden yellow\n",
    "    'Public Sentiment': '#5B5EA6',  # a mid-tone periwinkle\n",
    "    'Quality of Life': '#9B2335',  # a rich burgundy\n",
    "    'Security and Defense': '#DFCFBE',  # a light taupe\n",
    "}\n",
    "\n",
    "# Function to color a word based on its category\n",
    "def color_word(word, category):\n",
    "    color = category_colors.get(category, 'black')  # Default to black if category not found\n",
    "    return f'<span style=\"color: {color};\">{word}</span>'\n",
    "\n",
    "\n",
    "# Function to process and color the article based on predictions\n",
    "def color_article(article, predicted_labels, used_predicates, used_arg0, used_arg1, y_columns, threshold = 0.8):\n",
    "\n",
    "    # get col name from 1 hot predicted_labels\n",
    "    frame = y_columns[np.where(predicted_labels[0] == 1)[0][0]]\n",
    "    \n",
    "    # Instead of printing, we'll build an HTML string\n",
    "    html_output = \"\"\n",
    "\n",
    "    # Print the predicted frame with color coding\n",
    "    frame_color = category_colors.get(frame, 'black')\n",
    "    html_output += f\"<div>Predicted Frame: <span style='color: {frame_color};'>{frame}</span><br></div>\"\n",
    "\n",
    "\n",
    "    # Split article into sentences using NLTK\n",
    "    sentences = nltk.sent_tokenize(article)\n",
    "    \n",
    "    predicted_frames = []\n",
    "\n",
    "    # Loop through the predictions and color the words accordingly\n",
    "    for sentence_idx in range(len(sentences)):\n",
    "        current_sent = sentences[sentence_idx]\n",
    "\n",
    "        current_predicates = []\n",
    "        current_arg0s = []\n",
    "        current_arg1s = []\n",
    "\n",
    "        for span_idx in range(used_predicates.shape[3]):\n",
    "            for cat_idx, category in enumerate(y_columns):\n",
    "                if used_predicates[0][0][sentence_idx][span_idx][cat_idx] > threshold:\n",
    "                    token_ids = example_dataset[0][\"predicate_ids\"][sentence_idx][span_idx].int().numpy()\n",
    "                    decoded_tokens = decode_token_ids(token_ids)\n",
    "                    decoded_words = decoded_tokens.split()\n",
    "\n",
    "                    # check if word not empty\n",
    "                    if decoded_words:\n",
    "                        predicted_frames.append(category)\n",
    "                        colored_words = [color_word(word, category) for word in decoded_words]\n",
    "                        colored_sentence = ' '.join(colored_words)\n",
    "                        current_predicates.append(colored_sentence)\n",
    "\n",
    "                if used_arg0[0][0][sentence_idx][span_idx][cat_idx] > threshold:\n",
    "                    token_ids = example_dataset[0][\"arg0_ids\"][sentence_idx][span_idx].int().numpy()\n",
    "                    decoded_tokens = decode_token_ids(token_ids)\n",
    "                    decoded_words = decoded_tokens.split()\n",
    "                \n",
    "                    # check if word not empty\n",
    "                    if decoded_words:\n",
    "                        predicted_frames.append(category)\n",
    "                        colored_words = [color_word(word, category) for word in decoded_words]\n",
    "                        colored_sentence = ' '.join(colored_words)\n",
    "                        current_arg0s.append(colored_sentence)\n",
    "\n",
    "                if used_arg1[0][0][sentence_idx][span_idx][cat_idx] > threshold:\n",
    "                    token_ids = example_dataset[0][\"arg1_ids\"][sentence_idx][span_idx].int().numpy()\n",
    "                    decoded_tokens = decode_token_ids(token_ids)\n",
    "                    decoded_words = decoded_tokens.split()\n",
    "                \n",
    "                    # check if word not empty\n",
    "                    if decoded_words:\n",
    "                        predicted_frames.append(category)\n",
    "                        colored_words = [color_word(word, category) for word in decoded_words]\n",
    "                        colored_sentence = ' '.join(colored_words)\n",
    "                        current_arg1s.append(colored_sentence)\n",
    "        \n",
    "        # Append to HTML output\n",
    "        html_output += f\"{current_sent} (p: {', '.join(current_predicates)}, a0: {', '.join(current_arg0s)}, a1: {', '.join(current_arg1s)})<br>\"\n",
    "\n",
    "    # Print color legend for used categories\n",
    "    html_output += \"<br><br><div>Color Legend:<br>\"\n",
    "\n",
    "    for category in set(predicted_frames):\n",
    "        html_output += f\"{category}: <span style='color: {category_colors[category]};'></span><br>\"\n",
    "    html_output += \"</div>\"\n",
    "\n",
    "    return html_output\n",
    "\n",
    "# Display HTML in Jupyter Notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(color_article(example_article, predicted_labels, used_labels_p, used_labels_a0, used_labels_a1, y_columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05c4f9fa65704ea5ba7f80e879e08510": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f54181f857a4110bf9976a56c97de97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fbfe002ccd541f9b6ab62ed4eebef35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1048f064e69b46d497b65cb9b88d0142": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc871ed94e764fe884ffdafca1445bfe",
       "IPY_MODEL_b4e7e77911e5408b84d89ccaff134708",
       "IPY_MODEL_c322e99ad9504207b7cfb9ae2ca9d6b1"
      ],
      "layout": "IPY_MODEL_dd92db5892be4357a6446146d9e9a0dd"
     }
    },
    "145c8616bab245358c8052beac5bd2bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20d6b378a69744e4a327d929c391b475": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "364a4257f8a641e18b7bde39e63a618d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "409a73804fc34a63a619a42b9468be46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "475020323c07410e87d366a6f553aafb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_145c8616bab245358c8052beac5bd2bc",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca901348dcd1428ebecc55d659fa40d1",
      "value": 28
     }
    },
    "4ddbb3fa3b924a9e9650f51204580f8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a5458551f974a94b61ff658ae59fa70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b48b8b79d6a4ff1b0655a6bfcf7a422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c06d626b5ee4fe8bdb9be7fb879aae0",
      "placeholder": "",
      "style": "IPY_MODEL_779202125255413cb719a43fe5d14ce7",
      "value": "Downloading ()solve/main/vocab.txt: 100%"
     }
    },
    "5d40c6e034fd4b278969dd928cc07dff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f4b55ead6884f3790a3acfa43232fc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87907508c4eb43f3a27858f1d397ca99",
      "placeholder": "",
      "style": "IPY_MODEL_d21ed2e527464a0ca0cb2b3b4fad928a",
      "value": "Downloading ()okenizer_config.json: 100%"
     }
    },
    "650f8e0f82e849f58a47507291b15500": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbb672669b9f4c36873048cfa1d2daf6",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cf42b39e3be94c90b3a63d05783481a3",
      "value": 231508
     }
    },
    "67ea6145091b440da6b4c5d5f8695aa9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ac495a816994aaeb8926c6c97d103ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ddbb3fa3b924a9e9650f51204580f8d",
      "placeholder": "",
      "style": "IPY_MODEL_a115ed03534a4c64a17fa55d93a0f93a",
      "value": " 570/570 [00:00&lt;00:00, 47.0kB/s]"
     }
    },
    "71add6bf108545af8db6aeb392793759": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5f4b55ead6884f3790a3acfa43232fc9",
       "IPY_MODEL_475020323c07410e87d366a6f553aafb",
       "IPY_MODEL_e2454ec2f5904e0bacde56ae7d4089a9"
      ],
      "layout": "IPY_MODEL_0f54181f857a4110bf9976a56c97de97"
     }
    },
    "779202125255413cb719a43fe5d14ce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87907508c4eb43f3a27858f1d397ca99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c06d626b5ee4fe8bdb9be7fb879aae0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e13dccd922946ab873cc287fcca5baf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93fa55d130db463c8ddffc947f2e99cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a6d16261d5b4693b09acf83dcb38920": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0265a21ae504485ab59207228c1f6d6",
      "placeholder": "",
      "style": "IPY_MODEL_8e13dccd922946ab873cc287fcca5baf",
      "value": " 232k/232k [00:00&lt;00:00, 9.51MB/s]"
     }
    },
    "a0265a21ae504485ab59207228c1f6d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a115ed03534a4c64a17fa55d93a0f93a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a74bacc051494d1ea0f4cbd656505cfe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4e7e77911e5408b84d89ccaff134708": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67ea6145091b440da6b4c5d5f8695aa9",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bef37b47c8ee49778464c05adcffa30d",
      "value": 466062
     }
    },
    "b83493088cd24629a04ec612aa522ed4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c222d2a5e4664f9b9aefed7f093d4a4e",
       "IPY_MODEL_fa94f90b735f418fb2b502f7877b5306",
       "IPY_MODEL_6ac495a816994aaeb8926c6c97d103ae"
      ],
      "layout": "IPY_MODEL_409a73804fc34a63a619a42b9468be46"
     }
    },
    "bef37b47c8ee49778464c05adcffa30d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c222d2a5e4664f9b9aefed7f093d4a4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_364a4257f8a641e18b7bde39e63a618d",
      "placeholder": "",
      "style": "IPY_MODEL_93fa55d130db463c8ddffc947f2e99cd",
      "value": "Downloading ()lve/main/config.json: 100%"
     }
    },
    "c322e99ad9504207b7cfb9ae2ca9d6b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d40c6e034fd4b278969dd928cc07dff",
      "placeholder": "",
      "style": "IPY_MODEL_f6b3c67d61114db5810bd78339a218d9",
      "value": " 466k/466k [00:00&lt;00:00, 1.88MB/s]"
     }
    },
    "ca901348dcd1428ebecc55d659fa40d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cf42b39e3be94c90b3a63d05783481a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d21ed2e527464a0ca0cb2b3b4fad928a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d568d9b894694b6fb432456031cee230": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbb672669b9f4c36873048cfa1d2daf6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd92db5892be4357a6446146d9e9a0dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2454ec2f5904e0bacde56ae7d4089a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20d6b378a69744e4a327d929c391b475",
      "placeholder": "",
      "style": "IPY_MODEL_05c4f9fa65704ea5ba7f80e879e08510",
      "value": " 28.0/28.0 [00:00&lt;00:00, 1.97kB/s]"
     }
    },
    "e9ee9acc5e414c65ad10b8864cc07b07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f19c187b1e8c4bbe9fda366f24f2b79e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5b48b8b79d6a4ff1b0655a6bfcf7a422",
       "IPY_MODEL_650f8e0f82e849f58a47507291b15500",
       "IPY_MODEL_9a6d16261d5b4693b09acf83dcb38920"
      ],
      "layout": "IPY_MODEL_d568d9b894694b6fb432456031cee230"
     }
    },
    "f6b3c67d61114db5810bd78339a218d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa94f90b735f418fb2b502f7877b5306": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a74bacc051494d1ea0f4cbd656505cfe",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0fbfe002ccd541f9b6ab62ed4eebef35",
      "value": 570
     }
    },
    "fc871ed94e764fe884ffdafca1445bfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a5458551f974a94b61ff658ae59fa70",
      "placeholder": "",
      "style": "IPY_MODEL_e9ee9acc5e414c65ad10b8864cc07b07",
      "value": "Downloading ()/main/tokenizer.json: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
