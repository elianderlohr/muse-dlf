{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Waumhn_ldoGu"
   },
   "source": [
    "## FRISS with MFC\n",
    "\n",
    "Implementation of the FRISS using the Media Frames Corpus (MFC) from Card et al. (2015). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = \"data/mfc/immigration_labeled.json\"\n",
    "codes_path = \"data/mfc/codes.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from path \n",
    "import json\n",
    "\n",
    "with open(labels_path) as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "with open(codes_path) as f:\n",
    "    codes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# articles list\n",
    "articles_list = []\n",
    "\n",
    "# Iterate through the data to fill the DataFrame\n",
    "for article_id, article_data in labels.items():\n",
    "    annotations_data = article_data['annotations']\n",
    "\n",
    "    irrelevant_dict = annotations_data['irrelevant']\n",
    "\n",
    "    text = article_data['text']\n",
    "    irrelevant = article_data['irrelevant']\n",
    "\n",
    "    # if primary_frame is none set to 15.0\n",
    "    if article_data['primary_frame'] is not None:\n",
    "        primary_frame = str(article_data['primary_frame']).split(\".\")[0] + \".0\"\n",
    "    else:\n",
    "        primary_frame = \"15.0\"\n",
    "\n",
    "    # get primary frame from code\n",
    "    primary_frame = str(codes[primary_frame])\n",
    "\n",
    "    # split text into sentences using nltk library\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # iterate through sentences\n",
    "    for sentence in sentences:\n",
    "        article = {\n",
    "            'article_id': article_id,\n",
    "            'irrelevant': irrelevant,\n",
    "            'text': sentence,\n",
    "            'document_frame': primary_frame\n",
    "        }\n",
    "\n",
    "        articles_list.append(article)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "df = pd.DataFrame(articles_list, columns=['article_id', 'irrelevant', 'text', 'document_frame'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1296
    },
    "executionInfo": {
     "elapsed": 3772,
     "status": "ok",
     "timestamp": 1696624002536,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "DG_Xix7gdoGy",
    "outputId": "d6fad26e-e6f7-4c20-f4bb-c7b01d51eb33",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[df[\"irrelevant\"] == False][[\"article_id\", \"text\", \"document_frame\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create for each code a col and fill with 1 if code is in code col\n",
    "df = pd.concat([df, pd.get_dummies(df['document_frame'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='transformers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract SRL Embeddings from articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pycuda\n",
    "!pip install allennlp allennlp-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get name / id of cuda device\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "cuda.init()\n",
    "device = cuda.Device(0)\n",
    "print(device.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def batched_extract_srl_components(batched_sentences, predictor):\n",
    "    # Convert each sentence into the required format for the predictor\n",
    "    batched_sentences = [{'sentence': sentence} for sentence in batched_sentences]\n",
    "\n",
    "    # Prepare the batched input for the predictor\n",
    "    batched_srl = predictor.predict_batch_json(batched_sentences)\n",
    "\n",
    "    # Extract SRL components from the batched predictions\n",
    "    results = []\n",
    "    for index, srl in enumerate(batched_srl):\n",
    "        sentence_results = []\n",
    "        for verb_entry in srl['verbs']:\n",
    "            arg_components = {'ARG0': [], 'ARG1': []}\n",
    "            for i, tag in enumerate(verb_entry['tags']):\n",
    "                if 'ARG0' in tag:\n",
    "                    arg_components['ARG0'].append(srl['words'][i])\n",
    "                elif 'ARG1' in tag:\n",
    "                    arg_components['ARG1'].append(srl['words'][i])\n",
    "\n",
    "            if arg_components['ARG0'] or arg_components['ARG1']:\n",
    "                sentence_results.append({\n",
    "                    'predicate': verb_entry['verb'],\n",
    "                    'ARG0': ' '.join(arg_components['ARG0']),\n",
    "                    'ARG1': ' '.join(arg_components['ARG1'])\n",
    "                })\n",
    "\n",
    "        if sentence_results:\n",
    "            # add empty dict if predicate, arg0 or arg1 is empty\n",
    "            if not sentence_results[0]['predicate']:\n",
    "                results.append({'predicate': '', 'ARG0': '', 'ARG1': ''})\n",
    "            elif not sentence_results[0]['ARG0']:\n",
    "                results.append({'predicate': '', 'ARG0': '', 'ARG1': ''})\n",
    "            elif not sentence_results[0]['ARG1']:\n",
    "                results.append({'predicate': '', 'ARG0': '', 'ARG1': ''})\n",
    "            else:\n",
    "                results.append(sentence_results)    \n",
    "        else:\n",
    "            results.append([{'predicate': '', 'ARG0': '', 'ARG1': ''}])\n",
    "\n",
    "    return results\n",
    "\n",
    "def optimized_extract_srl(X, predictor, batch_size=32):\n",
    "    all_results = []\n",
    "\n",
    "    # Process sentences in batches\n",
    "    for i in tqdm(range(0, len(X), batch_size), desc=\"Processing Batches\"):\n",
    "        batched_sentences = X[i:i+batch_size]\n",
    "\n",
    "        batch_results = batched_extract_srl_components(batched_sentences, predictor)\n",
    "\n",
    "        all_results.extend(batch_results)\n",
    "\n",
    "    return pd.Series(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_X_srl(X, recalculate=False, pickle_path=\"../notebooks/classifier/X_srl_filtered.pkl\"):\n",
    "    \"\"\"\n",
    "    Returns the X_srl either by loading from a pickled file or recalculating.\n",
    "    \"\"\"\n",
    "    if recalculate or not os.path.exists(pickle_path):\n",
    "        print(\"Recalculate SRL\")\n",
    "        # Load predictor\n",
    "        predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\", cuda_device=0)\n",
    "\n",
    "        # make sentences max 480 chars long\n",
    "        X = X.apply(lambda x: x[:480])\n",
    "\n",
    "        X_srl = optimized_extract_srl(X, predictor, batch_size=32)\n",
    "        with open(pickle_path, 'wb') as f:\n",
    "            pickle.dump(X_srl, f)\n",
    "    else:\n",
    "        print(\"Load SRL from Pickle\")\n",
    "        with tqdm(total=os.path.getsize(pickle_path)) as pbar:\n",
    "            with open(pickle_path, 'rb') as f:\n",
    "                X_srl = pickle.load(f)\n",
    "                pbar.update(os.path.getsize(pickle_path))\n",
    "                \n",
    "    return X_srl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_X_srl(df[\"text\"], recalculate=False, pickle_path=\"../notebooks/FRISS_srl.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def free_gpu():\n",
    "    print(torch.cuda.mem_get_info())\n",
    "    print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def list_gpu_tensors():\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                if obj.is_cuda:\n",
    "                    obj = obj.cpu()\n",
    "                    obj = obj.to(\"cpu\")\n",
    "                    print(type(obj), obj.size())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "list_gpu_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "class ArticleDataset(Dataset):\n",
    "    def __init__(self, X, X_srl, tokenizer, labels=None, max_sentences_per_article=32, max_sentence_length=32, max_args_per_sentence=10, max_arg_length=16):\n",
    "        self.X = X  # DataFrame where each row has multiple sentences\n",
    "        self.X_srl = X_srl  # DataFrame where each row has multiple dictionaries for SRL\n",
    "        self.labels = labels  # DataFrame where each row has a list of lists of integers\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sentences_per_article = max_sentences_per_article\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.max_args_per_sentence = max_args_per_sentence\n",
    "        self.max_arg_length = max_arg_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentences = self.X.iloc[idx]\n",
    "        srl_data = self.X_srl.iloc[idx]\n",
    "        labels = self.labels.iloc[idx]\n",
    "\n",
    "        # Tokenize sentences and get attention masks\n",
    "        sentence_ids, sentence_attention_masks = [], []\n",
    "        for sentence in sentences:\n",
    "            encoded = self.tokenizer(sentence, add_special_tokens=True, max_length=self.max_sentence_length, truncation=True, padding='max_length', return_attention_mask=True)\n",
    "            sentence_ids.append(encoded['input_ids'])\n",
    "            sentence_attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "        # Padding for sentences if necessary\n",
    "        while len(sentence_ids) < self.max_sentences_per_article:\n",
    "            sentence_ids.append([0] * self.max_sentence_length)\n",
    "            sentence_attention_masks.append([0] * self.max_sentence_length)\n",
    "\n",
    "        sentence_ids = sentence_ids[:self.max_sentences_per_article]\n",
    "        sentence_attention_masks = sentence_attention_masks[:self.max_sentences_per_article]\n",
    "\n",
    "        # Process SRL data\n",
    "        predicates, arg0s, arg1s = [], [], []\n",
    "        predicate_attention_masks, arg0_attention_masks, arg1_attention_masks = [], [], []\n",
    "        for srl_items in srl_data:\n",
    "            sentence_predicates, sentence_arg0s, sentence_arg1s = [], [], []\n",
    "            sentence_predicate_masks, sentence_arg0_masks, sentence_arg1_masks = [], [], []\n",
    "\n",
    "            if not isinstance(srl_items, list):\n",
    "                srl_items = [srl_items]\n",
    "\n",
    "            for item in srl_items:\n",
    "                encoded_predicate = self.tokenizer(item[\"predicate\"], add_special_tokens=True, max_length=self.max_arg_length, truncation=True, padding='max_length', return_attention_mask=True)\n",
    "                encoded_arg0 = self.tokenizer(item[\"ARG0\"], add_special_tokens=True, max_length=self.max_arg_length, truncation=True, padding='max_length', return_attention_mask=True)\n",
    "                encoded_arg1 = self.tokenizer(item[\"ARG1\"], add_special_tokens=True, max_length=self.max_arg_length, truncation=True, padding='max_length', return_attention_mask=True)\n",
    "\n",
    "                sentence_predicates.append(encoded_predicate['input_ids'])\n",
    "                sentence_arg0s.append(encoded_arg0['input_ids'])\n",
    "                sentence_arg1s.append(encoded_arg1['input_ids'])\n",
    "\n",
    "                sentence_predicate_masks.append(encoded_predicate['attention_mask'])\n",
    "                sentence_arg0_masks.append(encoded_arg0['attention_mask'])\n",
    "                sentence_arg1_masks.append(encoded_arg1['attention_mask'])\n",
    "\n",
    "            # Padding for SRL elements\n",
    "            for _ in range(self.max_args_per_sentence):\n",
    "                sentence_predicates.append([0] * self.max_arg_length)\n",
    "                sentence_arg0s.append([0] * self.max_arg_length)\n",
    "                sentence_arg1s.append([0] * self.max_arg_length)\n",
    "\n",
    "                sentence_predicate_masks.append([0] * self.max_arg_length)\n",
    "                sentence_arg0_masks.append([0] * self.max_arg_length)\n",
    "                sentence_arg1_masks.append([0] * self.max_arg_length)\n",
    "\n",
    "            sentence_predicates = sentence_predicates[:self.max_args_per_sentence]\n",
    "            sentence_arg0s = sentence_arg0s[:self.max_args_per_sentence]\n",
    "            sentence_arg1s = sentence_arg1s[:self.max_args_per_sentence]\n",
    "\n",
    "            sentence_predicate_masks = sentence_predicate_masks[:self.max_args_per_sentence]\n",
    "            sentence_arg0_masks = sentence_arg0_masks[:self.max_args_per_sentence]\n",
    "            sentence_arg1_masks = sentence_arg1_masks[:self.max_args_per_sentence]\n",
    "\n",
    "            predicates.append(sentence_predicates)\n",
    "            arg0s.append(sentence_arg0s)\n",
    "            arg1s.append(sentence_arg1s)\n",
    "\n",
    "            predicate_attention_masks.append(sentence_predicate_masks)\n",
    "            arg0_attention_masks.append(sentence_arg0_masks)\n",
    "            arg1_attention_masks.append(sentence_arg1_masks)\n",
    "\n",
    "        # Padding for SRL data\n",
    "        srl_padding = [[0] * self.max_arg_length] * self.max_args_per_sentence\n",
    "        mask_padding = [[0] * self.max_arg_length] * self.max_args_per_sentence\n",
    "\n",
    "        predicates = (predicates + [srl_padding] * self.max_sentences_per_article)[:self.max_sentences_per_article]\n",
    "        arg0s = (arg0s + [srl_padding] * self.max_sentences_per_article)[:self.max_sentences_per_article]\n",
    "        arg1s = (arg1s + [srl_padding] * self.max_sentences_per_article)[:self.max_sentences_per_article]\n",
    "\n",
    "        predicate_attention_masks = (predicate_attention_masks + [mask_padding] * self.max_sentences_per_article)[:self.max_sentences_per_article]\n",
    "        arg0_attention_masks = (arg0_attention_masks + [mask_padding] * self.max_sentences_per_article)[:self.max_sentences_per_article]\n",
    "        arg1_attention_masks = (arg1_attention_masks + [mask_padding] * self.max_sentences_per_article)[:self.max_sentences_per_article]\n",
    "\n",
    "        data = {\n",
    "            'sentence_ids': torch.tensor(sentence_ids, dtype=torch.long),\n",
    "            'sentence_attention_masks': torch.tensor(sentence_attention_masks, dtype=torch.long),\n",
    "            'predicate_ids': torch.tensor(predicates, dtype=torch.long),\n",
    "            'predicate_attention_masks': torch.tensor(predicate_attention_masks, dtype=torch.long),\n",
    "            'arg0_ids': torch.tensor(arg0s, dtype=torch.long),\n",
    "            'arg0_attention_masks': torch.tensor(arg0_attention_masks, dtype=torch.long),\n",
    "            'arg1_ids': torch.tensor(arg1s, dtype=torch.long),\n",
    "            'arg1_attention_masks': torch.tensor(arg1_attention_masks, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels[0], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Extract individual lists from the batch\n",
    "    sentence_ids = [item['sentence_ids'] for item in batch]\n",
    "    sentence_attention_masks = [item['sentence_attention_masks'] for item in batch]\n",
    "    predicate_ids = [item['predicate_ids'] for item in batch]\n",
    "    predicate_attention_masks = [item['predicate_attention_masks'] for item in batch]\n",
    "    arg0_ids = [item['arg0_ids'] for item in batch]\n",
    "    arg0_attention_masks = [item['arg0_attention_masks'] for item in batch]\n",
    "    arg1_ids = [item['arg1_ids'] for item in batch]\n",
    "    arg1_attention_masks = [item['arg1_attention_masks'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Pad each list\n",
    "    sentence_ids = torch.nn.utils.rnn.pad_sequence(sentence_ids, batch_first=True, padding_value=0)\n",
    "    sentence_attention_masks = torch.nn.utils.rnn.pad_sequence(sentence_attention_masks, batch_first=True, padding_value=0)\n",
    "    predicate_ids = torch.nn.utils.rnn.pad_sequence(predicate_ids, batch_first=True, padding_value=0)\n",
    "    predicate_attention_masks = torch.nn.utils.rnn.pad_sequence(predicate_attention_masks, batch_first=True, padding_value=0)\n",
    "    arg0_ids = torch.nn.utils.rnn.pad_sequence(arg0_ids, batch_first=True, padding_value=0)\n",
    "    arg0_attention_masks = torch.nn.utils.rnn.pad_sequence(arg0_attention_masks, batch_first=True, padding_value=0)\n",
    "    arg1_ids = torch.nn.utils.rnn.pad_sequence(arg1_ids, batch_first=True, padding_value=0)\n",
    "    arg1_attention_masks = torch.nn.utils.rnn.pad_sequence(arg1_attention_masks, batch_first=True, padding_value=0)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Create the output dictionary\n",
    "    output_dict = {\n",
    "        'sentence_ids': sentence_ids,\n",
    "        'sentence_attention_masks': sentence_attention_masks,\n",
    "        'predicate_ids': predicate_ids,\n",
    "        'predicate_attention_masks': predicate_attention_masks,\n",
    "        'arg0_ids': arg0_ids,\n",
    "        'arg0_attention_masks': arg0_attention_masks,\n",
    "        'arg1_ids': arg1_ids,\n",
    "        'arg1_attention_masks': arg1_attention_masks,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def preprocess_df(df, recalculate_srl=False, pickle_path=\"../notebooks/FRISS_srl.pkl\"):\n",
    "    # reset index of df\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Get X_srl\n",
    "    X_srl = get_X_srl(df[\"text\"], recalculate=recalculate_srl, pickle_path=pickle_path)\n",
    "\n",
    "    # reset index of X_srl\n",
    "    X_srl = X_srl.reset_index(drop=True)\n",
    "\n",
    "    # Columns to be one-hot encoded in y_subset\n",
    "    y_cols = ['Capacity and Resources', 'Crime and Punishment', 'Cultural Identity', \n",
    "            'Economic', 'External Regulation and Reputation', 'Fairness and Equality', \n",
    "            'Health and Safety', 'Legality, Constitutionality, Jurisdiction', \n",
    "            'Morality', 'Other', 'Policy Prescription and Evaluation', 'Political', \n",
    "            'Public Sentiment', 'Quality of Life', 'Security and Defense']\n",
    "\n",
    "    # Creating y_subset\n",
    "    y_subset = df.groupby('article_id')[y_cols].apply(lambda x: x.values.tolist()).reset_index(name='encoded_values')\n",
    "    y_subset = y_subset['encoded_values']\n",
    "\n",
    "    # Aggregating 'text' column in df into a list of strings for each article_id\n",
    "    X_subset = df.groupby('article_id')['text'].apply(list).reset_index(name='text')\n",
    "    X_subset = X_subset['text']\n",
    "\n",
    "    # Assuming X_srl follows the same index order as df\n",
    "    X_srl_subset = X_srl.groupby(df['article_id']).apply(lambda x: x.values.tolist()).reset_index(name='srl_values')\n",
    "    X_srl_subset = X_srl_subset['srl_values']\n",
    "\n",
    "    return X_subset, X_srl_subset, y_subset\n",
    "\n",
    "def get_datasets_dataloaders(df, tokenizer, recalculate_srl=False, pickle_path=\"../notebooks/FRISS_srl.pkl\", batch_size=16, max_sentences_per_article=32, max_sentence_length=32,  max_args_per_sentence=10,  max_arg_length=16, test_size=0.1):\n",
    "    \n",
    "    X_subset, X_srl_subset, y_subset = preprocess_df(df, recalculate_srl=recalculate_srl, pickle_path=pickle_path)\n",
    "\n",
    "    # Len\n",
    "    print(\"X:\", len(X_subset))\n",
    "    print(\"X_srl:\", len(X_srl_subset))\n",
    "    print(\"y:\", len(y_subset))\n",
    "\n",
    "    print(\"CREATING DATASETS\")\n",
    "    \n",
    "    # Assuming X, X_srl, and y are already defined and have the same number of samples\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_subset, y_subset, test_size=test_size, random_state=42)\n",
    "    \n",
    "    print(\"TRAIN TEST SPLIT DONE\")\n",
    "    \n",
    "    X_srl_train, X_srl_test, _, _ = train_test_split(X_srl_subset, y_subset, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Create the dataset\n",
    "    train_dataset = ArticleDataset(X_train, X_srl_train, tokenizer, y_train, max_sentences_per_article, max_sentence_length,  max_args_per_sentence, max_arg_length)\n",
    "    test_dataset = ArticleDataset(X_test, X_srl_test, tokenizer, y_test, max_sentences_per_article, max_sentence_length, max_args_per_sentence, max_arg_length)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "    \n",
    "    print(\"CREATION DONE\")\n",
    "    return train_dataset, test_dataset , train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "max_sentences_per_article = 24\n",
    "max_sentence_length = 32\n",
    "\n",
    "max_args_per_sentence = 10\n",
    "max_arg_length = 8\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "train_dataset, test_dataset, train_dataloader, test_dataloader = get_datasets_dataloaders(df, tokenizer, recalculate_srl=False, batch_size=batch_size, max_sentences_per_article=max_sentences_per_article, max_sentence_length=max_sentence_length, max_arg_length=max_arg_length, pickle_path=\"../notebooks/FRISS_srl.pkl\", test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_dataloader(article, tokenizer, batch_size=1):\n",
    "    X = pd.Series([article])\n",
    "    y = None  # No labels for this single article\n",
    "    \n",
    "    predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\", cuda_device=0)\n",
    "    # Directly use the optimized_extract_srl function since we don't need to cache for single articles\n",
    "    X_srl = optimized_extract_srl(X, predictor)\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = ArticleDataset(X, X_srl, tokenizer, y)\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_dataloader(X, tokenizer, batch_size=4):\n",
    "    y = None\n",
    "    \n",
    "    predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\", cuda_device=0)\n",
    "    # Directly use the optimized_extract_srl function since we don't need to cache for single articles\n",
    "    X_srl = optimized_extract_srl(X, predictor)\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = ArticleDataset(X, X_srl, tokenizer, y)\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Model\n",
    "The Model consist out of various Layers.\n",
    "\n",
    "1. SRL_Embedding\n",
    "2. Autoencoder\n",
    "3. FRISSLoss\n",
    "4. Unsupervised\n",
    "5. Supervised\n",
    "6. FRISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SRL_Embeddings\n",
    "\n",
    "The layer takes tensors of token IDs with the shape [batch_size, max_num_sentences, max_num_tokens] for the sentence, predicates, arg0 and arg1 and returns for each sentence an embedding with shape [batch_size, embedding_dim] for the sentence, predicate, arg0 and arg1. \n",
    "\n",
    "The single embedding for the sentence is extracted by taking the [CLS] token embedding. For the predicate, arg0 and arg1 by taking the mean over all word embeddings in this list of tokens. \n",
    "\n",
    "> Possible improvements: Better way of extracting the single embedding for predicate, arg0 and arg1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class SRL_Embeddings(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"bert-base-uncased\"):\n",
    "        super(SRL_Embeddings, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "        self.embedding_dim = 768  # for bert-base-uncased\n",
    "\n",
    "    def forward(self, sentence_ids, sentence_attention_masks, predicate_ids, predicate_attention_masks, arg0_ids, arg0_attention_masks, arg1_ids, arg1_attention_masks):\n",
    "        with torch.no_grad():\n",
    "            # Sentence embeddings\n",
    "            sentence_embeddings = self.bert_model(input_ids=sentence_ids.view(-1, sentence_ids.size(-1)), \n",
    "                                                  attention_mask=sentence_attention_masks.view(-1, sentence_attention_masks.size(-1)))[0]\n",
    "            sentence_embeddings = sentence_embeddings.view(sentence_ids.size(0), sentence_ids.size(1), -1, self.embedding_dim)\n",
    "            sentence_embeddings = sentence_embeddings.mean(dim=2)\n",
    "\n",
    "            # Predicate embeddings\n",
    "            predicate_embeddings = self.bert_model(input_ids=predicate_ids.view(-1, predicate_ids.size(-1)), \n",
    "                                                   attention_mask=predicate_attention_masks.view(-1, predicate_attention_masks.size(-1)))[0]\n",
    "            predicate_embeddings = predicate_embeddings.view(predicate_ids.size(0), predicate_ids.size(1), predicate_ids.size(2), -1, self.embedding_dim)\n",
    "            predicate_embeddings = predicate_embeddings.mean(dim=3)\n",
    "\n",
    "            # ARG0 embeddings\n",
    "            arg0_embeddings = self.bert_model(input_ids=arg0_ids.view(-1, arg0_ids.size(-1)), \n",
    "                                              attention_mask=arg0_attention_masks.view(-1, arg0_attention_masks.size(-1)))[0]\n",
    "            arg0_embeddings = arg0_embeddings.view(arg0_ids.size(0), arg0_ids.size(1), arg0_ids.size(2), -1, self.embedding_dim)\n",
    "            arg0_embeddings = arg0_embeddings.mean(dim=3)\n",
    "\n",
    "            # ARG1 embeddings\n",
    "            arg1_embeddings = self.bert_model(input_ids=arg1_ids.view(-1, arg1_ids.size(-1)), \n",
    "                                              attention_mask=arg1_attention_masks.view(-1, arg1_attention_masks.size(-1)))[0]\n",
    "            arg1_embeddings = arg1_embeddings.view(arg1_ids.size(0), arg1_ids.size(1), arg1_ids.size(2), -1, self.embedding_dim)\n",
    "            arg1_embeddings = arg1_embeddings.mean(dim=3)\n",
    "\n",
    "        return sentence_embeddings, predicate_embeddings, arg0_embeddings, arg1_embeddings\n",
    "\n",
    "# Generate dummy data for the SRL_Embeddings\n",
    "batch_size = 2\n",
    "num_sentences = 12\n",
    "sentence_length = 8\n",
    "num_args = 9\n",
    "predicate_length = 8\n",
    "arg0_length = 8\n",
    "arg1_length = 8\n",
    "\n",
    "# Dummy data for sentences, predicates, arg0, and arg1\n",
    "sentence_ids = torch.randint(0, 10000, (batch_size, num_sentences, sentence_length))\n",
    "predicate_ids = torch.randint(0, 10000, (batch_size, num_sentences, num_args, predicate_length))\n",
    "arg0_ids = torch.randint(0, 10000, (batch_size, num_sentences, num_args, arg0_length))\n",
    "arg1_ids = torch.randint(0, 10000, (batch_size, num_sentences, num_args, arg1_length))\n",
    "\n",
    "# Mock attention masks\n",
    "sentence_attention_masks = torch.randint(0, 2, (batch_size, num_sentences, sentence_length))\n",
    "predicate_attention_masks = torch.randint(0, 2, (batch_size, num_sentences, num_args, predicate_length))\n",
    "arg0_attention_masks = torch.randint(0, 2, (batch_size, num_sentences, num_args, arg0_length))\n",
    "arg1_attention_masks = torch.randint(0, 2, (batch_size, num_sentences, num_args, arg1_length))\n",
    "\n",
    "srl_embeddings = SRL_Embeddings()\n",
    "\n",
    "sentence_embeddings, predicate_embeddings, arg0_embeddings, arg1_embeddings = srl_embeddings(\n",
    "    sentence_ids, sentence_attention_masks, \n",
    "    predicate_ids, predicate_attention_masks, \n",
    "    arg0_ids, arg0_attention_masks, \n",
    "    arg1_ids, arg1_attention_masks\n",
    ")\n",
    "\n",
    "print(\"Inputs shapes: \", sentence_ids.shape, predicate_ids.shape, arg0_ids.shape, arg1_ids.shape)\n",
    "print(\"Outputs shapes: \", sentence_embeddings.shape, predicate_embeddings.shape, arg0_embeddings.shape, arg1_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import log_softmax, softmax\n",
    "\n",
    "class CombinedAutoencoder(nn.Module):\n",
    "    def __init__(self, D_w, D_h, K, dropout_prob=0.3):\n",
    "        super(CombinedAutoencoder, self).__init__()\n",
    "        \n",
    "        self.D_h = D_h\n",
    "        self.K = K\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        # Shared feed-forward layer for all views\n",
    "        self.feed_forward_shared = nn.Linear(2 * D_w, D_h)\n",
    "        \n",
    "        # Unique feed-forward layers for each view\n",
    "        self.feed_forward_unique = nn.ModuleDict({\n",
    "            'a0': nn.Linear(D_h, K),\n",
    "            'p': nn.Linear(D_h, K),\n",
    "            'a1': nn.Linear(D_h, K),\n",
    "        })\n",
    "\n",
    "        # Initializing F matrices for each view\n",
    "        self.F_matrices = nn.ParameterDict({\n",
    "            'a0': nn.Parameter(torch.Tensor(K, D_w)),\n",
    "            'p': nn.Parameter(torch.Tensor(K, D_w)),\n",
    "            'a1': nn.Parameter(torch.Tensor(K, D_w)),\n",
    "        })\n",
    "\n",
    "        # init F matrices with xavier_uniform and nn.init.calculate_gain('relu')\n",
    "        for _, value in self.F_matrices.items():\n",
    "            nn.init.xavier_uniform_(value.data, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        # Additional layers and parameters\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        self.batch_norm = nn.BatchNorm1d(D_h)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.activation2 = nn.Sigmoid()\n",
    "\n",
    "    def sample_gumbel(self, shape, eps=1e-20, device='cpu'):\n",
    "        \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "        U = torch.rand(shape, device=device)\n",
    "        return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "    def gumbel_softmax_sample(self, logits, t):\n",
    "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "        y = logits + self.sample_gumbel(logits.size(), device=logits.device)\n",
    "        return softmax(y / t, dim=-1)\n",
    "\n",
    "\n",
    "    def gumbel_logsoftmax_sample(self, logits, t):\n",
    "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "        y = logits + self.sample_gumbel(logits.size(), device=logits.device)\n",
    "        return log_softmax(y / t, dim=-1)\n",
    "\n",
    "\n",
    "    def custom_gumbel_softmax(self, logits, tau, hard=False, log=False):\n",
    "        \"\"\"Sample from the Gumbel-Softmax distribution and optionally discretize.\n",
    "        Args:\n",
    "        logits: [batch_size, n_class] unnormalized log-probs\n",
    "        tau: non-negative scalar\n",
    "        hard: if True, take argmax, but differentiate w.r.t. soft sample y\n",
    "        Returns:\n",
    "        [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n",
    "        If hard=True, then the returned sample will be one-hot, otherwise it will\n",
    "        be a probabilitiy distribution that sums to 1 across classes\n",
    "        \"\"\"\n",
    "        if log:\n",
    "            y = self.gumbel_logsoftmax_sample(logits, tau)\n",
    "        else:\n",
    "            y = self.gumbel_softmax_sample(logits, tau)\n",
    "        if hard:\n",
    "            shape = y.size()\n",
    "            _, ind = y.max(dim=-1)\n",
    "            y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "            y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "            y_hard = y_hard.view(*shape)\n",
    "            # Set gradients w.r.t. y_hard gradients w.r.t. y\n",
    "            y_hard = (y_hard - y).detach() + y\n",
    "            return y_hard\n",
    "        return y\n",
    "\n",
    "    def forward(self, v_p, v_a0, v_a1, v_sentence, tau):\n",
    "        h_p = self.process_through_shared(v_p, v_sentence)\n",
    "        h_a0 = self.process_through_shared(v_a0, v_sentence)\n",
    "        h_a1 = self.process_through_shared(v_a1, v_sentence)\n",
    "\n",
    "        logits_p = self.feed_forward_unique['p'](h_p)\n",
    "        logits_a0 = self.feed_forward_unique['a0'](h_a0)\n",
    "        logits_a1 = self.feed_forward_unique['a1'](h_a1) \n",
    "\n",
    "        d_p = torch.softmax(logits_p, dim=1)\n",
    "        d_a0 = torch.softmax(logits_a0, dim=1)\n",
    "        d_a1 = torch.softmax(logits_a1, dim=1)\n",
    "        \n",
    "        # TODO - Paper said we pass the output of softmax into the Gumbel-Softmax but code passes the logits\n",
    "\n",
    "        # gz_p = self.custom_gumbel_softmax(dz_p, tau=tau, hard=False, log=True)\n",
    "        # gz_a0 = self.custom_gumbel_softmax(dz_a0, tau=tau, hard=False, log=True)\n",
    "        # gz_a1 = self.custom_gumbel_softmax(dz_a1, tau=tau, hard=False, log=True)\n",
    "\n",
    "        g_p = self.custom_gumbel_softmax(logits_p, tau=tau, hard=False, log=True)\n",
    "        g_a0 = self.custom_gumbel_softmax(logits_a0, tau=tau, hard=False, log=True)\n",
    "        g_a1 = self.custom_gumbel_softmax(logits_a1, tau=tau, hard=False, log=True)\n",
    "\n",
    "        vhat_p = torch.matmul(g_p, self.F_matrices['p'])\n",
    "        vhat_a0 = torch.matmul(g_a0, self.F_matrices['a0'])\n",
    "        vhat_a1 = torch.matmul(g_a1, self.F_matrices['a1'])\n",
    "\n",
    "        return {\n",
    "            \"p\": {\"vhat\": vhat_p, \"d\": d_p, \"g\": g_p, \"F\": self.F_matrices['p']},\n",
    "            \"a0\": {\"vhat\": vhat_a0, \"d\": d_a0, \"g\": g_a0, \"F\": self.F_matrices['a0']},\n",
    "            \"a1\": {\"vhat\": vhat_a1, \"d\": d_a1, \"g\": g_a1, \"F\": self.F_matrices['a1']}\n",
    "        }\n",
    "        \n",
    "    def process_through_shared(self, v_z, v_sentence):\n",
    "        # Concatenating v_z with the sentence embedding\n",
    "        concatenated = torch.cat((v_z, v_sentence), dim=-1)\n",
    "        \n",
    "        # Applying dropout\n",
    "        dropped = self.dropout1(concatenated)\n",
    "\n",
    "        # Passing through the shared linear layer\n",
    "        h_shared = self.feed_forward_shared(dropped)\n",
    "\n",
    "        # Applying batch normalization and ReLU activation\n",
    "        h_shared = self.batch_norm(h_shared)\n",
    "        h_shared = self.activation(h_shared)\n",
    "\n",
    "        # Applying dropout again\n",
    "        h_shared = self.dropout2(h_shared)\n",
    "\n",
    "        return h_shared\n",
    "\n",
    "# Mock Data Preparation\n",
    "D_h = 768\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "K = 20\n",
    "tau = 0.9\n",
    "\n",
    "# Generating mock embeddings for article, predicate, ARG0, ARG1, and their corresponding sentence embeddings\n",
    "article_embedding = torch.randn(batch_size, embedding_dim)\n",
    "v_p = torch.randn(batch_size, embedding_dim)\n",
    "v_a0 = torch.randn(batch_size, embedding_dim)\n",
    "v_a1 = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "# Testing CombinedAutoencoder\n",
    "autoencoder = CombinedAutoencoder(embedding_dim, D_h, K)\n",
    "outputs = autoencoder(v_p, v_a0, v_a1, article_embedding, tau)\n",
    "\n",
    "# Check shapes of the outputs\n",
    "print(\"Output shapes:\")\n",
    "for key, value in outputs.items():\n",
    "    print(f\"{key} -> vhat: {value['vhat'].shape}, d: {value['d'].shape}, g: {value['g'].shape}, F: {value['F'].shape}\")\n",
    "\n",
    "# check if tensor have nan values\n",
    "def check_nan(tensor):\n",
    "    # if tensor has any nan values, return True\n",
    "    if torch.isnan(tensor).any():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Check if any of the outputs have NaN values\n",
    "print(\"NaN values:\")\n",
    "for key, value in outputs.items():\n",
    "    print(f\"{key} -> vhat: {check_nan(value['vhat'])}, d: {check_nan(value['d'])}, g: {check_nan(value['g'])}, F: {check_nan(value['F'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FRISSLoss\n",
    "\n",
    "The layer calculates the unsupervised loss for predicate, arg0 and arg1. \n",
    "\n",
    "The forward function takes as input 3 dicts with the parameters `v`, `v_hat`, `g` and `F`. Where `v` is the embedding of the predicate, arg0 or arg1. The `v_hat` (size: [batch_size, embedding_dim]) is the reconstructed embedding for the predicate, arg0 and arg1. The `g` is the gumbel softmax result (size: [batch_size, embedding_dim]). The `F` (size: [K, embedding_dim]) which is the descriptor dictionary.\n",
    "\n",
    "The layer returns the loss for each batch. So the output is [batch_size]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FRISSLoss(nn.Module):\n",
    "    def __init__(self, lambda_orthogonality, M, t):\n",
    "        super(FRISSLoss, self).__init__()\n",
    "        \n",
    "        self.lambda_orthogonality = lambda_orthogonality\n",
    "        self.M = M\n",
    "        self.t = t\n",
    "        self.triplet_loss = nn.TripletMarginLoss(margin=M)\n",
    "\n",
    "    def contrastive_loss(self, v, vhat, negatives):\n",
    "        batch_size = vhat.size(0)\n",
    "        N = negatives.size(0)\n",
    "        loss = torch.zeros(batch_size, device=v.device)\n",
    "\n",
    "        # Calculate true distance between reconstructed and real embeddings\n",
    "        true_distance = self.l2(vhat, v)\n",
    "\n",
    "        for i in range(N):  # loop over each element in \"negatives\"\n",
    "            \n",
    "            # Tranform negative from [embedding dim] to [batch size, embedding_dim] \n",
    "            negative = negatives[i, :].expand(v.size(0), -1)\n",
    "\n",
    "            # Calculate negative distance for current negative embedding\n",
    "            negative_distance = self.l2(vhat, negative)\n",
    "\n",
    "            # Compute loss based on the provided logic: l2(vhat, v) + 1 + l2(vhat, negative) and clamp to 0 if below 0\n",
    "            current_loss = 1 + true_distance - negative_distance\n",
    "            loss += torch.clamp(current_loss, min=0.0)\n",
    "\n",
    "        # Normalize the total loss by N\n",
    "        return loss / N\n",
    "\n",
    "    \n",
    "    def l2(self, u, v):\n",
    "        return torch.sqrt(torch.sum((u - v) ** 2, dim=1))\n",
    "    \n",
    "    def focal_triplet_loss_WRONG(self, v, vhat_z, g, F):\n",
    "        losses = []\n",
    "        for i in range(F.size(0)):  # Iterate over each negative example\n",
    "            # For each negative, compute the loss against the anchor and positive\n",
    "            loss = self.triplet_loss(vhat_z, v, F[i].unsqueeze(0).expand(v.size(0), -1))\n",
    "            losses.append(loss)\n",
    "\n",
    "        loss_tensor = torch.stack(losses) \n",
    "        loss = loss_tensor.mean(dim=0).mean()\n",
    "        return loss\n",
    "    \n",
    "    def focal_triplet_loss(self, v, vhat_z, g, F):\n",
    "        _, indices = torch.topk(g, self.t, largest=False, dim=1)\n",
    "\n",
    "        F_t = torch.stack([F[indices[i]] for i in range(g.size(0))])\n",
    "\n",
    "        g_tz = torch.stack([g[i, indices[i]] for i in range(g.size(0))])\n",
    "                    \n",
    "        g_t = g_tz / g_tz.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # if division by zero set all nan values to 0\n",
    "        g_t[torch.isnan(g_t)] = 0\n",
    "        \n",
    "        m_t = self.M * ((1 - g_t)**2)\n",
    "\n",
    "        # Initializing loss\n",
    "        loss = torch.zeros_like(v[:, 0])\n",
    "        \n",
    "        # Iteratively adding to the loss for each negative embedding\n",
    "        for i in range(self.t):\n",
    "            current_v_t = F_t[:, i]\n",
    "            current_m_t = m_t[:, i]\n",
    "            \n",
    "            current_loss = current_m_t + self.l2(vhat_z, v) - self.l2(vhat_z, current_v_t)\n",
    "            \n",
    "            loss += torch.max(torch.zeros_like(current_loss), current_loss)\n",
    "             \n",
    "        # Normalizing\n",
    "        loss = loss / self.t\n",
    "        return loss\n",
    "\n",
    "    def orthogonality_term(self, F, reg=1e-4):\n",
    "        gram_matrix = torch.mm(F, F.T)  # Compute the Gram matrix F * F^T\n",
    "        identity_matrix = torch.eye(gram_matrix.size(0), device=gram_matrix.device)  # Create an identity matrix\n",
    "        ortho_loss = (gram_matrix - identity_matrix).abs().sum()\n",
    "        return ortho_loss\n",
    "\n",
    "\n",
    "    def forward(self, p, a0, a1, p_negatives, a0_negatives, a1_negatives):\n",
    "        # Extract components from dictionary for predicate p\n",
    "        v_p, vhat_p, d_p, g_p, F_p = p[\"v\"], p[\"vhat\"], p[\"d\"], p[\"g\"], p[\"F\"]\n",
    "        \n",
    "        # Extract components from dictionary for ARG0\n",
    "        v_a0, vhat_a0, d_a0, g_a0, F_a0 = a0[\"v\"], a0[\"vhat\"], a0[\"d\"], a0[\"g\"], a0[\"F\"]\n",
    "\n",
    "        # Extract components from dictionary for ARG1\n",
    "        v_a1, vhat_a1, d_a1, g_a1, F_a1 = a1[\"v\"], a1[\"vhat\"], a1[\"d\"], a1[\"g\"], a1[\"F\"]\n",
    "        \n",
    "         # Calculate losses for predicate\n",
    "        Ju_p = self.contrastive_loss(v_p, vhat_p, p_negatives)        \n",
    "        Jt_p = self.focal_triplet_loss(v_p, vhat_p, g_p, F_p)        \n",
    "        Jz_p = Ju_p + Jt_p + self.lambda_orthogonality * self.orthogonality_term(F_p) ** 2\n",
    "        \n",
    "        # Calculate losses for ARG0\n",
    "        Ju_a0 = self.contrastive_loss(v_a0, vhat_a0, a0_negatives)\n",
    "        Jt_a0 = self.focal_triplet_loss(v_a0, vhat_a0, g_a0, F_a0)\n",
    "        Jz_a0 = Ju_a0 + Jt_a0 + self.lambda_orthogonality * self.orthogonality_term(F_a0) ** 2\n",
    "        \n",
    "        # Calculate losses for ARG1\n",
    "        Ju_a1 = self.contrastive_loss(v_a1, vhat_a1, a1_negatives)\n",
    "        Jt_a1 = self.focal_triplet_loss(v_a1, vhat_a1, g_a1, F_a1)\n",
    "        Jz_a1 = Ju_a1 + Jt_a1 + self.lambda_orthogonality * self.orthogonality_term(F_a1) ** 2\n",
    "        \n",
    "        if torch.isnan(Jz_p).any():\n",
    "            print(\"Jz_p has nan\")\n",
    "            \n",
    "        if torch.isnan(Jz_a0).any():\n",
    "            print(\"Jz_a0 has nan\")\n",
    "            \n",
    "        if torch.isnan(Jz_a1).any():\n",
    "            print(\"Jz_a1 has nan\")\n",
    "        \n",
    "        # Aggregate the losses\n",
    "        loss = Jz_p + Jz_a0 + Jz_a1\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "# Mock Data Preparation\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "K = 15  # Number of frames/descriptors\n",
    "\n",
    "# Generating mock embeddings for article, predicate, ARG0, ARG1 and their reconstructions\n",
    "article_embedding = torch.randn(batch_size, embedding_dim)\n",
    "v_p = torch.randn(batch_size, embedding_dim)\n",
    "vhat_p = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "v_a0 = torch.randn(batch_size, embedding_dim)\n",
    "vhat_a0 = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "v_a1 = torch.randn(batch_size, embedding_dim)\n",
    "vhat_a1 = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "# Generating mock descriptor weights and descriptor matrices for predicate, ARG0, ARG1\n",
    "d_p = torch.randn(batch_size, K)\n",
    "d_a0 = torch.randn(batch_size, K)\n",
    "d_a1 = torch.randn(batch_size, K)\n",
    "\n",
    "F_p = torch.randn(K, embedding_dim)\n",
    "F_a0 = torch.randn(K, embedding_dim)\n",
    "F_a1 = torch.randn(K, embedding_dim)\n",
    "\n",
    "g_p = torch.randn(batch_size, K)\n",
    "g_a0 = torch.randn(batch_size, K)\n",
    "g_a1 = torch.randn(batch_size, K)\n",
    "\n",
    "# Generating some negative samples (let's assume 5 negative samples per batch entry)\n",
    "num_negatives = 8\n",
    "negatives_p = torch.randn(num_negatives, embedding_dim)\n",
    "negatives_a0 = torch.randn(num_negatives, embedding_dim)\n",
    "negatives_a1 = torch.randn(num_negatives, embedding_dim)\n",
    "\n",
    "# Initialize loss function\n",
    "lambda_orthogonality = 1e-3\n",
    "\n",
    "t = 8  # Number of descriptors with smallest weights for negative samples\n",
    "M = t\n",
    "\n",
    "loss_fn = FRISSLoss(lambda_orthogonality, M, t)\n",
    "\n",
    "# Organizing inputs into dictionaries\n",
    "p = {\"v\": v_p, \"vhat\": vhat_p, \"d\": d_p, \"g\": g_p, \"F\": F_p}\n",
    "a0 = {\"v\": v_a0, \"vhat\": vhat_a0, \"d\": d_a0, \"g\": g_a0, \"F\": F_a0}\n",
    "a1 = {\"v\": v_a1, \"vhat\": vhat_a1, \"d\": d_a1, \"g\": g_a1, \"F\": F_a1}\n",
    "\n",
    "loss_fn = FRISSLoss(lambda_orthogonality, M, t)\n",
    "loss = loss_fn(p, a0, a1, negatives_p, negatives_a0, negatives_a1)\n",
    "print(\"FRiSSLoss output:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FRISSUnsupervised\n",
    "\n",
    "The `FRISSUnsupervised` layer integrates multiple autoencoders and the previously described `FRISSLoss` layer to achieve an unsupervised learning process over the predicates and their arguments.\n",
    "\n",
    "### Forward Method:\n",
    "\n",
    "**Inputs**:\n",
    "1. **v_p**: Embedding of the predicate with size: [batch_size, D_w].\n",
    "2. **v_a0**: Embedding of the ARG0 (first argument) with size: [batch_size, D_w].\n",
    "3. **v_a1**: Embedding of the ARG1 (second argument) with size: [batch_size, D_w].\n",
    "4. **v_article**: Embedding of the article with size: [batch_size, D_w].\n",
    "5. **negatives**: Tensor containing negative samples with size: [batch_size, num_negatives, D_w].\n",
    "6. **tau**: A scalar parameter for the Gumbel softmax in the autoencoder.\n",
    "\n",
    "**Outputs**:\n",
    "- A dictionary `results` containing:\n",
    "    - **loss**: A tensor representing the combined unsupervised loss over the batch with size: [batch_size].\n",
    "    - **p**: Dictionary containing components for the predicate, including reconstructed embedding (`vhat`), descriptor weights (`d`), Gumbel softmax result (`g`), and the descriptor matrix (`F`).\n",
    "    - **a0**: Same as `p` but for ARG0.\n",
    "    - **a1**: Same as `p` but for ARG1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming you have already defined CombinedAutoencoder and its methods as provided earlier.\n",
    "\n",
    "class FRISSUnsupervised(nn.Module):\n",
    "    def __init__(self, D_w, D_h, K, num_frames, lambda_orthogonality, M, t, dropout_prob=0.3):\n",
    "        super(FRISSUnsupervised, self).__init__()\n",
    "        \n",
    "        self.loss_fn = FRISSLoss(lambda_orthogonality, M, t)      \n",
    "        \n",
    "        # Using the CombinedAutoencoder instead of individual Autoencoders\n",
    "        self.combined_autoencoder = CombinedAutoencoder(D_w, D_h, K, dropout_prob=dropout_prob)\n",
    "\n",
    "    def forward(self, v_p, v_a0, v_a1, v_sentence, p_negatives, a0_negatives, a1_negatives, tau):\n",
    "        outputs = self.combined_autoencoder(v_p, v_a0, v_a1, v_sentence, tau)\n",
    "\n",
    "        outputs_p = outputs[\"p\"]\n",
    "        outputs_p[\"v\"] = v_p\n",
    "        \n",
    "        outputs_a0 = outputs[\"a0\"]\n",
    "        outputs_a0[\"v\"] = v_a0\n",
    "        \n",
    "        outputs_a1 = outputs[\"a1\"]\n",
    "        outputs_a1[\"v\"] = v_a1\n",
    "        \n",
    "        loss = self.loss_fn(\n",
    "            outputs_p,\n",
    "            outputs_a0, \n",
    "            outputs_a1, \n",
    "            p_negatives, a0_negatives, a1_negatives\n",
    "        )\n",
    "\n",
    "        results = {\n",
    "            \"loss\": loss,\n",
    "            \"p\": outputs[\"p\"],\n",
    "            \"a0\": outputs[\"a0\"],\n",
    "            \"a1\": outputs[\"a1\"]\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Mock Data Preparation\n",
    "D_h = 768\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "K = 20\n",
    "num_frames = 15\n",
    "tau = 0.9\n",
    "lambda_orthogonality = 0.1  # Placeholder value, please replace with your actual value\n",
    "M = 7  # Placeholder value, please replace with your actual value\n",
    "t = 7  # Placeholder value, please replace with your actual value\n",
    "\n",
    "# Generating mock embeddings for article, predicate, ARG0, ARG1, and their corresponding sentence embeddings\n",
    "article_embedding = torch.randn(batch_size, embedding_dim)\n",
    "v_p = torch.randn(batch_size, embedding_dim)\n",
    "v_a0 = torch.randn(batch_size, embedding_dim)\n",
    "v_a1 = torch.randn(batch_size, embedding_dim)\n",
    "\n",
    "# Generating some negative samples (let's assume 5 negative samples per batch entry)\n",
    "num_negatives = 10\n",
    "negatives_p = torch.randn(num_negatives, embedding_dim)\n",
    "negatives_a0 = torch.randn(num_negatives, embedding_dim)\n",
    "negatives_a1 = torch.randn(num_negatives, embedding_dim)\n",
    "\n",
    "# Testing FRISSUnsupervised\n",
    "unsupervised_module = FRISSUnsupervised(embedding_dim, D_h, K, num_frames, lambda_orthogonality, M, t)\n",
    "results = unsupervised_module(v_p, v_a0, v_a1, article_embedding, negatives_p, negatives_a0, negatives_a1, tau)\n",
    "\n",
    "# Print the results' shapes for verification\n",
    "print(\"Results' Shapes:\")\n",
    "for key, value in results.items():\n",
    "    if key == \"loss\":\n",
    "        print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(f\"{key} -> vhat: {value['vhat'].shape}, d: {value['d'].shape}, g: {value['g'].shape}, F: {value['F'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. FRISSSupervised\n",
    "\n",
    "The layer takes the embeddings from the args and the sentence and predicts frames. \n",
    "\n",
    "The embeddings for the args are averaged for each arg individually and then averaged on args level. The final embedding is feed into a linear layer and passed through a sigmoid function. \n",
    "\n",
    "The sentence embedding is feed into a linear layer and then into a relu function. After again in a linear function and then averaged. The average embeddung is again feed into a linear layer and lastly in a signoid function. \n",
    "\n",
    "It returns a span and sentence based prediction of shape [batch_size, num_frames]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FRISSSupervised(nn.Module):\n",
    "    def __init__(self, D_w, K, num_frames, dropout_prob=0.3):\n",
    "        super(FRISSSupervised, self).__init__()\n",
    "\n",
    "        self.D_w = D_w\n",
    "                \n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.feed_forward_sentence1 = nn.Linear(D_w, D_w)\n",
    "        self.feed_forward_sentence2 = nn.Linear(D_w, num_frames)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Adding two dropout layers\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "    def forward(self, d_p, d_a0, d_a1, vs):\n",
    "        # Span-based Classification   \n",
    "\n",
    "        # Aggregate the SRL descriptors to have one descriptor per sentence\n",
    "        d_p = d_p.mean(dim=2)\n",
    "        d_a0 = d_a0.mean(dim=2)\n",
    "        d_a1 = d_a1.mean(dim=2)\n",
    "\n",
    "        # Take the mean over descriptors\n",
    "        w_u = (d_p + d_a0 + d_a1) / 3\n",
    "\n",
    "        w_u = w_u.sum(dim=1)\n",
    "\n",
    "        # Sentence-based Classification\n",
    "\n",
    "        # Apply the first dropout to vs\n",
    "        vs = self.dropout1(vs)\n",
    "\n",
    "        ws = self.relu(self.feed_forward_sentence1(vs))\n",
    "\n",
    "        # Mean over sentences and apply the second dropout\n",
    "        ws = self.dropout2(ws.mean(dim=1))\n",
    "\n",
    "        # Pass through the second feed forward network\n",
    "        ws = self.feed_forward_sentence2(ws)\n",
    "\n",
    "        # The softmax layer is commented out as it is not used with CrossEntropyLoss\n",
    "        # ys_hat = self.softmax(ws)\n",
    "\n",
    "        # combined pred = sum of span-based and sentence-based predictions\n",
    "        combined = w_u + ws\n",
    "\n",
    "        return w_u, ws, combined\n",
    "\n",
    "\n",
    "# Mock Data Preparation\n",
    "\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "num_frames = 15  # Assuming the number of frames is equal to K for simplicity\n",
    "num_sentences = 32\n",
    "K = 15\n",
    "num_args = 9\n",
    "\n",
    "# Generating mock dsz representations for predicate, ARG0, ARG1\n",
    "d_p = torch.randn(batch_size, num_sentences, num_args, K)\n",
    "d_a0 = torch.randn(batch_size, num_sentences, num_args, K)\n",
    "d_a1 = torch.randn(batch_size, num_sentences, num_args, K) \n",
    "\n",
    "# Adjusting the num_heads parameter\n",
    "srl_heads = 4\n",
    "sentence_heads = 8\n",
    "\n",
    "# Adjust the mock sentence embeddings shape\n",
    "vs = torch.randn(batch_size, num_sentences, embedding_dim)\n",
    "\n",
    "# Initialize and test the supervised module\n",
    "supervised_module = FRISSSupervised(embedding_dim, K, num_frames)\n",
    "\n",
    "# Forward pass the mock data\n",
    "yu_hat, ys_hat, combined_pred = supervised_module(d_p, d_a0, d_a1, vs)\n",
    "yu_hat.shape, ys_hat.shape, combined_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FRISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FRISS(nn.Module):\n",
    "    def __init__(self, embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob=0.3, bert_model_name=\"bert-base-uncased\"):\n",
    "        super(FRISS, self).__init__()\n",
    "        \n",
    "        # Aggregation layer replaced with SRL_Embeddings\n",
    "        self.aggregation = SRL_Embeddings(bert_model_name)\n",
    "        \n",
    "        # Unsupervised training module\n",
    "        self.unsupervised = FRISSUnsupervised(embedding_dim, D_h, K, num_frames, lambda_orthogonality, M, t, dropout_prob=dropout_prob)\n",
    "        \n",
    "        # Supervised training module\n",
    "        self.supervised = FRISSSupervised(embedding_dim, K, num_frames, dropout_prob=dropout_prob)\n",
    "        \n",
    "    def negative_sampling(self, embeddings, num_negatives=8):\n",
    "        batch_size, num_sentences, num_args, embedding_dim = embeddings.size()\n",
    "        all_negatives = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(num_sentences):\n",
    "                # Flatten the arguments dimension to sample across all arguments in the sentence\n",
    "                flattened_embeddings = embeddings[i, j].view(-1, embedding_dim)\n",
    "                \n",
    "                # Get indices of non-padded embeddings (assuming padding is represented by all-zero vectors)\n",
    "                non_padded_indices = torch.where(torch.any(flattened_embeddings != 0, dim=1))[0]\n",
    "\n",
    "                # Randomly sample negative indices from non-padded embeddings\n",
    "                if len(non_padded_indices) > 0:\n",
    "                    negative_indices = non_padded_indices[torch.randint(0, len(non_padded_indices), (num_negatives,))]\n",
    "                else:\n",
    "                    # If no non-padded embeddings, use zeros\n",
    "                    negative_indices = torch.zeros(num_negatives, dtype=torch.long)\n",
    "\n",
    "                negative_samples = flattened_embeddings[negative_indices, :]\n",
    "                all_negatives.append(negative_samples)\n",
    "\n",
    "        # Concatenate all negative samples into a single tensor\n",
    "        all_negatives = torch.cat(all_negatives, dim=0)\n",
    "\n",
    "        # If more samples than required, randomly select 'num_negatives' samples\n",
    "        if all_negatives.size(0) > num_negatives:\n",
    "            indices = torch.randperm(all_negatives.size(0))[:num_negatives]\n",
    "            all_negatives = all_negatives[indices]\n",
    "\n",
    "        return all_negatives\n",
    "    \n",
    "    def forward(self, sentence_ids, sentence_attention_masks, predicate_ids, predicate_attention_masks, arg0_ids, arg0_attention_masks, arg1_ids, arg1_attention_masks, tau):\n",
    "        # Convert input IDs to embeddings\n",
    "        sentence_embeddings, predicate_embeddings, arg0_embeddings, arg1_embeddings = self.aggregation(sentence_ids, sentence_attention_masks, predicate_ids, predicate_attention_masks, arg0_ids, arg0_attention_masks, arg1_ids, arg1_attention_masks)\n",
    "        \n",
    "        # Handle multiple spans by averaging predictions\n",
    "        unsupervised_losses = torch.zeros((sentence_embeddings.size(0),), device=sentence_embeddings.device)\n",
    "        \n",
    "        # Creating storage for aggregated d tensors\n",
    "        d_p_list, d_a0_list, d_a1_list = [], [], []\n",
    "        \n",
    "        negatives_p = self.negative_sampling(predicate_embeddings)\n",
    "        negatives_a0 = self.negative_sampling(arg0_embeddings)\n",
    "        negatives_a1 = self.negative_sampling(arg1_embeddings)\n",
    "\n",
    "        # Process each sentence \n",
    "        for sentence_idx in range(sentence_embeddings.size(1)):\n",
    "            s_sentence_span = sentence_embeddings[:, sentence_idx, :]\n",
    "\n",
    "            d_p_sentence_list = []\n",
    "            d_a0_sentence_list = []\n",
    "            d_a1_sentence_list = []\n",
    "\n",
    "            # Process each span\n",
    "            for span_idx in range(predicate_embeddings.size(2)):                \n",
    "                v_p_span = predicate_embeddings[:, sentence_idx, span_idx, :]\n",
    "                v_a0_span = arg0_embeddings[:, sentence_idx, span_idx, :]\n",
    "                v_a1_span = arg1_embeddings[:, sentence_idx, span_idx, :]\n",
    "\n",
    "                # Feed the embeddings to the unsupervised module\n",
    "                unsupervised_results = self.unsupervised(v_p_span, v_a0_span, v_a1_span, s_sentence_span, negatives_p, negatives_a0, negatives_a1, tau)                \n",
    "                unsupervised_losses += unsupervised_results[\"loss\"]\n",
    "                \n",
    "                if torch.isnan(unsupervised_results[\"loss\"]).any():\n",
    "                    print(\"loss is nan\")\n",
    "                \n",
    "                # Use the vhat (reconstructed embeddings) for supervised predictions\n",
    "                d_p_sentence_list.append(unsupervised_results['p']['d'])\n",
    "                d_a0_sentence_list.append(unsupervised_results['a0']['d'])\n",
    "                d_a1_sentence_list.append(unsupervised_results['a1']['d'])        \n",
    "\n",
    "\n",
    "            # Aggregating across all spans\n",
    "            d_p_sentence = torch.stack(d_p_sentence_list, dim=1)\n",
    "            d_a0_sentence = torch.stack(d_a0_sentence_list, dim=1)\n",
    "            d_a1_sentence = torch.stack(d_a1_sentence_list, dim=1)\n",
    "\n",
    "            d_p_list.append(d_p_sentence)\n",
    "            d_a0_list.append(d_a0_sentence)\n",
    "            d_a1_list.append(d_a1_sentence)\n",
    "\n",
    "        # Aggregating across all spans\n",
    "        d_p_aggregated = torch.stack(d_p_list, dim=1)\n",
    "        d_a0_aggregated = torch.stack(d_a0_list, dim=1)\n",
    "        d_a1_aggregated = torch.stack(d_a1_list, dim=1)\n",
    "        \n",
    "        # Supervised predictions\n",
    "        span_pred, sentence_pred, combined_pred = self.supervised(d_p_aggregated, d_a0_aggregated, d_a1_aggregated, sentence_embeddings)\n",
    "    \n",
    "        # Identify valid (non-nan) losses\n",
    "        valid_losses = ~torch.isnan(unsupervised_losses)\n",
    "\n",
    "        # Take average by summing the valid losses and dividing by num sentences so that padded sentences are also taken in equation\n",
    "        unsupervised_loss = unsupervised_losses[valid_losses].sum() / (sentence_embeddings.shape[1] * sentence_embeddings.shape[2])\n",
    "        \n",
    "        return unsupervised_loss, span_pred, sentence_pred, combined_pred\n",
    "\n",
    "\n",
    "# Set the necessary parameters\n",
    "batch_size = 2\n",
    "embedding_dim = 768\n",
    "K = 14  # Number of frames/descriptors\n",
    "num_frames = 14  # Assuming the number of frames is equal to K for simplicity\n",
    "D_h = 512  # Dimension of the hidden representation\n",
    "lambda_orthogonality = 0.1\n",
    "M = 8\n",
    "t = 8\n",
    "tau = 1.0\n",
    "\n",
    "# Define some mock token IDs data parameters\n",
    "max_sentences_per_article = 8\n",
    "max_sentence_length = 10\n",
    "num_sentences = max_sentences_per_article\n",
    "max_args_per_sentence = 3\n",
    "\n",
    "# Generating mock token IDs for predicate, ARG0, ARG1, and their corresponding sentences\n",
    "# We assume a vocab size of 30522 (standard BERT vocab size) for simplicity.\n",
    "vocab_size = 30522\n",
    "\n",
    "sentence_ids = torch.randint(0, vocab_size, (batch_size, max_sentences_per_article, max_sentence_length))\n",
    "predicate_ids = torch.randint(0, vocab_size, (batch_size, max_sentences_per_article, max_args_per_sentence, max_sentence_length))\n",
    "arg0_ids = torch.randint(0, vocab_size, (batch_size, max_sentences_per_article, max_args_per_sentence, max_sentence_length))\n",
    "arg1_ids = torch.randint(0, vocab_size, (batch_size, max_sentences_per_article, max_args_per_sentence, max_sentence_length))\n",
    "\n",
    "sentence_embeddings = torch.randn(batch_size, max_sentences_per_article, embedding_dim)\n",
    "predicate_embeddings = torch.randn(batch_size, max_sentences_per_article, max_args_per_sentence, embedding_dim)\n",
    "arg0_embeddings = torch.randn(batch_size, max_sentences_per_article, max_args_per_sentence, embedding_dim)\n",
    "arg1_embeddings = torch.randn(batch_size, max_sentences_per_article, max_args_per_sentence, embedding_dim)\n",
    "\n",
    "# Mock attention masks\n",
    "sentence_attention_masks = torch.randint(0, 2, (batch_size, max_sentences_per_article, max_sentence_length))\n",
    "predicate_attention_masks = torch.randint(0, 2, (batch_size, max_sentences_per_article, max_args_per_sentence, max_sentence_length))\n",
    "arg0_attention_masks = torch.randint(0, 2, (batch_size, max_sentences_per_article, max_args_per_sentence, max_sentence_length))\n",
    "arg1_attention_masks = torch.randint(0, 2, (batch_size, max_sentences_per_article, max_args_per_sentence, max_sentence_length))\n",
    "\n",
    "# Initialize the FRISS model\n",
    "friss_model = FRISS(embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K=K, num_frames=num_frames)\n",
    "\n",
    "# Forward pass the mock data\n",
    "unsupervised_loss, span_pred, sentence_pred, combined_pred = friss_model(sentence_ids, sentence_attention_masks, predicate_ids, predicate_attention_masks, arg0_ids, arg0_attention_masks, arg1_ids, arg1_attention_masks, 1)\n",
    "unsupervised_loss, span_pred.shape, sentence_pred.shape, combined_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1-Score (micro-averaged) and Average Precision Score are chosen as primary metrics for evaluating the multi-label classification task due to the following reasons:\n",
    "\n",
    "1. **F1-Score (Micro)**:\n",
    "    - The micro-averaged F1-score computes global counts of true positives, false negatives, and false positives. \n",
    "    - It provides a balance between precision (the number of correct positive results divided by the number of all positive results) and recall (the number of correct positive results divided by the number of positive results that should have been returned).\n",
    "    - Given the imbalance in the label distribution observed in the dataset, the micro-averaged F1-score is robust against this imbalance, making it a suitable metric for optimization.\n",
    "\n",
    "2. **Average Precision Score**:\n",
    "    - This metric summarizes the precision-recall curve, giving a single value that represents the average of precision values at different recall levels.\n",
    "    - It's especially valuable when class imbalances exist, as it gives more weight to the positive class (the rarer class in an imbalanced dataset).\n",
    "\n",
    "Using these metrics will ensure that the model is optimized for a balanced performance across all labels, even if some labels are rarer than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import json\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, optimizer, loss_function, alpha=0.5, num_epochs=10, tau_min=1, tau_decay=0.95, device='cuda', save_path='../notebooks/'):\n",
    "    # Create a unique directory for this training session\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    save_dir = os.path.join(save_path, f'training_session_{timestamp}')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Create save directory: {save_dir}\")\n",
    "\n",
    "    # Save model settings\n",
    "    settings_path = os.path.join(save_dir, 'model_settings.json')\n",
    "    with open(settings_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'alpha': alpha,\n",
    "            'num_epochs': num_epochs,\n",
    "            'tau_min': tau_min,\n",
    "            'tau_decay': tau_decay,\n",
    "        }, f, indent=4)\n",
    "    \n",
    "    tau = 1\n",
    "    scheduler = StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "    global_steps = 0\n",
    "\n",
    "    metrics = {\n",
    "        'epoch': [],\n",
    "        'accuracy_span': [],\n",
    "        'accuracy_sentence': [],\n",
    "        'accuracy_combined': [],\n",
    "        'f1_span_micro': [],\n",
    "        'f1_sentence_micro': [],\n",
    "        'f1_combined_micro': [],\n",
    "        'f1_span_macro': [],\n",
    "        'f1_sentence_macro': [],\n",
    "        'f1_combined_macro': [],\n",
    "        'tau': [],\n",
    "        'lr': []\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        model.train()\n",
    "\n",
    "        # init loss\n",
    "        total_loss = 0\n",
    "        supervised_total_loss = 0\n",
    "        unsupervised_total_loss = 0\n",
    "\n",
    "        local_steps = 0\n",
    "        \n",
    "        batch_progress = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=\"Batches\", leave=False)\n",
    "        for batch_idx, batch in batch_progress:   \n",
    "            global_steps += 1\n",
    "            if global_steps % 50 == 0:\n",
    "                tau = max(tau_min, math.exp(-tau_decay * global_steps))\n",
    "\n",
    "            local_steps += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            sentence_ids = batch['sentence_ids'].to(device)\n",
    "            sentence_attention_masks = batch['sentence_attention_masks'].to(device)\n",
    "\n",
    "            predicate_ids = batch['predicate_ids'].to(device)\n",
    "            predicate_attention_masks = batch['predicate_attention_masks'].to(device)\n",
    "            \n",
    "            arg0_ids = batch['arg0_ids'].to(device)\n",
    "            arg0_attention_masks = batch['arg0_attention_masks'].to(device)\n",
    "\n",
    "            arg1_ids = batch['arg1_ids'].to(device)\n",
    "            arg1_attention_masks = batch['arg1_attention_masks'].to(device)\t\n",
    "\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            unsupervised_loss, span_logits, sentence_logits, _ = model(sentence_ids, sentence_attention_masks, predicate_ids, predicate_attention_masks, arg0_ids, arg0_attention_masks, arg1_ids, arg1_attention_masks, tau)\n",
    "                    \n",
    "            span_loss = 0.0\n",
    "            sentence_loss = 0.0\n",
    "\n",
    "            span_loss = loss_function(span_logits, labels.float())       \n",
    "            sentence_loss = loss_function(sentence_logits, labels.float())\n",
    "            \n",
    "            supervised_loss = span_loss + sentence_loss\n",
    "            \n",
    "            combined_loss = alpha * supervised_loss + (1-alpha) * unsupervised_loss\n",
    "            \n",
    "            if torch.isnan(combined_loss):\n",
    "                print(f\"NaN loss detected at epoch {epoch+1}, batch {batch_idx+1}. Stopping...\")\n",
    "                return\n",
    "        \n",
    "            combined_loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # After the backward pass\n",
    "            if any(p.grad is not None and torch.isnan(p.grad).any() for p in model.parameters()):\n",
    "                print(f\"NaN gradients detected at epoch {epoch+1}, batch {batch_idx+1}. Stopping...\")\n",
    "                return\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += combined_loss.item()\n",
    "            supervised_total_loss += supervised_loss.item()\n",
    "            unsupervised_total_loss += unsupervised_loss.item()\n",
    "\n",
    "            batch_progress.set_description(f\"Epoch {epoch+1} ({local_steps}) Total Loss: {combined_loss.item():.3f}, Span: {span_loss:.3f}, Sentence: {sentence_loss:.3f}, Supervised: {supervised_loss.item():.3f}, Unsupervised: {unsupervised_loss.item():.3f}\")\n",
    "                        \n",
    "            # Explicitly delete tensors to free up memory\n",
    "            del sentence_ids, predicate_ids, arg0_ids, arg1_ids, labels, unsupervised_loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Combined Loss: {total_loss/len(train_dataloader)}, Supervised Loss: {supervised_total_loss/len(train_dataloader)}, Unsupervised Loss: {unsupervised_total_loss/len(train_dataloader)}\")\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        span_preds = []\n",
    "        sentence_preds = []\n",
    "        combined_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                sentence_ids = batch['sentence_ids'].to(device)\n",
    "                sentence_attention_masks = batch['sentence_attention_masks'].to(device)\n",
    "\n",
    "                predicate_ids = batch['predicate_ids'].to(device)\n",
    "                predicate_attention_masks = batch['predicate_attention_masks'].to(device)\n",
    "                \n",
    "                arg0_ids = batch['arg0_ids'].to(device)\n",
    "                arg0_attention_masks = batch['arg0_attention_masks'].to(device)\n",
    "\n",
    "                arg1_ids = batch['arg1_ids'].to(device)\n",
    "                arg1_attention_masks = batch['arg1_attention_masks'].to(device)\t\n",
    "\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                _, span_logits, sentence_logits, combined_logits = model(sentence_ids, sentence_attention_masks, predicate_ids, predicate_attention_masks, arg0_ids, arg0_attention_masks, arg1_ids, arg1_attention_masks, tau)\n",
    "\n",
    "                span_pred = (torch.softmax(span_logits, dim=1) > 0.5).int()\n",
    "                sentence_pred = (torch.softmax(sentence_logits, dim=1) > 0.5).int()\n",
    "                combined_pred = (torch.softmax(combined_logits, dim=1) > 0.5).int()\n",
    "                \n",
    "                span_preds.append(span_pred.cpu().numpy())\n",
    "                sentence_preds.append(sentence_pred.cpu().numpy())\n",
    "                combined_preds.append(combined_pred.cpu().numpy())\n",
    "\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "                # Explicitly delete tensors to free up memory\n",
    "                del sentence_ids, predicate_ids, arg0_ids, arg1_ids, labels, span_logits, sentence_logits, sentence_pred\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        all_span_preds = np.vstack(span_preds)\n",
    "        all_sentence_preds = np.vstack(sentence_preds)\n",
    "        all_combined_preds = np.vstack(combined_preds)\n",
    "        all_labels = np.vstack(all_labels)\n",
    "\n",
    "        # Compute metrics\n",
    "        f1_span_micro = f1_score(all_labels, all_span_preds, average='micro', zero_division=0)\n",
    "        f1_sentence_micro = f1_score(all_labels, all_sentence_preds, average='micro', zero_division=0)\n",
    "        f1_combined_micro = f1_score(all_labels, all_combined_preds, average='micro', zero_division=0)\n",
    "\n",
    "        f1_span_macro = f1_score(all_labels, all_span_preds, average='macro', zero_division=0)\n",
    "        f1_sentence_macro = f1_score(all_labels, all_sentence_preds, average='macro', zero_division=0)\n",
    "        f1_combined_macro = f1_score(all_labels, all_combined_preds, average='macro', zero_division=0)\n",
    "\n",
    "        accuracy_span = accuracy_score(all_labels, all_span_preds)\n",
    "        accuracy_sentence = accuracy_score(all_labels, all_sentence_preds)\n",
    "        accuracy_combined = accuracy_score(all_labels, all_combined_preds)\n",
    "\n",
    "        print(f\"Validation Metrics - micro F1 - Span: {f1_span_micro:.2f}, Sentence: {f1_sentence_micro:.2f}, Combined: {f1_combined_micro:.2f}, macro F1 - Span: {f1_span_macro:.2f}, Sentence: {f1_sentence_macro:.2f}, Combined: {f1_combined_macro:.2f}\")\n",
    "\n",
    "        # Update metrics dictionary\n",
    "        metrics['epoch'].append(epoch + 1)\n",
    "        metrics['f1_span_micro'].append(f1_span_micro)\n",
    "        metrics['f1_sentence_micro'].append(f1_sentence_micro)\n",
    "        metrics['f1_combined_micro'].append(f1_combined_micro)\n",
    "        metrics['f1_span_macro'].append(f1_span_macro)\n",
    "        metrics['f1_sentence_macro'].append(f1_sentence_macro)\n",
    "        metrics['f1_combined_macro'].append(f1_combined_macro)\n",
    "        metrics['accuracy_span'].append(accuracy_span)\n",
    "        metrics['accuracy_sentence'].append(accuracy_sentence)\n",
    "        metrics['accuracy_combined'].append(accuracy_combined)\n",
    "        metrics['tau'].append(tau)\n",
    "        metrics['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        # Save metrics after each validation run\n",
    "        metrics_save_path = os.path.join(save_dir, 'metrics.json')\n",
    "        with open(metrics_save_path, 'w') as f:\n",
    "            json.dump(metrics, f, indent=4)\n",
    "        \n",
    "        # Save the model every 2 epochs\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            model_checkpoint_path = os.path.join(save_dir, f'model_checkpoint_epoch_{epoch + 1}.pth')\n",
    "            torch.save(model.state_dict(), model_checkpoint_path)\n",
    "            print(f\"Model checkpoint saved to {model_checkpoint_path}\")\n",
    "\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "max_sentences_per_article = 32\n",
    "max_sentence_length = 64\n",
    "\n",
    "max_args_per_sentence = 10\n",
    "max_arg_length = 16\n",
    "\n",
    "test_size = 0.1\n",
    "\n",
    "train_dataset, test_dataset, train_dataloader, test_dataloader = get_datasets_dataloaders(df, tokenizer, recalculate_srl=False, batch_size=batch_size, max_sentences_per_article=max_sentences_per_article, max_sentence_length=max_sentence_length, max_arg_length=max_arg_length, pickle_path=\"../notebooks/FRISS_srl.pkl\", test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_friss_model(embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob, bert_model_name=\"bert-base-uncased\", load=True, path=\"\", device='cuda'):\n",
    "    # Model instantiation\n",
    "    model = FRISS(embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob=dropout_prob, bert_model_name=bert_model_name)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if load:\n",
    "        assert path != \"\"\n",
    "        model.load_state_dict(torch.load(path, map_location=device))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(profile=\"full\")\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 768\n",
    "num_frames = 15\n",
    "\n",
    "D_h = 768\n",
    "lambda_orthogonality = 1e-3\n",
    "\n",
    "K = num_frames\n",
    "t = 8\n",
    "M = 8\n",
    "tau_min = 0.5\n",
    "tau_decay = 5e-4\n",
    "\n",
    "dropout_prob = 0.3\n",
    "\n",
    "friss_model_path = \"bert-base-uncased\"\n",
    "bert_model_path = \"bert-base-uncased\"\n",
    "\n",
    "# Model instantiation\n",
    "model = get_friss_model(embedding_dim, \n",
    "                        D_h, \n",
    "                        lambda_orthogonality, \n",
    "                        M, \n",
    "                        t, \n",
    "                        num_sentences, \n",
    "                        K, \n",
    "                        num_frames, \n",
    "                        dropout_prob=dropout_prob,\n",
    "                        bert_model_name=bert_model_path,\n",
    "                        load=False,\n",
    "                        path=friss_model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# LOSS\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Train the model\n",
    "alpha_value = 0.5\n",
    "num_epochs_value = 15\n",
    "\n",
    "save_path = \"models/\"\n",
    "\n",
    "metrics = train(model, \n",
    "                train_dataloader, \n",
    "                test_dataloader, \n",
    "                optimizer, \n",
    "                loss_function, \n",
    "                tau_min=tau_min, \n",
    "                tau_decay=tau_decay, \n",
    "                alpha=alpha_value, \n",
    "                num_epochs=num_epochs_value,\n",
    "                device=device, \n",
    "                save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 768\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def grid_search(train_dataloader, test_dataloader, search_space, num_epochs=10):\n",
    "    # Store the results for each hyperparameter combination\n",
    "    results = {}\n",
    "\n",
    "    # Fixed values for K and num_frames\n",
    "    K = 14\n",
    "    num_frames = 14\n",
    "\n",
    "    # Fixed values for dropout_prob and bert_model_name (adjust if necessary)\n",
    "    bert_model_name = \"../notebooks/models/fine-tuned-model/\"\n",
    "\n",
    "    # Initialize the file to write metrics\n",
    "    with open(\"../notebooks/grid_search_metrics.csv\", \"w\", newline='') as csvfile:\n",
    "        fieldnames = ['combination', 'alpha', 'lr', 'D_h', 'lambda_orthogonality', 'M', 't', 'tau_min', 'tau_decay', 'dropout_prob', 'epoch', 'f1_span_micro', 'f1_span_macro', 'f1_sentence_micro', 'f1_sentence_macro', 'f1_combined_micro', 'f1_combined_macro']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Calculate the total number of combinations\n",
    "        total_combinations = 1\n",
    "        for key, values in search_space.items():\n",
    "            total_combinations *= len(values)\n",
    "\n",
    "        # Loop through all combinations\n",
    "        for idx, combination in enumerate(product(*search_space.values())):\n",
    "            print(f\"Training combination {idx + 1}/{total_combinations}: {combination}\")\n",
    "\n",
    "            # Extract hyperparameters from the current combination\n",
    "            alpha, lr, tau_min, tau_decay, t, D_h, lambda_orthogonality, M, dropout_prob = combination\n",
    "\n",
    "            # Initialize the model with current hyperparameters\n",
    "            model = FRISS(embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob, bert_model_name)\n",
    "            model.to(device)\n",
    "        \n",
    "                \n",
    "            # Compute the `weight` parameter for each label\n",
    "            label_frequencies = y.mean()\n",
    "            weights = 1 / (label_frequencies + 1e-10)  # Adding a small value to avoid division by zero\n",
    "\n",
    "            # Compute the `pos_weight` parameter\n",
    "            pos_weights = (1 - label_frequencies) / (label_frequencies + 1e-10)\n",
    "\n",
    "            # Convert the computed weights and pos_weights to PyTorch tensors\n",
    "            weights_tensor = torch.tensor(weights.values, dtype=torch.float32).to(device)\n",
    "            pos_weights_tensor = torch.tensor(pos_weights.values, dtype=torch.float32).to(device)\n",
    "\n",
    "            loss_function = nn.BCEWithLogitsLoss(weight=weights_tensor, pos_weight=pos_weights_tensor, reduction=\"mean\")\n",
    "        \n",
    "            # Define the optimizer\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "            # Define loss_function if needed (add this if your train function requires it)\n",
    "\n",
    "            # Train the model with the current hyperparameters\n",
    "            epoch_metrics = train(model, train_dataloader, test_dataloader, optimizer, loss_function, alpha=alpha, num_epochs=num_epochs, tau_min=tau_min, tau_decay=tau_decay, device=device, save=False)\n",
    "\n",
    "            # Write the metrics to the CSV file\n",
    "            for epoch in range(num_epochs):\n",
    "                f1_span_micro = epoch_metrics['f1_span_micro'][epoch]\n",
    "                f1_span_macro = epoch_metrics['f1_span_macro'][epoch]\n",
    "                f1_sentence_micro = epoch_metrics['f1_sentence_micro'][epoch]\n",
    "                f1_sentence_macro = epoch_metrics['f1_sentence_macro'][epoch]\n",
    "                f1_combined_micro = epoch_metrics['f1_combined_micro'][epoch]\n",
    "                f1_combined_macro = epoch_metrics['f1_combined_macro'][epoch]\n",
    "                row = {\n",
    "                    'combination': idx,\n",
    "                    'alpha': alpha,\n",
    "                    'lr': lr,\n",
    "                    'D_h': D_h,\n",
    "                    'lambda_orthogonality': lambda_orthogonality,\n",
    "                    'M': M,\n",
    "                    't': t,\n",
    "                    'tau_min': tau_min,\n",
    "                    'tau_decay': tau_decay,\n",
    "                    'dropout_prob': dropout_prob,\n",
    "                    'epoch': epoch + 1,\n",
    "                    'f1_span_micro': f1_span_micro,\n",
    "                    'f1_span_macro': f1_span_macro,\n",
    "                    'f1_sentence_micro': f1_sentence_micro,\n",
    "                    'f1_sentence_macro': f1_sentence_macro,\n",
    "                    'f1_combined_micro': f1_combined_micro,\n",
    "                    'f1_combined_macro': f1_combined_macro\n",
    "                }\n",
    "                writer.writerow(row)\n",
    "                csvfile.flush()\n",
    "\n",
    "    return results\n",
    "\n",
    "search_space = {\n",
    "    'alpha': [0.5, 0.2, 0.8],\n",
    "    'lr': [1e-5, 2e-5, 5e-4, 1e-3],\n",
    "    'tau_min': [0.5],\n",
    "    'tau_decay': [5e-4],\n",
    "    't': [5, 8, 10, 20],\n",
    "    'D_h': [768, 768 * 2, 768 // 2, 768 * 3],\n",
    "    'lambda_orthogonality': [1e-6, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2],\n",
    "    'M': [5, 8, 10, 20],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "# Call the grid search function\n",
    "results = grid_search(train_dataloader, test_dataloader, search_space, 10)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_path(path, embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads the weights into an instance of the model class from the given path.\n",
    "    \n",
    "    Args:\n",
    "    - model_class (torch.nn.Module): The class of the model (uninitialized).\n",
    "    - path (str): Path to the saved weights.\n",
    "    - device (str): Device to load the model on ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - model (torch.nn.Module): Model with weights loaded.\n",
    "    \"\"\"\n",
    "\n",
    "    # Model instantiation\n",
    "    model = FRISS(embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob=dropout_prob)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    \n",
    "    #model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 768\n",
    "num_frames = 15\n",
    "\n",
    "D_h = 768\n",
    "lambda_orthogonality = 1e-3\n",
    "\n",
    "K = num_frames\n",
    "t = 8\n",
    "M = 8\n",
    "tau_min = 0.5\n",
    "tau_decay = 5e-4\n",
    "\n",
    "dropout_prob = 0.3\n",
    "\n",
    "friss_model_path = \"bert-base-uncased\"\n",
    "bert_model_path = \"bert-base-uncased\"\n",
    "\n",
    "\n",
    "model = load_model_from_path('models/friss_best_v2/model_checkpoint_epoch_2.pth', embedding_dim, D_h, lambda_orthogonality, M, t, num_sentences, K, num_frames, dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, y_columns, device='cuda'):\n",
    "    \"\"\"\n",
    "    Make predictions with the given model and dataloader.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): The model to make predictions with.\n",
    "    - dataloader (DataLoader): DataLoader for the dataset to predict on.\n",
    "    - y_columns (pandas.Index): Column names from the y dataframe which correspond to labels.\n",
    "    - device (str): Device to make predictions on ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - predicted_labels (list of lists): List containing the predicted labels for each instance.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds_span = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move data to device\n",
    "            sentence_ids = batch['sentence_ids'].to(device)\n",
    "            predicate_ids = batch['predicate_ids'].to(device)\n",
    "            arg0_ids = batch['arg0_ids'].to(device)\n",
    "            arg1_ids = batch['arg1_ids'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            _, span_logits, sentence_logits, combined_logits = model(sentence_ids, predicate_ids, arg0_ids, arg1_ids, 0.8)\n",
    "            combined_pred = (torch.softmax(combined_logits, dim=1) > 0.5).float()\n",
    "\n",
    "            all_preds_span.append(combined_pred.cpu().numpy())\n",
    "                \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    predictions = np.vstack(all_preds_span)\n",
    "    \n",
    "    # Convert boolean predictions to labels\n",
    "    predicted_labels = []\n",
    "    for pred in predictions:\n",
    "        labels = list(y_columns[pred.astype(bool)])\n",
    "        predicted_labels.append(labels)\n",
    "    \n",
    "    return predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# article813452859\n",
    "article = \"\"\"Sadiq Khan Slammed for Pro-EU 'Message of Support' During Firework Display\n",
    "\n",
    "The spectacular fireworks that lit up the London sky on Monday night caused a stir on social media over the display's pro-EU message, at a time when the nation is divided over its looming withdrawal from the bloc.\n",
    "London Mayor Sadiq Khan faced mounting criticism after the capital's New Year's Eve fireworks display, which celebrated ties with the European Union, left a bad taste in the mouths of some Brits.\n",
    "The 135-metre-high London Eye was lit up in blue while its tubs turned yellow, with the giant Ferris wheel resembling the star-studded flag of the European Union.Sadiq Khan called his fireworks display a \"message of support\" to EU citizens living in London.\n",
    "\"Our one million EU citizens are Londoners, they make a huge contribution, and no matter the outcome of Brexit  they will always be welcome\", he said.\n",
    "To the one million EU citizens who have made our city your home: you are Londoners, you make a huge contribution and you are welcome here.\n",
    "I'm proud that tonight we will welcome in the new year with a message of support to you.\n",
    "#LondonNYE #LondonIsOpen https://t.co/XctrgfXXaM  Sadiq Khan (@SadiqKhan) 31  2018 .\n",
    "However, a host of Londoners rushed to Twitter to accuse their mayor of \"politicising\" the celebrations  with some are even calling for his resignation.\n",
    "I cannot believe this event has been politicised.\n",
    "This man has no shame.\n",
    "Just resign.\n",
    " wayne campbell (@campbs177) 31  2018 .\n",
    "Thanks a lot Sadiq Khan you ruined the fireworks display by talking about Europe, need I remind you about Brexit.\n",
    "You have started of the new year by talking about relationships with the European Union.\n",
    "Well done.\n",
    "We need Boris Johnson back.\n",
    " Mitchell T Cannon (@MitchellTCanno1) 1  2019 .\n",
    "Another shameless attempt at using party politics on what is supposed to be a happy occasion  droneguy (@shelbyguitars) 1  2019 .\n",
    "Politicising another innocent event that should be no different to anyone no matter who they are or where they are from!\n",
    "Shameful!\n",
    "!\n",
    " Mike Dyer (@Miked2372Mike) 31  2018 .\n",
    "Someone was stabbed down the road from me last night.\n",
    "How about sorting that stuff out instead of politicizing something that should be fun for everyone?How many times does it have to be said.\n",
    "Commenting on Brexit isn't your job.\n",
    " Peter Rockett (@rockettp) 31  2018 .\n",
    "The UK voted to leave the EU in June 2016 via a nationwide referendum, with 51.9 per cent voting in favour of pulling out of the bloc, while 48.1 per cent wanted to remain.\n",
    "The withdrawal is scheduled for the end of March; the Article 50 deadline.\n",
    "The Remain sentiment dominated London, with nearly 60 percent of voters wanting Britain to stay in the European Union.\n",
    "Sadiq Khan, an outspoken Remainer himself, earlier called for a second referendum on Brexit.\n",
    "\"The government's abject failure  and the huge risk we face of a bad deal or a 'no deal' Brexit  means that giving people a fresh say is now the right  and only  approach left for our country,\" he said in September.\n",
    "\"\"\"\n",
    "\n",
    "test_article = get_article_dataloader(article, tokenizer)\n",
    "predict(model, test_article, y.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run test for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = get_test_dataloader(df_test[\"content\"], tokenizer, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(model, test_dataloader, y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.concat([df_test, df_preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds[\"pred_frames\"] = df_preds.apply(lambda l: list([l[0], l[1], l[2], l[3], l[4], l[5]]), axis=1)\n",
    "\n",
    "df_preds[\"pred_frames\"] = df_preds[\"pred_frames\"].apply(lambda l: \",\".join([ f for f in l if f is not None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.to_csv(\"../notebooks/test.csv\", sep=\"\\t\", index=False, columns=[\"article_id\", \"pred_frames\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(model, dataloader, y_columns, device='cuda'):\n",
    "    \"\"\"\n",
    "    Make predictions with the given model and dataloader.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): The model to make predictions with.\n",
    "    - dataloader (DataLoader): DataLoader for the dataset to predict on.\n",
    "    - y_columns (pandas.Index): Column names from the y dataframe which correspond to labels.\n",
    "    - device (str): Device to make predictions on ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - predicted_labels (list of lists): List containing the predicted labels for each instance.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds_span = []\n",
    "    \n",
    "    # Initialize usage lists for each label\n",
    "    num_labels = len(y_columns)\n",
    "    all_used_labels_p = []\n",
    "    all_used_labels_a0 = []\n",
    "    all_used_labels_a1 = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            used_labels_p = []\n",
    "            used_labels_a0 = []\n",
    "            used_labels_a1 = []\n",
    "    \n",
    "            # Move data to device\n",
    "            sentence_ids = batch['sentence_ids'].to(device)\n",
    "            predicate_ids = batch['predicate_ids'].to(device)\n",
    "            arg0_ids = batch['arg0_ids'].to(device)\n",
    "            arg1_ids = batch['arg1_ids'].to(device)\n",
    "            \n",
    "            sentence_embeddings, predicate_embeddings, arg0_embeddings, arg1_embeddings = model.aggregation(sentence_ids, predicate_ids, arg0_ids, arg1_ids)\n",
    "            \n",
    "            # Process each span\n",
    "            for span_idx in range(sentence_embeddings.size(1)):\n",
    "                s_sentence_span = sentence_embeddings[:, span_idx, :]\n",
    "                v_p_span = predicate_embeddings[:, span_idx, :]\n",
    "                v_a0_span = arg0_embeddings[:, span_idx, :]\n",
    "                v_a1_span = arg1_embeddings[:, span_idx, :]\n",
    "            \n",
    "                #unsupervised.combined_autoencoder v_p, v_a0, v_a1, v_sentence, tau\n",
    "                output = model.unsupervised.combined_autoencoder(v_p_span, v_a0_span, v_a1_span, s_sentence_span, 0.6)\n",
    "                \n",
    "                #print(output[\"p\"][\"g\"].cpu().numpy())\n",
    "                used_labels_p.append(output[\"p\"][\"g\"].cpu().numpy())\n",
    "                used_labels_a0.append(output[\"a0\"][\"g\"].cpu().numpy())\n",
    "                used_labels_a1.append(output[\"a1\"][\"g\"].cpu().numpy())\n",
    "\n",
    "            \n",
    "            # Forward pass\n",
    "            _, span_logits, sentence_logits, combined_logits = model(sentence_ids, predicate_ids, arg0_ids, arg1_ids, 0.6)\n",
    "            combined_pred = (torch.sigmoid(combined_logits) > 0.5).float()\n",
    "\n",
    "            all_preds_span.append(combined_pred.cpu().numpy())\n",
    "                \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            all_used_labels_p.append(used_labels_p)\n",
    "            all_used_labels_a0.append(used_labels_a0)\n",
    "            all_used_labels_a1.append(used_labels_a1)\n",
    "\n",
    "    predictions = np.vstack(all_preds_span)\n",
    "    \n",
    "    # Convert boolean predictions to labels\n",
    "    predicted_labels = []\n",
    "    for pred in predictions:\n",
    "        labels = list(y_columns[pred.astype(bool)])\n",
    "        predicted_labels.append(labels)\n",
    "    \n",
    "    return predicted_labels, all_used_labels_p, all_used_labels_a0, all_used_labels_a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = 32\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset, test_dataset, train_dataloader, test_dataloader = get_datasets_dataloaders(X, y, tokenizer, recalculate_srl=False, batch_size=batch_size, max_sentences_per_article=num_sentences, max_sentence_length=64, max_arg_length=12, pickle_path=\"notebooks/X_srl_full.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted_labels, used_labels_p, used_labels_a0, used_labels_a1 = inspect(model, train_dataloader, y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(used_labels_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(y.columns)\n",
    "\n",
    "category_lists_p = {category: [] for category in categories}\n",
    "category_lists_a1 = {category: [] for category in categories}\n",
    "category_lists_a0 = {category: [] for category in categories}\n",
    "\n",
    "loader = test_dataloader\n",
    "\n",
    "for batch_idx in range(len(loader.dataset)):\n",
    "    # Iterate over each sentence\n",
    "    ds = loader.dataset[batch_idx]\n",
    "\n",
    "    for sentence_idx in range(len(used_labels_p[batch_idx])):\n",
    "\n",
    "        # Update the lists for each category\n",
    "        for cat_idx, category in enumerate(categories):\n",
    "            \n",
    "            if used_labels_p[batch_idx][cat_idx][0][cat_idx] > 0.8:\n",
    "                category_lists_p[category].append(ds[\"predicate_ids\"][sentence_idx].numpy())\n",
    "            \n",
    "            if used_labels_a0[batch_idx][cat_idx][0][cat_idx] > 0.8:\n",
    "                category_lists_a0[category].append(ds[\"arg0_ids\"][sentence_idx].numpy())\n",
    "                \n",
    "            if used_labels_a1[batch_idx][cat_idx][0][cat_idx] > 0.8:\n",
    "                category_lists_a1[category].append(ds[\"arg1_ids\"][sentence_idx].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def decode_tokens(token_dict, stop_words):\n",
    "    decoded_data = {}\n",
    "    for category, token_lists in token_dict.items():\n",
    "        decoded_data[category] = []\n",
    "        for tokens in token_lists:\n",
    "            if np.any(tokens > 0):\n",
    "                # Decode the tokens\n",
    "                decoded_text = tokenizer.decode(tokens, skip_special_tokens=True).strip()\n",
    "                # Tokenize and remove stop words\n",
    "                words = word_tokenize(decoded_text)\n",
    "                filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "                # Join the words back into a string\n",
    "                decoded_data[category].append(' '.join(filtered_words))\n",
    "    return decoded_data\n",
    "\n",
    "stop_words = set(stopwords.words('english'))  # Assuming your text is in English\n",
    "\n",
    "# Decode the token IDs for each ARG\n",
    "decoded_predicate = decode_tokens(category_lists_p, stop_words)\n",
    "decoded_arg0 = decode_tokens(category_lists_a0, stop_words)\n",
    "decoded_arg1 = decode_tokens(category_lists_a1, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to collect DataFrame rows\n",
    "rows = []\n",
    "\n",
    "# Populate the list with rows\n",
    "for frame in set(decoded_predicate) | set(decoded_arg0) | set(decoded_arg1):\n",
    "    # Get the lists, joining multiple words with a comma\n",
    "    pred_words = ', '.join([ s.strip() for s in list(set(decoded_predicate.get(frame, []))) if s is not None or l != \"\"])\n",
    "    arg0_words = ', '.join(list(set(decoded_arg0.get(frame, []))))\n",
    "    arg1_words = ', '.join(list(set(decoded_arg1.get(frame, []))))\n",
    "\n",
    "    # Create a dictionary for the row\n",
    "    row = {\n",
    "        \"Frame\": frame,\n",
    "        \"Predicate\": pred_words,\n",
    "        \"ARG0\": arg0_words,\n",
    "        \"ARG1\": arg1_words\n",
    "    }\n",
    "    \n",
    "    # Append the row dictionary to the rows list\n",
    "    rows.append(row)\n",
    "\n",
    "# Convert the list of rows to a DataFrame\n",
    "df_full_table = pd.DataFrame(rows)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_full_table.style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05c4f9fa65704ea5ba7f80e879e08510": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f54181f857a4110bf9976a56c97de97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fbfe002ccd541f9b6ab62ed4eebef35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1048f064e69b46d497b65cb9b88d0142": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc871ed94e764fe884ffdafca1445bfe",
       "IPY_MODEL_b4e7e77911e5408b84d89ccaff134708",
       "IPY_MODEL_c322e99ad9504207b7cfb9ae2ca9d6b1"
      ],
      "layout": "IPY_MODEL_dd92db5892be4357a6446146d9e9a0dd"
     }
    },
    "145c8616bab245358c8052beac5bd2bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20d6b378a69744e4a327d929c391b475": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "364a4257f8a641e18b7bde39e63a618d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "409a73804fc34a63a619a42b9468be46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "475020323c07410e87d366a6f553aafb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_145c8616bab245358c8052beac5bd2bc",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca901348dcd1428ebecc55d659fa40d1",
      "value": 28
     }
    },
    "4ddbb3fa3b924a9e9650f51204580f8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a5458551f974a94b61ff658ae59fa70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b48b8b79d6a4ff1b0655a6bfcf7a422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c06d626b5ee4fe8bdb9be7fb879aae0",
      "placeholder": "",
      "style": "IPY_MODEL_779202125255413cb719a43fe5d14ce7",
      "value": "Downloading ()solve/main/vocab.txt: 100%"
     }
    },
    "5d40c6e034fd4b278969dd928cc07dff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f4b55ead6884f3790a3acfa43232fc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87907508c4eb43f3a27858f1d397ca99",
      "placeholder": "",
      "style": "IPY_MODEL_d21ed2e527464a0ca0cb2b3b4fad928a",
      "value": "Downloading ()okenizer_config.json: 100%"
     }
    },
    "650f8e0f82e849f58a47507291b15500": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbb672669b9f4c36873048cfa1d2daf6",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cf42b39e3be94c90b3a63d05783481a3",
      "value": 231508
     }
    },
    "67ea6145091b440da6b4c5d5f8695aa9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ac495a816994aaeb8926c6c97d103ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ddbb3fa3b924a9e9650f51204580f8d",
      "placeholder": "",
      "style": "IPY_MODEL_a115ed03534a4c64a17fa55d93a0f93a",
      "value": " 570/570 [00:00&lt;00:00, 47.0kB/s]"
     }
    },
    "71add6bf108545af8db6aeb392793759": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5f4b55ead6884f3790a3acfa43232fc9",
       "IPY_MODEL_475020323c07410e87d366a6f553aafb",
       "IPY_MODEL_e2454ec2f5904e0bacde56ae7d4089a9"
      ],
      "layout": "IPY_MODEL_0f54181f857a4110bf9976a56c97de97"
     }
    },
    "779202125255413cb719a43fe5d14ce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87907508c4eb43f3a27858f1d397ca99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c06d626b5ee4fe8bdb9be7fb879aae0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e13dccd922946ab873cc287fcca5baf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93fa55d130db463c8ddffc947f2e99cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a6d16261d5b4693b09acf83dcb38920": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0265a21ae504485ab59207228c1f6d6",
      "placeholder": "",
      "style": "IPY_MODEL_8e13dccd922946ab873cc287fcca5baf",
      "value": " 232k/232k [00:00&lt;00:00, 9.51MB/s]"
     }
    },
    "a0265a21ae504485ab59207228c1f6d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a115ed03534a4c64a17fa55d93a0f93a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a74bacc051494d1ea0f4cbd656505cfe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4e7e77911e5408b84d89ccaff134708": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67ea6145091b440da6b4c5d5f8695aa9",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bef37b47c8ee49778464c05adcffa30d",
      "value": 466062
     }
    },
    "b83493088cd24629a04ec612aa522ed4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c222d2a5e4664f9b9aefed7f093d4a4e",
       "IPY_MODEL_fa94f90b735f418fb2b502f7877b5306",
       "IPY_MODEL_6ac495a816994aaeb8926c6c97d103ae"
      ],
      "layout": "IPY_MODEL_409a73804fc34a63a619a42b9468be46"
     }
    },
    "bef37b47c8ee49778464c05adcffa30d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c222d2a5e4664f9b9aefed7f093d4a4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_364a4257f8a641e18b7bde39e63a618d",
      "placeholder": "",
      "style": "IPY_MODEL_93fa55d130db463c8ddffc947f2e99cd",
      "value": "Downloading ()lve/main/config.json: 100%"
     }
    },
    "c322e99ad9504207b7cfb9ae2ca9d6b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d40c6e034fd4b278969dd928cc07dff",
      "placeholder": "",
      "style": "IPY_MODEL_f6b3c67d61114db5810bd78339a218d9",
      "value": " 466k/466k [00:00&lt;00:00, 1.88MB/s]"
     }
    },
    "ca901348dcd1428ebecc55d659fa40d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cf42b39e3be94c90b3a63d05783481a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d21ed2e527464a0ca0cb2b3b4fad928a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d568d9b894694b6fb432456031cee230": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbb672669b9f4c36873048cfa1d2daf6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd92db5892be4357a6446146d9e9a0dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2454ec2f5904e0bacde56ae7d4089a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20d6b378a69744e4a327d929c391b475",
      "placeholder": "",
      "style": "IPY_MODEL_05c4f9fa65704ea5ba7f80e879e08510",
      "value": " 28.0/28.0 [00:00&lt;00:00, 1.97kB/s]"
     }
    },
    "e9ee9acc5e414c65ad10b8864cc07b07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f19c187b1e8c4bbe9fda366f24f2b79e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5b48b8b79d6a4ff1b0655a6bfcf7a422",
       "IPY_MODEL_650f8e0f82e849f58a47507291b15500",
       "IPY_MODEL_9a6d16261d5b4693b09acf83dcb38920"
      ],
      "layout": "IPY_MODEL_d568d9b894694b6fb432456031cee230"
     }
    },
    "f6b3c67d61114db5810bd78339a218d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa94f90b735f418fb2b502f7877b5306": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a74bacc051494d1ea0f4cbd656505cfe",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0fbfe002ccd541f9b6ab62ed4eebef35",
      "value": 570
     }
    },
    "fc871ed94e764fe884ffdafca1445bfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a5458551f974a94b61ff658ae59fa70",
      "placeholder": "",
      "style": "IPY_MODEL_e9ee9acc5e414c65ad10b8864cc07b07",
      "value": "Downloading ()/main/tokenizer.json: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
