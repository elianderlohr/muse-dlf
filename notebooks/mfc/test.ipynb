{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=67480, step=1)\n"
     ]
    }
   ],
   "source": [
    "# load pkl from data\\srls\\mfc\\FRISS_srl.pkl\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('../../data/srls/mfc/FRISS_srl.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predicates per sentence\n",
      "Mean: 3.1542679312388855\n",
      "Max: 17\n",
      "Min: 1\n",
      "Number of sentences: 67480\n"
     ]
    }
   ],
   "source": [
    "# create statistics\n",
    "\n",
    "num_preds = []\n",
    "\n",
    "for k, v in data.items():\n",
    "    num_preds.append(len(v))\n",
    "\n",
    "print('Number of predicates per sentence')\n",
    "print('Mean:', sum(num_preds) / len(num_preds))\n",
    "print('Max:', max(num_preds))\n",
    "print('Min:', min(num_preds))\n",
    "print('Number of sentences:', len(num_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'predicate': 'need',\n",
       "  'ARG0': 'IMM-10005 PRIMARY Immigrants without HOPE',\n",
       "  'ARG1': 'help entering college Anxiety'},\n",
       " {'predicate': 'entering',\n",
       "  'ARG0': 'IMM-10005 PRIMARY Immigrants without HOPE',\n",
       "  'ARG1': 'college Anxiety Jose Alvarado'},\n",
       " {'predicate': 'gripped',\n",
       "  'ARG0': 'IMM-10005 PRIMARY Immigrants without HOPE need help entering college Anxiety',\n",
       "  'ARG1': 'Jose Alvarado'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MRC preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\\mfc\\data_prepared.json\n",
    "import pandas as pd\n",
    "\n",
    "with open('../../data/mfc/data_prepared.json', 'r') as f:\n",
    "    data_prepared = pd.read_json(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67480, 18)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = text.replace('  ', ' ')\n",
    "    text = text.strip()\n",
    "\n",
    "    # some texts start with \"IMM-XXXXX PRIMARY\" remove\n",
    "    text = re.sub(r'^IMM-\\d+ PRIMARY', '', text)\n",
    "\n",
    "    # remove leading and trailing whitespaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "data_prepared['text'] = data_prepared['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>text</th>\n",
       "      <th>document_frame</th>\n",
       "      <th>Capacity and Resources</th>\n",
       "      <th>Crime and Punishment</th>\n",
       "      <th>Cultural Identity</th>\n",
       "      <th>Economic</th>\n",
       "      <th>External Regulation and Reputation</th>\n",
       "      <th>Fairness and Equality</th>\n",
       "      <th>Health and Safety</th>\n",
       "      <th>Legality, Constitutionality, Jurisdiction</th>\n",
       "      <th>Morality</th>\n",
       "      <th>Other</th>\n",
       "      <th>Policy Prescription and Evaluation</th>\n",
       "      <th>Political</th>\n",
       "      <th>Public Sentiment</th>\n",
       "      <th>Quality of Life</th>\n",
       "      <th>Security and Defense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>Immigrants without HOPE need help entering col...</td>\n",
       "      <td>Quality of Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>It mounted as students went around the room te...</td>\n",
       "      <td>Quality of Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>Georgia Tech.</td>\n",
       "      <td>Quality of Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>University of Georgia.</td>\n",
       "      <td>Quality of Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>\"All I could say was, 'I'm planning to see if ...</td>\n",
       "      <td>Quality of Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             article_id                                               text  \\\n",
       "0  Immigration1.0-10005  Immigrants without HOPE need help entering col...   \n",
       "1  Immigration1.0-10005  It mounted as students went around the room te...   \n",
       "2  Immigration1.0-10005                                      Georgia Tech.   \n",
       "3  Immigration1.0-10005                             University of Georgia.   \n",
       "4  Immigration1.0-10005  \"All I could say was, 'I'm planning to see if ...   \n",
       "\n",
       "    document_frame  Capacity and Resources  Crime and Punishment  \\\n",
       "0  Quality of Life                       0                     0   \n",
       "1  Quality of Life                       0                     0   \n",
       "2  Quality of Life                       0                     0   \n",
       "3  Quality of Life                       0                     0   \n",
       "4  Quality of Life                       0                     0   \n",
       "\n",
       "   Cultural Identity  Economic  External Regulation and Reputation  \\\n",
       "0                  0         0                                   0   \n",
       "1                  0         0                                   0   \n",
       "2                  0         0                                   0   \n",
       "3                  0         0                                   0   \n",
       "4                  0         0                                   0   \n",
       "\n",
       "   Fairness and Equality  Health and Safety  \\\n",
       "0                      0                  0   \n",
       "1                      0                  0   \n",
       "2                      0                  0   \n",
       "3                      0                  0   \n",
       "4                      0                  0   \n",
       "\n",
       "   Legality, Constitutionality, Jurisdiction  Morality  Other  \\\n",
       "0                                          0         0      0   \n",
       "1                                          0         0      0   \n",
       "2                                          0         0      0   \n",
       "3                                          0         0      0   \n",
       "4                                          0         0      0   \n",
       "\n",
       "   Policy Prescription and Evaluation  Political  Public Sentiment  \\\n",
       "0                                   0          0                 0   \n",
       "1                                   0          0                 0   \n",
       "2                                   0          0                 0   \n",
       "3                                   0          0                 0   \n",
       "4                                   0          0                 0   \n",
       "\n",
       "   Quality of Life  Security and Defense  \n",
       "0                1                     0  \n",
       "1                1                     0  \n",
       "2                1                     0  \n",
       "3                1                     0  \n",
       "4                1                     0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats for text length\n",
    "\n",
    "text_lengths = data_prepared['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    67480.000000\n",
       "mean       139.985803\n",
       "std         78.341919\n",
       "min          1.000000\n",
       "25%         81.000000\n",
       "50%        130.000000\n",
       "75%        188.000000\n",
       "max       1102.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_prepared.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to json\n",
    "df.to_json('../../data/mfc/data_prepared_cleaned.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67480, 18)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load\n",
    "with open('../../data/mfc/data_prepared_cleaned.json', 'r') as f:\n",
    "    df = pd.read_json(f)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load frameaxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6097, 11)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\\frameaxis\\mfc\\frameaxis_contextualized_mft.pkl\n",
    "\n",
    "with open('../../data/frameaxis/mfc/frameaxis_contextualized_mft.pkl', 'rb') as f:\n",
    "    data_fa = pickle.load(f)\n",
    "\n",
    "data_fa.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test SRLs Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    " \n",
    "logger = logging.getLogger() \n",
    "\n",
    "\n",
    "class SRLProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        dataframe_path=\"../notebooks/classifier/X_srl_filtered.pkl\",\n",
    "        force_recalculate=False,\n",
    "        save_type=\"pickle\",\n",
    "        device=0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the SRLProcessor with a DataFrame, a path to a pickle file, and a flag indicating whether to force recalculation.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame with text data.\n",
    "            dataframe_path (str): Path to save/load the SRL components DataFrame.\n",
    "            force_recalculate (bool): If True, forces recalculation of SRL components.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.dataframe_path = dataframe_path\n",
    "        self.force_recalculate = force_recalculate\n",
    "        self.save_type = save_type\n",
    "        self.device = device\n",
    "\n",
    "        # allowed save types\n",
    "        if self.save_type not in [\"csv\", \"pickle\", \"json\"]:\n",
    "            raise ValueError(\n",
    "                \"Invalid save_type. Must be one of 'csv', 'pickle', or 'json'.\"\n",
    "            )\n",
    "\n",
    "    def get_srl_embeddings(self):\n",
    "        \"\"\"\n",
    "        Main method to process the SRL components, either by loading them from a pickle file or by recalculating.\n",
    "        \"\"\"\n",
    "        if self.force_recalculate or not os.path.exists(self.dataframe_path):\n",
    "            return self._recalculate_srl()\n",
    "        else:\n",
    "            return self._load_srl()\n",
    "\n",
    "    def _recalculate_srl(self):\n",
    "        \"\"\"\n",
    "        Recalculates the SRL components for the sentences in the DataFrame and returns a DataFrame\n",
    "        with columns for article_id, text, and srls, where srls is a list of SRL components for each text entry.\n",
    "        \"\"\"\n",
    "        logger.info(\"Recalculating SRL components...\")\n",
    "        predictor = Predictor.from_path(\n",
    "            \"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\",\n",
    "            cuda_device=self.device,\n",
    "        )\n",
    "\n",
    "        # Directly process each text entry to get SRLs and associate with article_id and text\n",
    "        srl_data = []\n",
    "        for _, row in tqdm(\n",
    "            self.df.iterrows(), desc=\"Processing SRL Batches\", total=len(self.df)\n",
    "        ):\n",
    "            article_id, text = row[\"article_id\"], row[\"text\"]\n",
    "            srls = self._extract_srl_batch(\n",
    "                [text], predictor\n",
    "            )  # Process a single text entry as a batch of size 1\n",
    "            srl_data.append(\n",
    "                {\n",
    "                    \"article_id\": article_id,\n",
    "                    \"text\": text,\n",
    "                    \"srls\": srls[\n",
    "                        0\n",
    "                    ],  # Extract the first (and only) element as each text is processed individually\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Convert the processed data into a DataFrame\n",
    "        result_df = pd.DataFrame(srl_data)\n",
    "\n",
    "        # Save the DataFrame if a path is specified\n",
    "        if self.dataframe_path:\n",
    "            if self.save_type == \"csv\":\n",
    "                result_df.to_csv(self.dataframe_path, index=False)\n",
    "            elif self.save_type == \"json\":\n",
    "                result_df.to_json(self.dataframe_path)\n",
    "            elif self.save_type == \"pickle\":\n",
    "                with open(self.dataframe_path, \"wb\") as f:\n",
    "                    pickle.dump(result_df, f)\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def _load_srl(self):\n",
    "        \"\"\"\n",
    "        Loads the SRL components from a pickle file.\n",
    "        \"\"\"\n",
    "        logger.info(\"Loading SRL components from pickle...\")\n",
    "        with open(self.dataframe_path, \"rb\") as f:\n",
    "            srl_series = pickle.load(f)\n",
    "        return srl_series\n",
    "\n",
    "    def _extract_srl_batch(self, batched_sentences, predictor):\n",
    "        \"\"\"\n",
    "        Extracts SRL components for a batch of sentences.\n",
    "        \"\"\"\n",
    "        batched_sentences = [{\"sentence\": sentence} for sentence in batched_sentences]\n",
    "        batched_srl = predictor.predict_batch_json(batched_sentences)\n",
    "\n",
    "        results = []\n",
    "        for srl in batched_srl:\n",
    "            sentence_results = []\n",
    "\n",
    "            for verb_entry in srl[\"verbs\"]:\n",
    "                arg_components = {\"ARG0\": [], \"ARG1\": []}\n",
    "                for i, tag in enumerate(verb_entry[\"tags\"]):\n",
    "                    if \"ARG0\" in tag:\n",
    "                        arg_components[\"ARG0\"].append(srl[\"words\"][i])\n",
    "                    elif \"ARG1\" in tag:\n",
    "                        arg_components[\"ARG1\"].append(srl[\"words\"][i])\n",
    "                        \n",
    "                (\n",
    "                    sentence_results.append(\n",
    "                        {\n",
    "                            \"predicate\": verb_entry[\"verb\"],\n",
    "                            \"ARG0\": \" \".join(arg_components[\"ARG0\"]),\n",
    "                            \"ARG1\": \" \".join(arg_components[\"ARG1\"]),\n",
    "                        }\n",
    "                    )\n",
    "                    if arg_components[\"ARG0\"] or arg_components[\"ARG1\"]\n",
    "                    else {\"predicate\": \"\", \"ARG0\": \"\", \"ARG1\": \"\"}\n",
    "                )\n",
    "            results.append(\n",
    "                sentence_results\n",
    "                if sentence_results\n",
    "                else [{\"predicate\": \"\", \"ARG0\": \"\", \"ARG1\": \"\"}]\n",
    "            )\n",
    "        return results\n",
    "\n",
    "    def _batch_process_srl(self, texts, article_ids, predictor, batch_size=32):\n",
    "        \"\"\"\n",
    "        Extracts SRL components for all sentences in a DataFrame in an optimized, batched manner.\n",
    "        Now also includes article IDs to ensure SRL components are associated with the correct articles.\n",
    "        \"\"\"\n",
    "        results_by_article = defaultdict(list)\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing SRL Batches\"):\n",
    "            batched_sentences = texts[i : i + batch_size].tolist()\n",
    "            batch_article_ids = article_ids[i : i + batch_size].tolist()\n",
    "            batch_results = self._extract_srl_batch(batched_sentences, predictor)\n",
    "            for article_id, srls in zip(batch_article_ids, batch_results):\n",
    "                results_by_article[article_id].extend(srls)\n",
    "        return results_by_article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first_50 = df.head(50)\n",
    "df_first = df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>text</th>\n",
       "      <th>document_frame</th>\n",
       "      <th>Capacity and Resources</th>\n",
       "      <th>Crime and Punishment</th>\n",
       "      <th>Cultural Identity</th>\n",
       "      <th>Economic</th>\n",
       "      <th>External Regulation and Reputation</th>\n",
       "      <th>Fairness and Equality</th>\n",
       "      <th>Health and Safety</th>\n",
       "      <th>Legality, Constitutionality, Jurisdiction</th>\n",
       "      <th>Morality</th>\n",
       "      <th>Other</th>\n",
       "      <th>Policy Prescription and Evaluation</th>\n",
       "      <th>Political</th>\n",
       "      <th>Public Sentiment</th>\n",
       "      <th>Quality of Life</th>\n",
       "      <th>Security and Defense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>Immigrants without HOPE need help entering col...</td>\n",
       "      <td>Quality of Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Immigration1.0-10005</td>\n",
       "      <td>It mounted as students went around the room te...</td>\n",
       "      <td>Quality of Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             article_id                                               text  \\\n",
       "0  Immigration1.0-10005  Immigrants without HOPE need help entering col...   \n",
       "1  Immigration1.0-10005  It mounted as students went around the room te...   \n",
       "\n",
       "    document_frame  Capacity and Resources  Crime and Punishment  \\\n",
       "0  Quality of Life                       0                     0   \n",
       "1  Quality of Life                       0                     0   \n",
       "\n",
       "   Cultural Identity  Economic  External Regulation and Reputation  \\\n",
       "0                  0         0                                   0   \n",
       "1                  0         0                                   0   \n",
       "\n",
       "   Fairness and Equality  Health and Safety  \\\n",
       "0                      0                  0   \n",
       "1                      0                  0   \n",
       "\n",
       "   Legality, Constitutionality, Jurisdiction  Morality  Other  \\\n",
       "0                                          0         0      0   \n",
       "1                                          0         0      0   \n",
       "\n",
       "   Policy Prescription and Evaluation  Political  Public Sentiment  \\\n",
       "0                                   0          0                 0   \n",
       "1                                   0          0                 0   \n",
       "\n",
       "   Quality of Life  Security and Defense  \n",
       "0                1                     0  \n",
       "1                1                     0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_processor = SRLProcessor(df, dataframe_path=\"srl.pkl\", force_recalculate=True, device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\elias\\AppData\\Local\\Temp\\tmp5e3o661f\\config.json as plain json\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Processing SRL Batches:   0%|          | 243/67480 [01:23<6:23:09,  2.92it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_srls \u001b[38;5;241m=\u001b[39m \u001b[43msrl_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_srl_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[38], line 47\u001b[0m, in \u001b[0;36mSRLProcessor.get_srl_embeddings\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03mMain method to process the SRL components, either by loading them from a pickle file or by recalculating.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_recalculate \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe_path):\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recalculate_srl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_srl()\n",
      "Cell \u001b[1;32mIn[38], line 68\u001b[0m, in \u001b[0;36mSRLProcessor._recalculate_srl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miterrows(), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing SRL Batches\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf)\n\u001b[0;32m     66\u001b[0m ):\n\u001b[0;32m     67\u001b[0m     article_id, text \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 68\u001b[0m     srls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_srl_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Process a single text entry as a batch of size 1\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     srl_data\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m     72\u001b[0m         {\n\u001b[0;32m     73\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: article_id,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m         }\n\u001b[0;32m     79\u001b[0m     )\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Convert the processed data into a DataFrame\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[38], line 110\u001b[0m, in \u001b[0;36mSRLProcessor._extract_srl_batch\u001b[1;34m(self, batched_sentences, predictor)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03mExtracts SRL components for a batch of sentences.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m batched_sentences \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m: sentence} \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m batched_sentences]\n\u001b[1;32m--> 110\u001b[0m batched_srl \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_batch_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_sentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m srl \u001b[38;5;129;01min\u001b[39;00m batched_srl:\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp310\\lib\\site-packages\\allennlp_models\\structured_prediction\\predictors\\srl.py:191\u001b[0m, in \u001b[0;36mSemanticRoleLabelerPredictor.predict_batch_json\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    189\u001b[0m outputs: List[Dict[\u001b[38;5;28mstr\u001b[39m, numpy\u001b[38;5;241m.\u001b[39mndarray]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batched_instances:\n\u001b[1;32m--> 191\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_on_instances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    193\u001b[0m verbs_per_sentence \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(sent) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m instances_per_sentence]\n\u001b[0;32m    194\u001b[0m return_dicts: List[JsonDict] \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbs\u001b[39m\u001b[38;5;124m\"\u001b[39m: []} \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp310\\lib\\site-packages\\allennlp\\models\\model.py:217\u001b[0m, in \u001b[0;36mModel.forward_on_instances\u001b[1;34m(self, instances)\u001b[0m\n\u001b[0;32m    215\u001b[0m dataset\u001b[38;5;241m.\u001b[39mindex_instances(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[0;32m    216\u001b[0m model_input \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mmove_to_device(dataset\u001b[38;5;241m.\u001b[39mas_tensor_dict(), cuda_device)\n\u001b[1;32m--> 217\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_output_human_readable(\u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_input))\n\u001b[0;32m    219\u001b[0m instance_separated_output: List[Dict[\u001b[38;5;28mstr\u001b[39m, numpy\u001b[38;5;241m.\u001b[39mndarray]] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    220\u001b[0m     {} \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39minstances\n\u001b[0;32m    221\u001b[0m ]\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mitems()):\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp310\\lib\\site-packages\\allennlp_models\\structured_prediction\\models\\srl_bert.py:141\u001b[0m, in \u001b[0;36mSrlBert.forward\u001b[1;34m(self, tokens, verb_indicator, metadata, tags)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03m# Parameters\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03m    A scalar loss to be optimised.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    140\u001b[0m mask \u001b[38;5;241m=\u001b[39m get_text_field_mask(tokens)\n\u001b[1;32m--> 141\u001b[0m bert_embeddings, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token_ids_from_text_field_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverb_indicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m embedded_text_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dropout(bert_embeddings)\n\u001b[0;32m    149\u001b[0m batch_size, sequence_length, _ \u001b[38;5;241m=\u001b[39m embedded_text_input\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1018\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1009\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1011\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1012\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1013\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1017\u001b[0m )\n\u001b[1;32m-> 1018\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1030\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1031\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    598\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    599\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    600\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:493\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    483\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    492\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 493\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:432\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    415\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    421\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    422\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    423\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    424\u001b[0m         hidden_states,\n\u001b[0;32m    425\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    430\u001b[0m         output_attentions,\n\u001b[0;32m    431\u001b[0m     )\n\u001b[1;32m--> 432\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:382\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 382\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    384\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_srls = srl_processor.get_srl_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of SRLs per sentence\n",
      "Mean: 2.9\n",
      "Max: 9\n",
      "Min: 1\n"
     ]
    }
   ],
   "source": [
    "# stats for srls\n",
    "\n",
    "num_srls = []\n",
    "\n",
    "for i, row in df_srls.iterrows():\n",
    "    num_srls.append(len(row[\"srls\"]))\n",
    "\n",
    "print('Number of SRLs per sentence')\n",
    "print('Mean:', sum(num_srls) / len(num_srls))\n",
    "print('Max:', max(num_srls))\n",
    "print('Min:', min(num_srls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class ArticleDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X,\n",
    "        X_srl,\n",
    "        X_frameaxis,\n",
    "        tokenizer,\n",
    "        labels=None,\n",
    "        max_sentences_per_article=32,\n",
    "        max_sentence_length=32,\n",
    "        max_args_per_sentence=10,\n",
    "        max_arg_length=16,\n",
    "        frameaxis_dim=20,\n",
    "    ):\n",
    "        self.X = X\n",
    "        self.X_srl = X_srl\n",
    "        self.X_frameaxis = X_frameaxis\n",
    "        self.labels = labels\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sentences_per_article = max_sentences_per_article\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.max_args_per_sentence = max_args_per_sentence\n",
    "        self.max_arg_length = max_arg_length\n",
    "\n",
    "        self.frameaxis_dim = frameaxis_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentences = self.X.iloc[idx]\n",
    "        srl_data = self.X_srl.iloc[idx]\n",
    "        frameaxis_data = self.X_frameaxis.iloc[idx]\n",
    "\n",
    "        # labels\n",
    "        labels = self.labels.iloc[idx]\n",
    "\n",
    "        # Tokenize sentences and get attention masks\n",
    "        sentence_ids, sentence_attention_masks = [], []\n",
    "        for sentence in sentences:\n",
    "            encoded = self.tokenizer(\n",
    "                sentence,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_sentence_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_attention_mask=True,\n",
    "            )\n",
    "            sentence_ids.append(encoded[\"input_ids\"])\n",
    "            sentence_attention_masks.append(encoded[\"attention_mask\"])\n",
    "\n",
    "        # Padding for sentences if necessary\n",
    "        while len(sentence_ids) < self.max_sentences_per_article:\n",
    "            sentence_ids.append([0] * self.max_sentence_length)\n",
    "            sentence_attention_masks.append([0] * self.max_sentence_length)\n",
    "\n",
    "        sentence_ids = sentence_ids[: self.max_sentences_per_article]\n",
    "        sentence_attention_masks = sentence_attention_masks[\n",
    "            : self.max_sentences_per_article\n",
    "        ]\n",
    "\n",
    "        # frameaxis\n",
    "        while len(frameaxis_data) < self.max_sentences_per_article:\n",
    "            frameaxis_data.append([0] * self.frameaxis_dim)\n",
    "\n",
    "        frameaxis_data = frameaxis_data[: self.max_sentences_per_article]\n",
    "\n",
    "        # replace nan values in frameaxis with 0\n",
    "        frameaxis_data = pd.DataFrame(frameaxis_data).fillna(0).values.tolist()\n",
    "\n",
    "        # Process SRL data\n",
    "        predicates, arg0s, arg1s = [], [], []\n",
    "        predicate_attention_masks, arg0_attention_masks, arg1_attention_masks = (\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "        )\n",
    "        for srl_items in srl_data:\n",
    "            sentence_predicates, sentence_arg0s, sentence_arg1s = [], [], []\n",
    "            sentence_predicate_masks, sentence_arg0_masks, sentence_arg1_masks = (\n",
    "                [],\n",
    "                [],\n",
    "                [],\n",
    "            )\n",
    "\n",
    "            if not isinstance(srl_items, list):\n",
    "                srl_items = [srl_items]\n",
    "\n",
    "            for item in srl_items:\n",
    "                encoded_predicate = self.tokenizer(\n",
    "                    item[\"predicate\"],\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_arg_length,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_attention_mask=True,\n",
    "                )\n",
    "                encoded_arg0 = self.tokenizer(\n",
    "                    item[\"ARG0\"],\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_arg_length,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_attention_mask=True,\n",
    "                )\n",
    "                encoded_arg1 = self.tokenizer(\n",
    "                    item[\"ARG1\"],\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_arg_length,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_attention_mask=True,\n",
    "                )\n",
    "\n",
    "                sentence_predicates.append(encoded_predicate[\"input_ids\"])\n",
    "                sentence_arg0s.append(encoded_arg0[\"input_ids\"])\n",
    "                sentence_arg1s.append(encoded_arg1[\"input_ids\"])\n",
    "\n",
    "                sentence_predicate_masks.append(encoded_predicate[\"attention_mask\"])\n",
    "                sentence_arg0_masks.append(encoded_arg0[\"attention_mask\"])\n",
    "                sentence_arg1_masks.append(encoded_arg1[\"attention_mask\"])\n",
    "\n",
    "            # Padding for SRL elements\n",
    "            for _ in range(self.max_args_per_sentence):\n",
    "                sentence_predicates.append([0] * self.max_arg_length)\n",
    "                sentence_arg0s.append([0] * self.max_arg_length)\n",
    "                sentence_arg1s.append([0] * self.max_arg_length)\n",
    "\n",
    "                sentence_predicate_masks.append([0] * self.max_arg_length)\n",
    "                sentence_arg0_masks.append([0] * self.max_arg_length)\n",
    "                sentence_arg1_masks.append([0] * self.max_arg_length)\n",
    "\n",
    "            sentence_predicates = sentence_predicates[: self.max_args_per_sentence]\n",
    "            sentence_arg0s = sentence_arg0s[: self.max_args_per_sentence]\n",
    "            sentence_arg1s = sentence_arg1s[: self.max_args_per_sentence]\n",
    "\n",
    "            sentence_predicate_masks = sentence_predicate_masks[\n",
    "                : self.max_args_per_sentence\n",
    "            ]\n",
    "            sentence_arg0_masks = sentence_arg0_masks[: self.max_args_per_sentence]\n",
    "            sentence_arg1_masks = sentence_arg1_masks[: self.max_args_per_sentence]\n",
    "\n",
    "            predicates.append(sentence_predicates)\n",
    "            arg0s.append(sentence_arg0s)\n",
    "            arg1s.append(sentence_arg1s)\n",
    "\n",
    "            predicate_attention_masks.append(sentence_predicate_masks)\n",
    "            arg0_attention_masks.append(sentence_arg0_masks)\n",
    "            arg1_attention_masks.append(sentence_arg1_masks)\n",
    "\n",
    "        # Padding for SRL data\n",
    "        srl_padding = [[0] * self.max_arg_length] * self.max_args_per_sentence\n",
    "        mask_padding = [[0] * self.max_arg_length] * self.max_args_per_sentence\n",
    "\n",
    "        predicates = (predicates + [srl_padding] * self.max_sentences_per_article)[\n",
    "            : self.max_sentences_per_article\n",
    "        ]\n",
    "        arg0s = (arg0s + [srl_padding] * self.max_sentences_per_article)[\n",
    "            : self.max_sentences_per_article\n",
    "        ]\n",
    "        arg1s = (arg1s + [srl_padding] * self.max_sentences_per_article)[\n",
    "            : self.max_sentences_per_article\n",
    "        ]\n",
    "\n",
    "        predicate_attention_masks = (\n",
    "            predicate_attention_masks + [mask_padding] * self.max_sentences_per_article\n",
    "        )[: self.max_sentences_per_article]\n",
    "        arg0_attention_masks = (\n",
    "            arg0_attention_masks + [mask_padding] * self.max_sentences_per_article\n",
    "        )[: self.max_sentences_per_article]\n",
    "        arg1_attention_masks = (\n",
    "            arg1_attention_masks + [mask_padding] * self.max_sentences_per_article\n",
    "        )[: self.max_sentences_per_article]\n",
    "\n",
    "        data = {\n",
    "            \"sentence_ids\": torch.tensor(sentence_ids, dtype=torch.long),\n",
    "            \"sentence_attention_masks\": torch.tensor(\n",
    "                sentence_attention_masks, dtype=torch.long\n",
    "            ),\n",
    "            \"predicate_ids\": torch.tensor(predicates, dtype=torch.long),\n",
    "            \"predicate_attention_masks\": torch.tensor(\n",
    "                predicate_attention_masks, dtype=torch.long\n",
    "            ),\n",
    "            \"arg0_ids\": torch.tensor(arg0s, dtype=torch.long),\n",
    "            \"arg0_attention_masks\": torch.tensor(\n",
    "                arg0_attention_masks, dtype=torch.long\n",
    "            ),\n",
    "            \"arg1_ids\": torch.tensor(arg1s, dtype=torch.long),\n",
    "            \"arg1_attention_masks\": torch.tensor(\n",
    "                arg1_attention_masks, dtype=torch.long\n",
    "            ),\n",
    "            \"frameaxis\": torch.tensor(frameaxis_data, dtype=torch.float),\n",
    "            \"labels\": torch.tensor(labels[0], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Extract individual lists from the batch\n",
    "    sentence_ids = [item[\"sentence_ids\"] for item in batch]\n",
    "    sentence_attention_masks = [item[\"sentence_attention_masks\"] for item in batch]\n",
    "    predicate_ids = [item[\"predicate_ids\"] for item in batch]\n",
    "    predicate_attention_masks = [item[\"predicate_attention_masks\"] for item in batch]\n",
    "    arg0_ids = [item[\"arg0_ids\"] for item in batch]\n",
    "    arg0_attention_masks = [item[\"arg0_attention_masks\"] for item in batch]\n",
    "    arg1_ids = [item[\"arg1_ids\"] for item in batch]\n",
    "    arg1_attention_masks = [item[\"arg1_attention_masks\"] for item in batch]\n",
    "    frameaxis = [item[\"frameaxis\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    # Pad each list\n",
    "    sentence_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        sentence_ids, batch_first=True, padding_value=0\n",
    "    )\n",
    "    sentence_attention_masks = torch.nn.utils.rnn.pad_sequence(\n",
    "        sentence_attention_masks, batch_first=True, padding_value=0\n",
    "    )\n",
    "    predicate_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        predicate_ids, batch_first=True, padding_value=0\n",
    "    )\n",
    "    predicate_attention_masks = torch.nn.utils.rnn.pad_sequence(\n",
    "        predicate_attention_masks, batch_first=True, padding_value=0\n",
    "    )\n",
    "    arg0_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        arg0_ids, batch_first=True, padding_value=0\n",
    "    )\n",
    "    arg0_attention_masks = torch.nn.utils.rnn.pad_sequence(\n",
    "        arg0_attention_masks, batch_first=True, padding_value=0\n",
    "    )\n",
    "    arg1_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        arg1_ids, batch_first=True, padding_value=0\n",
    "    )\n",
    "    arg1_attention_masks = torch.nn.utils.rnn.pad_sequence(\n",
    "        arg1_attention_masks, batch_first=True, padding_value=0\n",
    "    )\n",
    "    frameaxis = torch.nn.utils.rnn.pad_sequence(\n",
    "        frameaxis, batch_first=True, padding_value=0\n",
    "    )\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Create the output dictionary\n",
    "    output_dict = {\n",
    "        \"sentence_ids\": sentence_ids,\n",
    "        \"sentence_attention_masks\": sentence_attention_masks,\n",
    "        \"predicate_ids\": predicate_ids,\n",
    "        \"predicate_attention_masks\": predicate_attention_masks,\n",
    "        \"arg0_ids\": arg0_ids,\n",
    "        \"arg0_attention_masks\": arg0_attention_masks,\n",
    "        \"arg1_ids\": arg1_ids,\n",
    "        \"arg1_attention_masks\": arg1_attention_masks,\n",
    "        \"frameaxis\": frameaxis,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "    return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast, BertModel, RobertaTokenizerFast, RobertaModel\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import json\n",
    "\n",
    "from logging import getLogger\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "# make logger print to console\n",
    "import sys\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "\n",
    "class FrameAxisProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        path_antonym_pairs=\"frameaxis/axes/custom.tsv\",\n",
    "        dataframe_path=None,\n",
    "        bert_model_name=\"bert-base-uncased\",\n",
    "        name_tokenizer=\"bert-base-uncased\",\n",
    "        path_name_bert_model=\"bert-base-uncased\",\n",
    "        force_recalculate=False,\n",
    "        save_type=\"pickle\",\n",
    "        dim_names=[\"positive\", \"negative\"],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        FrameAxisProcessor constructor\n",
    "\n",
    "        Args:\n",
    "        df (pd.DataFrame): DataFrame with text data\n",
    "        path_antonym_pairs (str): Path to the antonym pairs file\n",
    "        dataframe_path (str): Path to save the FrameAxis Embeddings DataFrame for saving and loading\n",
    "        name_tokenizer (str): Name or path of the model\n",
    "        path_name_bert_model (str): Name or path of the model\n",
    "        force_recalculate (bool): If True, recalculate the FrameAxis Embeddings\n",
    "        save_type (str): Type of file to save the FrameAxis Embeddings DataFrame\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.force_recalculate = force_recalculate\n",
    "        self.dataframe_path = dataframe_path\n",
    "\n",
    "        if bert_model_name == \"bert-base-uncased\":\n",
    "            self.tokenizer = BertTokenizerFast.from_pretrained(name_tokenizer)\n",
    "            self.model = BertModel.from_pretrained(path_name_bert_model)\n",
    "        elif bert_model_name == \"roberta-base\":\n",
    "            self.tokenizer = RobertaTokenizerFast.from_pretrained(name_tokenizer)\n",
    "            self.model = RobertaModel.from_pretrained(path_name_bert_model)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            logger.info(\"Using CUDA\")\n",
    "            self.model.cuda()\n",
    "\n",
    "        self.antonym_pairs = {}\n",
    "        with open(path_antonym_pairs) as f:\n",
    "            self.antonym_pairs = json.load(f)\n",
    "\n",
    "        self.dim_names = dim_names\n",
    "\n",
    "        # Load the stopwords and non-word characters\n",
    "        nltk.download(\"stopwords\")\n",
    "        self.stopwords = set(stopwords.words(\"english\"))\n",
    "        self.non_word_characters = set(string.punctuation)\n",
    "\n",
    "        # allowed save types\n",
    "        self.save_type = save_type\n",
    "        if save_type not in [\"csv\", \"pickle\", \"json\"]:\n",
    "            raise ValueError(\n",
    "                \"Invalid save_type. Must be one of 'csv', 'pickle', or 'json'.\"\n",
    "            )\n",
    "\n",
    "    def _load_antonym_pairs(self, axis_path):\n",
    "        axes_df = pd.read_csv(axis_path, sep=\"\\t\", header=None)\n",
    "        return [tuple(x) for x in axes_df.values]\n",
    "\n",
    "    def precompute_antonym_embeddings(self):\n",
    "        frame_axis_words = []\n",
    "        for _, pairs in self.antonym_pairs.items():\n",
    "            for dim, words in pairs.items():\n",
    "                frame_axis_words.extend(words)\n",
    "\n",
    "        antonym_embeddings = {}\n",
    "\n",
    "        for _, row in tqdm(\n",
    "            self.df.iterrows(),\n",
    "            desc=\"Generating antonym embeddings\",\n",
    "            total=self.df.shape[0],\n",
    "        ):\n",
    "            # Access the article text from the 'text' column\n",
    "            article_text = row[\"text\"]\n",
    "            embeddings = self.get_embeddings_for_words(article_text, frame_axis_words)\n",
    "\n",
    "            # Add the embeddings to the microframes based on the word\n",
    "            for word, embedding in embeddings.items():\n",
    "                antonym_embeddings.setdefault(word, []).append(embedding)\n",
    "\n",
    "        antonym_avg_embeddings = {}\n",
    "\n",
    "        for key, value in tqdm(\n",
    "            self.antonym_pairs.items(), desc=\"Generating average embeddings\"\n",
    "        ):\n",
    "            antonym_avg_embeddings[key] = {}\n",
    "            for dim, words in tqdm(\n",
    "                value.items(), desc=\"Processing dimension\", leave=False\n",
    "            ):\n",
    "                antonym_avg_embeddings[key][dim] = {}\n",
    "\n",
    "                for word in tqdm(words, desc=\"Processing word\", leave=False):\n",
    "                    # Ensure the word is in antonym_embeddings to handle cases where it might not be found\n",
    "                    if word in antonym_embeddings:\n",
    "                        word_embed = antonym_embeddings[word]\n",
    "\n",
    "                        # Convert each tensor in word_embed to the appropriate device (GPU if available)\n",
    "                        word_embed = [\n",
    "                            embed.to(self.model.device) for embed in word_embed\n",
    "                        ]\n",
    "\n",
    "                        # Get the average of the torch word embeddings, ensuring computation happens on the same device\n",
    "                        avg_word_embed = torch.mean(torch.stack(word_embed), dim=0)\n",
    "\n",
    "                        antonym_avg_embeddings[key][dim][word] = avg_word_embed\n",
    "\n",
    "        microframes = {}\n",
    "\n",
    "        for key, value in tqdm(\n",
    "            antonym_avg_embeddings.items(), desc=\"Generating microframes\"\n",
    "        ):\n",
    "            microframes[key] = {}\n",
    "\n",
    "            pos_embeddings = antonym_avg_embeddings[key][self.dim_names[0]]\n",
    "            neg_embeddings = antonym_avg_embeddings[key][self.dim_names[1]]\n",
    "\n",
    "            \n",
    "            pos_embeddings_list = [embed for embed in pos_embeddings.values()]\n",
    "            neg_embeddings_list = [embed for embed in neg_embeddings.values()]\n",
    "\n",
    "            # only stack if not empty\n",
    "            if pos_embeddings_list:\n",
    "                pos_embedding_avg = torch.mean(torch.stack(pos_embeddings_list), dim=0)\n",
    "            else:\n",
    "                pos_embedding_avg = torch.zeros(768)\n",
    "\n",
    "            if neg_embeddings_list:\n",
    "                neg_embedding_avg = torch.mean(torch.stack(neg_embeddings_list), dim=0)\n",
    "            else:\n",
    "                neg_embedding_avg = torch.zeros(768)\n",
    "                \n",
    "            microframes[key] = {\n",
    "                self.dim_names[0]: pos_embedding_avg,\n",
    "                self.dim_names[1]: neg_embedding_avg,\n",
    "            }\n",
    "\n",
    "        return microframes\n",
    "\n",
    "    def calculate_word_contributions(self, df, antonym_pairs_embeddings):\n",
    "        \"\"\"\n",
    "        Calculates the bias scores for each word in each document and aggregates them into a list of dictionaries.\n",
    "        :param df: A DataFrame containing the articles.\n",
    "        :param antonym_pairs_embeddings: A dictionary containing the embeddings for antonym pairs for each dimension.\n",
    "        :return: A DataFrame with each row containing a list of dictionaries, each representing a word and its corresponding bias score.\n",
    "        \"\"\"\n",
    "\n",
    "        def calculate_word_contribution(article_id, text):\n",
    "            words, embeddings = self.get_embeddings_for_text(text)\n",
    "\n",
    "            if embeddings.numel() == 0:\n",
    "                print(f\"No embeddings found for article {article_id}, words: {words}\")\n",
    "                return []\n",
    "\n",
    "            # List to collect word contribution dictionaries\n",
    "            word_contributions = []\n",
    "\n",
    "            for word, embedding in zip(words, embeddings):\n",
    "                word_dict = {\"word\": word}\n",
    "                for dimension in antonym_pairs_embeddings:\n",
    "                    pos_embedding = antonym_pairs_embeddings[dimension][self.dim_names[0]].reshape(1, -1)\n",
    "                    neg_embedding = antonym_pairs_embeddings[dimension][self.dim_names[1]].reshape(1, -1)\n",
    "                    diff_vector = neg_embedding - pos_embedding\n",
    "\n",
    "                    diff_norm = F.normalize(diff_vector, p=2, dim=1).to(self.model.device)\n",
    "                    embedding = embedding.unsqueeze(0).to(self.model.device) if embedding.dim() == 1 else embedding.to(self.model.device)\n",
    "                    embedding_norm = F.normalize(embedding, p=2, dim=1)\n",
    "\n",
    "                    cos_sim = torch.matmul(embedding_norm, diff_norm.T).squeeze().cpu().item()\n",
    "\n",
    "                    word_dict[dimension] = cos_sim\n",
    "                \n",
    "                word_contributions.append(word_dict)\n",
    "\n",
    "            return word_contributions\n",
    "\n",
    "        tqdm.pandas(desc=\"Calculating Word Contributions\")\n",
    "        df['word_contributions'] = df.progress_apply(lambda row: calculate_word_contribution(row['article_id'], row['text']), axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def calculate_microframe_bias(self, df):\n",
    "        # Initialize a DataFrame to collect microframe bias results\n",
    "        bias_results = []\n",
    "\n",
    "        # Iterate over each row in the DataFrame\n",
    "        for idx, row in df.iterrows():\n",
    "            # Each 'word_contributions' entry is a list of dictionaries with words and their contributions\n",
    "            word_contributions = row['word_contributions']\n",
    "\n",
    "            # Initialize a dictionary to hold bias calculations for this article\n",
    "            bias_dict = {'article_id': row['article_id']}\n",
    "\n",
    "            if word_contributions:\n",
    "                dimensions = [k for k in word_contributions[0].keys() if k not in ['word']]\n",
    "                for dimension in dimensions:\n",
    "                    # Calculate weighted contributions for each dimension\n",
    "                    weighted_contributions = sum(d[dimension] for d in word_contributions if dimension in d)\n",
    "\n",
    "                    # Calculate microframe bias for the dimension\n",
    "                    microframe_bias = weighted_contributions / len(word_contributions)\n",
    "                    bias_dict[dimension + '_bias'] = microframe_bias\n",
    "\n",
    "            # Append the results for this article to the results list\n",
    "            bias_results.append(bias_dict)\n",
    "\n",
    "        # Convert the results list to a DataFrame\n",
    "        bias_df = pd.DataFrame(bias_results)\n",
    "        bias_df = bias_df.set_index('article_id')\n",
    "\n",
    "        return bias_df\n",
    "\n",
    "    def calculate_baseline_bias(self, df):\n",
    "        \"\"\"\n",
    "        Calculate the baseline microframe bias for the entire corpus.\n",
    "\n",
    "        :param df: A DataFrame with columns for article_id, word, and microframe cosine similarities.\n",
    "        :return: A dictionary of baseline biases for each microframe dimension.\n",
    "        \"\"\"\n",
    "\n",
    "        baseline_bias = {}        \n",
    "        for idx, row in df.iterrows():\n",
    "            word_contributions = row['word_contributions']\n",
    "\n",
    "            if word_contributions:\n",
    "                dimensions = [k for k in word_contributions[0].keys() if k not in ['word']]\n",
    "                for dimension in dimensions:\n",
    "                    baseline_bias.setdefault(dimension, []).extend(\n",
    "                        [d[dimension] for d in word_contributions if dimension in d]\n",
    "                    )\n",
    "\n",
    "        for dimension in baseline_bias:\n",
    "            baseline_bias[dimension] = sum(baseline_bias[dimension]) / len(baseline_bias[dimension])\n",
    "\n",
    "        return baseline_bias\n",
    "\n",
    "    def calculate_microframe_intensity(self, df):\n",
    "        \"\"\"\n",
    "        Calculate the microframe intensity for each document in the DataFrame.\n",
    "\n",
    "        :param df: A DataFrame containing the word contributions and article IDs.\n",
    "        :return: A DataFrame with the microframe intensity for each article and dimension.\n",
    "        \"\"\"\n",
    "        # First, calculate the baseline bias for the corpus\n",
    "        baseline_bias = self.calculate_baseline_bias(df)\n",
    "\n",
    "        # Initialize DataFrame to store intensity results\n",
    "        intensity_df = pd.DataFrame()\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            word_contributions = row['word_contributions']\n",
    "\n",
    "            # Initialize a DataFrame to store the intensity results for this article\n",
    "            intensity_dict = {'article_id': row['article_id']}\n",
    "            total_contributions = len(word_contributions)\n",
    "\n",
    "            for dimension in [k for k in word_contributions[0].keys() if k not in ['word']]:\n",
    "                # Calculate the second moment for the dimension\n",
    "                deviations_squared = sum((d[dimension] - baseline_bias[dimension]) ** 2 for d in word_contributions if dimension in d)\n",
    "                microframe_intensity = deviations_squared / total_contributions if total_contributions else 0\n",
    "\n",
    "                # Store the results\n",
    "                intensity_dict[dimension + '_intensity'] = microframe_intensity\n",
    "\n",
    "            # Append to the intensity DataFrame\n",
    "            intensity_df = pd.concat([intensity_df, pd.DataFrame([intensity_dict])], ignore_index=True)\n",
    "\n",
    "        intensity_df = intensity_df.set_index('article_id')\n",
    "\n",
    "        return intensity_df\n",
    "\n",
    "    def calculate_all_metrics(self, df, antonym_pairs_embeddings):\n",
    "        \"\"\"\n",
    "        Executes the calculation of word contributions, microframe bias, and microframe intensity for each article.\n",
    "\n",
    "        :param df: A DataFrame containing articles with 'article_id' and 'text' columns.\n",
    "        :param antonym_pairs_embeddings: A dictionary containing the embeddings for antonym pairs for each dimension.\n",
    "        :return: A DataFrame with the structure article_id | dim1_bias | dim1_intensity | ...\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(\"Calculating all metrics...\")\n",
    "        logger.info(\"Step 1: Calculating word contributions...\")\n",
    "        # Step 1: Calculate word contributions for each article and dimension\n",
    "        word_contributions_df = self.calculate_word_contributions(\n",
    "            df, antonym_pairs_embeddings\n",
    "        )\n",
    "\n",
    "        logger.info(\"Step 2: Calculating microframe bias...\")\n",
    "        # Step 2: Calculate microframe bias for each article and dimension\n",
    "        microframe_bias_df = self.calculate_microframe_bias(word_contributions_df)\n",
    "\n",
    "        logger.info(\"Step 3: Calculating microframe intensity...\")\n",
    "        # Step 3: Calculate microframe intensity for each article and dimension\n",
    "        microframe_intensity_df = self.calculate_microframe_intensity(\n",
    "            word_contributions_df\n",
    "        )\n",
    "\n",
    "        if 0 == 1:            \n",
    "            logger.info(\"Step 4: Merging bias and intensity dataframes...\")\n",
    "            # Merge the bias and intensity dataframes\n",
    "            final_df = microframe_bias_df.merge(\n",
    "                microframe_intensity_df, left_index=True, right_index=True\n",
    "            )\n",
    "\n",
    "            print(\"Size: \", final_df.shape)\n",
    "\n",
    "            # Reformat the final DataFrame to match the desired structure\n",
    "            final_df.reset_index(inplace=True)\n",
    "            final_columns = [\"article_id\"]\n",
    "            for dimension in [\n",
    "                col.replace(\"_bias\", \"\")\n",
    "                for col in microframe_bias_df.columns\n",
    "                if \"_bias\" in col\n",
    "            ]:\n",
    "                final_columns.append(dimension + \"_bias\")\n",
    "                final_columns.append(dimension + \"_intensity\")\n",
    "            final_df = final_df[final_columns]\n",
    "\n",
    "        return microframe_intensity_df\n",
    "\n",
    "    def get_embeddings_for_text(\n",
    "        self, text, remove_stopwords=True, remove_non_words=True\n",
    "    ):\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        ).to(self.model.device)\n",
    "\n",
    "        # Obtain the embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "        # Initialize lists for filtered tokens' embeddings and words\n",
    "        filtered_embeddings = []\n",
    "        filtered_words = []\n",
    "\n",
    "        # Obtain a list mapping words to tokens\n",
    "        word_ids = inputs.word_ids()\n",
    "\n",
    "        for w_idx in set(word_ids):\n",
    "            if w_idx is None:  # Skip special tokens\n",
    "                continue\n",
    "\n",
    "            # Obtain the start and end token positions for the current word\n",
    "            word_tokens_range = inputs.word_to_tokens(w_idx)\n",
    "\n",
    "            if word_tokens_range is None:\n",
    "                continue\n",
    "\n",
    "            start, end = word_tokens_range\n",
    "\n",
    "            # Reconstruct the word from tokens to check against stopwords and non-word characters\n",
    "            word = self.tokenizer.decode(inputs.input_ids[0][start:end])\n",
    "\n",
    "            # Normalize the word for checks\n",
    "            normalized_word = word.lower().strip(string.punctuation).strip()\n",
    "\n",
    "            if remove_stopwords and normalized_word in self.stopwords:\n",
    "                continue\n",
    "\n",
    "            if remove_non_words and all(\n",
    "                char in self.non_word_characters for char in normalized_word\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            # If the word passes the filters, append its embeddings and the word itself\n",
    "            word_embeddings = embeddings[start:end]\n",
    "            filtered_embeddings.append(word_embeddings.mean(dim=0))\n",
    "            filtered_words.append(normalized_word)\n",
    "\n",
    "        # Stack the filtered embeddings\n",
    "        filtered_embeddings_tensor = (\n",
    "            torch.stack(filtered_embeddings)\n",
    "            if filtered_embeddings\n",
    "            else torch.tensor([])\n",
    "        )\n",
    "\n",
    "        # if no embeddings were found, logger.info debug info\n",
    "        if filtered_embeddings_tensor.numel() == 0:\n",
    "            logger.info(\n",
    "                f\"No embeddings found for input text: {text}, after filtering: {filtered_words}\"\n",
    "            )\n",
    "\n",
    "        return filtered_words, filtered_embeddings_tensor\n",
    "\n",
    "    def get_embeddings_for_words(self, sentence, words):\n",
    "        \"\"\"\n",
    "        Get the contextualized embeddings for a list of words using the sentence as context.\n",
    "\n",
    "        :param sentence: The sentence to get embeddings from.\n",
    "        :param words: A list of words to get embeddings for.\n",
    "        :return: A dictionary containing the average embeddings for each word.\n",
    "        \"\"\"\n",
    "        sentence_words, word_embeddings = self.get_embeddings_for_text(\n",
    "            sentence, remove_stopwords=False, remove_non_words=False\n",
    "        )\n",
    "\n",
    "        # Initialize dictionary to hold word embeddings\n",
    "        embeddings = {}\n",
    "\n",
    "        # Iterate over each word to get its embedding\n",
    "        for word in words:\n",
    "            if word in sentence_words:\n",
    "                word_idx = sentence_words.index(word)\n",
    "\n",
    "                embedding = word_embeddings[word_idx]\n",
    "\n",
    "                embeddings[word] = embedding\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def get_frameaxis_data(self):\n",
    "        \"\"\"\n",
    "        Calculate the FrameAxis Values for the DataFrame\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: DataFrame with FrameAxis Embeddings\n",
    "        \"\"\"\n",
    "        # check if self.dataframe_path is None\n",
    "        if not self.force_recalculate and (\n",
    "            not self.dataframe_path or not os.path.exists(self.dataframe_path)\n",
    "        ):\n",
    "            self.force_recalculate = True\n",
    "\n",
    "        if self.force_recalculate:\n",
    "            logger.info(\"Calculating FrameAxis Embeddings\")\n",
    "\n",
    "            antonym_pairs_embeddings = self.precompute_antonym_embeddings()\n",
    "\n",
    "            frameaxis_df = self.calculate_all_metrics(self.df, antonym_pairs_embeddings)\n",
    "\n",
    "            if self.dataframe_path:\n",
    "                if self.save_type == \"csv\":\n",
    "                    frameaxis_df.to_csv(self.dataframe_path, index=False)\n",
    "                if self.save_type == \"json\":\n",
    "                    frameaxis_df.to_json(self.dataframe_path)\n",
    "                elif self.save_type == \"pickle\":\n",
    "                    with open(self.dataframe_path, \"wb\") as f:\n",
    "                        pickle.dump(frameaxis_df, f)\n",
    "\n",
    "            return frameaxis_df\n",
    "        else:\n",
    "            # load from pickle\n",
    "            logger.info(\"Loading FrameAxis Embeddings\")\n",
    "            with open(self.dataframe_path, \"rb\") as f:\n",
    "                frameaxis_df = pickle.load(f)\n",
    "\n",
    "            return frameaxis_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elias\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "frameaxis_preprocessor = FrameAxisProcessor(\n",
    "    df_first_50,\n",
    "    path_antonym_pairs=\"../../data/axis/mft.json\",\n",
    "    dataframe_path=\"frameaxis.pkl\",\n",
    "    force_recalculate=True,\n",
    "    save_type=\"pickle\",\n",
    "    dim_names=[\"virtue\", \"vice\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating antonym embeddings: 100%|██████████| 50/50 [00:06<00:00,  7.73it/s]\n",
      "Generating average embeddings:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Generating average embeddings:  60%|██████    | 3/5 [00:00<00:00, 16.44it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Generating average embeddings: 100%|██████████| 5/5 [00:00<00:00, 15.74it/s]\n",
      "Generating microframes: 100%|██████████| 5/5 [00:00<00:00, 1248.97it/s]\n",
      "Calculating Word Contributions:  62%|██████▏   | 31/50 [00:03<00:02,  7.91it/s]"
     ]
    }
   ],
   "source": [
    "fx_df = frameaxis_preprocessor.get_frameaxis_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'immigrants',\n",
       "  'care': 0.3375058174133301,\n",
       "  'loyalty': -0.2993716597557068,\n",
       "  'authority': 0.006949363276362419,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.37106406688690186},\n",
       " {'word': 'without',\n",
       "  'care': 0.42150604724884033,\n",
       "  'loyalty': -0.3720255494117737,\n",
       "  'authority': 0.009534255601465702,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.45589500665664673},\n",
       " {'word': 'hope',\n",
       "  'care': 0.4013954699039459,\n",
       "  'loyalty': -0.36111950874328613,\n",
       "  'authority': 0.020152224227786064,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.40954044461250305},\n",
       " {'word': 'need',\n",
       "  'care': 0.419064998626709,\n",
       "  'loyalty': -0.38503146171569824,\n",
       "  'authority': 0.020468231290578842,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.40951722860336304},\n",
       " {'word': 'help',\n",
       "  'care': 0.4097133278846741,\n",
       "  'loyalty': -0.3968883752822876,\n",
       "  'authority': 0.042825471609830856,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.42373716831207275},\n",
       " {'word': 'entering',\n",
       "  'care': 0.3933647871017456,\n",
       "  'loyalty': -0.3986949920654297,\n",
       "  'authority': 0.10797394067049026,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.4569146931171417},\n",
       " {'word': 'college',\n",
       "  'care': 0.4327069818973541,\n",
       "  'loyalty': -0.4067041277885437,\n",
       "  'authority': 0.07873570919036865,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.4539717137813568},\n",
       " {'word': 'anxiety',\n",
       "  'care': 0.40491431951522827,\n",
       "  'loyalty': -0.40378546714782715,\n",
       "  'authority': 0.06802364438772202,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.4909628927707672},\n",
       " {'word': 'gripped',\n",
       "  'care': 0.34792008996009827,\n",
       "  'loyalty': -0.35193055868148804,\n",
       "  'authority': 0.08797436952590942,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.44829261302948},\n",
       " {'word': 'jose',\n",
       "  'care': 0.37817704677581787,\n",
       "  'loyalty': -0.33017486333847046,\n",
       "  'authority': 0.08556583523750305,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.4465019702911377},\n",
       " {'word': 'alvarado',\n",
       "  'care': 0.3500090539455414,\n",
       "  'loyalty': -0.4013349413871765,\n",
       "  'authority': 0.04559655115008354,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.46084654331207275},\n",
       " {'word': 'third',\n",
       "  'care': 0.34851378202438354,\n",
       "  'loyalty': -0.32293838262557983,\n",
       "  'authority': -0.0559212826192379,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.4341393709182739},\n",
       " {'word': 'period',\n",
       "  'care': 0.35885825753211975,\n",
       "  'loyalty': -0.38639697432518005,\n",
       "  'authority': -0.005653202533721924,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.4526180922985077},\n",
       " {'word': 'calculus',\n",
       "  'care': 0.3850191831588745,\n",
       "  'loyalty': -0.36428743600845337,\n",
       "  'authority': -0.027881909161806107,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.4760323166847229},\n",
       " {'word': 'class',\n",
       "  'care': 0.39795419573783875,\n",
       "  'loyalty': -0.37785500288009644,\n",
       "  'authority': -0.027528641745448112,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.49096864461898804},\n",
       " {'word': 'north',\n",
       "  'care': 0.41090071201324463,\n",
       "  'loyalty': -0.339061975479126,\n",
       "  'authority': 0.03062911331653595,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.4326138198375702},\n",
       " {'word': 'atlanta',\n",
       "  'care': 0.39267101883888245,\n",
       "  'loyalty': -0.3832675516605377,\n",
       "  'authority': 0.010805170983076096,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.42723527550697327},\n",
       " {'word': 'high',\n",
       "  'care': 0.3625200390815735,\n",
       "  'loyalty': -0.3897514343261719,\n",
       "  'authority': -0.0820644199848175,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.3674452602863312},\n",
       " {'word': 'school',\n",
       "  'care': 0.36444002389907837,\n",
       "  'loyalty': -0.39990508556365967,\n",
       "  'authority': -0.04313705861568451,\n",
       "  'fairness': 0.0,\n",
       "  'sanctity': -0.37093615531921387}]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05c4f9fa65704ea5ba7f80e879e08510": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f54181f857a4110bf9976a56c97de97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fbfe002ccd541f9b6ab62ed4eebef35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1048f064e69b46d497b65cb9b88d0142": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc871ed94e764fe884ffdafca1445bfe",
       "IPY_MODEL_b4e7e77911e5408b84d89ccaff134708",
       "IPY_MODEL_c322e99ad9504207b7cfb9ae2ca9d6b1"
      ],
      "layout": "IPY_MODEL_dd92db5892be4357a6446146d9e9a0dd"
     }
    },
    "145c8616bab245358c8052beac5bd2bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20d6b378a69744e4a327d929c391b475": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "364a4257f8a641e18b7bde39e63a618d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "409a73804fc34a63a619a42b9468be46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "475020323c07410e87d366a6f553aafb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_145c8616bab245358c8052beac5bd2bc",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca901348dcd1428ebecc55d659fa40d1",
      "value": 28
     }
    },
    "4ddbb3fa3b924a9e9650f51204580f8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a5458551f974a94b61ff658ae59fa70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b48b8b79d6a4ff1b0655a6bfcf7a422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c06d626b5ee4fe8bdb9be7fb879aae0",
      "placeholder": "​",
      "style": "IPY_MODEL_779202125255413cb719a43fe5d14ce7",
      "value": "Downloading (…)solve/main/vocab.txt: 100%"
     }
    },
    "5d40c6e034fd4b278969dd928cc07dff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f4b55ead6884f3790a3acfa43232fc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87907508c4eb43f3a27858f1d397ca99",
      "placeholder": "​",
      "style": "IPY_MODEL_d21ed2e527464a0ca0cb2b3b4fad928a",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "650f8e0f82e849f58a47507291b15500": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbb672669b9f4c36873048cfa1d2daf6",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cf42b39e3be94c90b3a63d05783481a3",
      "value": 231508
     }
    },
    "67ea6145091b440da6b4c5d5f8695aa9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ac495a816994aaeb8926c6c97d103ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ddbb3fa3b924a9e9650f51204580f8d",
      "placeholder": "​",
      "style": "IPY_MODEL_a115ed03534a4c64a17fa55d93a0f93a",
      "value": " 570/570 [00:00&lt;00:00, 47.0kB/s]"
     }
    },
    "71add6bf108545af8db6aeb392793759": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5f4b55ead6884f3790a3acfa43232fc9",
       "IPY_MODEL_475020323c07410e87d366a6f553aafb",
       "IPY_MODEL_e2454ec2f5904e0bacde56ae7d4089a9"
      ],
      "layout": "IPY_MODEL_0f54181f857a4110bf9976a56c97de97"
     }
    },
    "779202125255413cb719a43fe5d14ce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87907508c4eb43f3a27858f1d397ca99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c06d626b5ee4fe8bdb9be7fb879aae0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e13dccd922946ab873cc287fcca5baf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93fa55d130db463c8ddffc947f2e99cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a6d16261d5b4693b09acf83dcb38920": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0265a21ae504485ab59207228c1f6d6",
      "placeholder": "​",
      "style": "IPY_MODEL_8e13dccd922946ab873cc287fcca5baf",
      "value": " 232k/232k [00:00&lt;00:00, 9.51MB/s]"
     }
    },
    "a0265a21ae504485ab59207228c1f6d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a115ed03534a4c64a17fa55d93a0f93a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a74bacc051494d1ea0f4cbd656505cfe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4e7e77911e5408b84d89ccaff134708": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67ea6145091b440da6b4c5d5f8695aa9",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bef37b47c8ee49778464c05adcffa30d",
      "value": 466062
     }
    },
    "b83493088cd24629a04ec612aa522ed4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c222d2a5e4664f9b9aefed7f093d4a4e",
       "IPY_MODEL_fa94f90b735f418fb2b502f7877b5306",
       "IPY_MODEL_6ac495a816994aaeb8926c6c97d103ae"
      ],
      "layout": "IPY_MODEL_409a73804fc34a63a619a42b9468be46"
     }
    },
    "bef37b47c8ee49778464c05adcffa30d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c222d2a5e4664f9b9aefed7f093d4a4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_364a4257f8a641e18b7bde39e63a618d",
      "placeholder": "​",
      "style": "IPY_MODEL_93fa55d130db463c8ddffc947f2e99cd",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "c322e99ad9504207b7cfb9ae2ca9d6b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d40c6e034fd4b278969dd928cc07dff",
      "placeholder": "​",
      "style": "IPY_MODEL_f6b3c67d61114db5810bd78339a218d9",
      "value": " 466k/466k [00:00&lt;00:00, 1.88MB/s]"
     }
    },
    "ca901348dcd1428ebecc55d659fa40d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cf42b39e3be94c90b3a63d05783481a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d21ed2e527464a0ca0cb2b3b4fad928a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d568d9b894694b6fb432456031cee230": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbb672669b9f4c36873048cfa1d2daf6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd92db5892be4357a6446146d9e9a0dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2454ec2f5904e0bacde56ae7d4089a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20d6b378a69744e4a327d929c391b475",
      "placeholder": "​",
      "style": "IPY_MODEL_05c4f9fa65704ea5ba7f80e879e08510",
      "value": " 28.0/28.0 [00:00&lt;00:00, 1.97kB/s]"
     }
    },
    "e9ee9acc5e414c65ad10b8864cc07b07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f19c187b1e8c4bbe9fda366f24f2b79e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5b48b8b79d6a4ff1b0655a6bfcf7a422",
       "IPY_MODEL_650f8e0f82e849f58a47507291b15500",
       "IPY_MODEL_9a6d16261d5b4693b09acf83dcb38920"
      ],
      "layout": "IPY_MODEL_d568d9b894694b6fb432456031cee230"
     }
    },
    "f6b3c67d61114db5810bd78339a218d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa94f90b735f418fb2b502f7877b5306": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a74bacc051494d1ea0f4cbd656505cfe",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0fbfe002ccd541f9b6ab62ed4eebef35",
      "value": 570
     }
    },
    "fc871ed94e764fe884ffdafca1445bfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a5458551f974a94b61ff658ae59fa70",
      "placeholder": "​",
      "style": "IPY_MODEL_e9ee9acc5e414c65ad10b8864cc07b07",
      "value": "Downloading (…)/main/tokenizer.json: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
