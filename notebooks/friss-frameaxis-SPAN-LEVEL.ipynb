{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../data/')\n",
    "\n",
    "labels_path = \"data/en/dev-labels-subtask-3.txt\"\n",
    "articles_path = \"data/en/dev-articles-subtask-3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>persuasion_technique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>813452859</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>813452859</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>813452859</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>813452859</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>813452859</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  sentence_id persuasion_technique\n",
       "0   813452859            1                  NaN\n",
       "1   813452859            3                  NaN\n",
       "2   813452859            4                  NaN\n",
       "3   813452859            5                  NaN\n",
       "4   813452859            6                  NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dev-labels-subtask-2.txt file\n",
    "labels_df = pd.read_csv(labels_path, sep='\\t', header=None, names=[\"article_id\", \"sentence_id\", \"persuasion_technique\"])\n",
    "\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>persuasion_technique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>813452859</td>\n",
       "      <td>7</td>\n",
       "      <td>Michael Swadling: I guess her only chance is i...</td>\n",
       "      <td>False_Dilemma-No_Choice,Loaded_Language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>813452859</td>\n",
       "      <td>9</td>\n",
       "      <td>There is a chance; as unfortunately there are ...</td>\n",
       "      <td>False_Dilemma-No_Choice,Loaded_Language,Name_C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>813452859</td>\n",
       "      <td>11</td>\n",
       "      <td>Michael Swadling: The EU withdrawal act is in ...</td>\n",
       "      <td>Conversation_Killer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>813452859</td>\n",
       "      <td>12</td>\n",
       "      <td>I often use the example of an iPhone to people...</td>\n",
       "      <td>Conversation_Killer,Red_Herring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>813452859</td>\n",
       "      <td>15</td>\n",
       "      <td>Michael Swadling: The EU makes a profit on its...</td>\n",
       "      <td>Obfuscation-Vagueness-Confusion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  sentence_id                                           sentence  \\\n",
       "0   813452859            7  Michael Swadling: I guess her only chance is i...   \n",
       "1   813452859            9  There is a chance; as unfortunately there are ...   \n",
       "2   813452859           11  Michael Swadling: The EU withdrawal act is in ...   \n",
       "3   813452859           12  I often use the example of an iPhone to people...   \n",
       "4   813452859           15  Michael Swadling: The EU makes a profit on its...   \n",
       "\n",
       "                                persuasion_technique  \n",
       "0            False_Dilemma-No_Choice,Loaded_Language  \n",
       "1  False_Dilemma-No_Choice,Loaded_Language,Name_C...  \n",
       "2                                Conversation_Killer  \n",
       "3                    Conversation_Killer,Red_Herring  \n",
       "4                    Obfuscation-Vagueness-Confusion  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique article IDs from the dev-labels data\n",
    "unique_article_ids = labels_df['article_id'].unique()\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# For each unique article ID, read the corresponding article file and join with the dev-labels data\n",
    "for article_id in unique_article_ids:\n",
    "    # Construct the file path for the article\n",
    "    file_path = f\"{articles_path}/article{article_id}.txt\"\n",
    "    \n",
    "    try:\n",
    "        # Load the article file\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Read the article sentences into a list\n",
    "            sentences = file.readlines()\n",
    "\n",
    "        # Filter dev-labels data for the current article_id and where persuasion_technique is not NaN\n",
    "        relevant_rows = labels_df[(labels_df['article_id'] == article_id) & (~labels_df['persuasion_technique'].isna())]\n",
    "\n",
    "        # For each relevant row, get the corresponding sentence and persuasion technique and append to the results list\n",
    "        for _, row in relevant_rows.iterrows():\n",
    "            sentence = sentences[row['sentence_id'] - 1].strip()  # Subtracting 1 because list indexing starts from 0\n",
    "            technique = row['persuasion_technique']\n",
    "            results.append([article_id, row['sentence_id'], sentence, technique])\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        # If the file for an article_id doesn't exist, continue to the next one\n",
    "        continue\n",
    "\n",
    "# Convert the results list to a dataframe\n",
    "df = pd.DataFrame(results, columns=['article_id', 'sentence_id', 'sentence', 'persuasion_technique'])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>persuasion_technique</th>\n",
       "      <th>persuasion_technique_list</th>\n",
       "      <th>False_Dilemma-No_Choice</th>\n",
       "      <th>Loaded_Language</th>\n",
       "      <th>Name_Calling-Labeling</th>\n",
       "      <th>Conversation_Killer</th>\n",
       "      <th>Red_Herring</th>\n",
       "      <th>...</th>\n",
       "      <th>Flag_Waving</th>\n",
       "      <th>Doubt</th>\n",
       "      <th>Whataboutism</th>\n",
       "      <th>Appeal_to_Fear-Prejudice</th>\n",
       "      <th>Causal_Oversimplification</th>\n",
       "      <th>Appeal_to_Hypocrisy</th>\n",
       "      <th>Appeal_to_Popularity</th>\n",
       "      <th>Appeal_to_Authority</th>\n",
       "      <th>Straw_Man</th>\n",
       "      <th>Guilt_by_Association</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>813452859</td>\n",
       "      <td>7</td>\n",
       "      <td>Michael Swadling: I guess her only chance is i...</td>\n",
       "      <td>False_Dilemma-No_Choice,Loaded_Language</td>\n",
       "      <td>[False_Dilemma-No_Choice, Loaded_Language]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>813452859</td>\n",
       "      <td>9</td>\n",
       "      <td>There is a chance; as unfortunately there are ...</td>\n",
       "      <td>False_Dilemma-No_Choice,Loaded_Language,Name_C...</td>\n",
       "      <td>[False_Dilemma-No_Choice, Loaded_Language, Nam...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>813452859</td>\n",
       "      <td>11</td>\n",
       "      <td>Michael Swadling: The EU withdrawal act is in ...</td>\n",
       "      <td>Conversation_Killer</td>\n",
       "      <td>[Conversation_Killer]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>813452859</td>\n",
       "      <td>12</td>\n",
       "      <td>I often use the example of an iPhone to people...</td>\n",
       "      <td>Conversation_Killer,Red_Herring</td>\n",
       "      <td>[Conversation_Killer, Red_Herring]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>813452859</td>\n",
       "      <td>15</td>\n",
       "      <td>Michael Swadling: The EU makes a profit on its...</td>\n",
       "      <td>Obfuscation-Vagueness-Confusion</td>\n",
       "      <td>[Obfuscation-Vagueness-Confusion]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  sentence_id                                           sentence  \\\n",
       "0   813452859            7  Michael Swadling: I guess her only chance is i...   \n",
       "1   813452859            9  There is a chance; as unfortunately there are ...   \n",
       "2   813452859           11  Michael Swadling: The EU withdrawal act is in ...   \n",
       "3   813452859           12  I often use the example of an iPhone to people...   \n",
       "4   813452859           15  Michael Swadling: The EU makes a profit on its...   \n",
       "\n",
       "                                persuasion_technique  \\\n",
       "0            False_Dilemma-No_Choice,Loaded_Language   \n",
       "1  False_Dilemma-No_Choice,Loaded_Language,Name_C...   \n",
       "2                                Conversation_Killer   \n",
       "3                    Conversation_Killer,Red_Herring   \n",
       "4                    Obfuscation-Vagueness-Confusion   \n",
       "\n",
       "                           persuasion_technique_list  False_Dilemma-No_Choice  \\\n",
       "0         [False_Dilemma-No_Choice, Loaded_Language]                        1   \n",
       "1  [False_Dilemma-No_Choice, Loaded_Language, Nam...                        1   \n",
       "2                              [Conversation_Killer]                        0   \n",
       "3                 [Conversation_Killer, Red_Herring]                        0   \n",
       "4                  [Obfuscation-Vagueness-Confusion]                        0   \n",
       "\n",
       "   Loaded_Language  Name_Calling-Labeling  Conversation_Killer  Red_Herring  \\\n",
       "0                1                      0                    0            0   \n",
       "1                1                      1                    0            0   \n",
       "2                0                      0                    1            0   \n",
       "3                0                      0                    1            1   \n",
       "4                0                      0                    0            0   \n",
       "\n",
       "   ...  Flag_Waving  Doubt  Whataboutism  Appeal_to_Fear-Prejudice  \\\n",
       "0  ...            0      0             0                         0   \n",
       "1  ...            0      0             0                         0   \n",
       "2  ...            0      0             0                         0   \n",
       "3  ...            0      0             0                         0   \n",
       "4  ...            0      0             0                         0   \n",
       "\n",
       "   Causal_Oversimplification  Appeal_to_Hypocrisy  Appeal_to_Popularity  \\\n",
       "0                          0                    0                     0   \n",
       "1                          0                    0                     0   \n",
       "2                          0                    0                     0   \n",
       "3                          0                    0                     0   \n",
       "4                          0                    0                     0   \n",
       "\n",
       "   Appeal_to_Authority  Straw_Man  Guilt_by_Association  \n",
       "0                    0          0                     0  \n",
       "1                    0          0                     0  \n",
       "2                    0          0                     0  \n",
       "3                    0          0                     0  \n",
       "4                    0          0                     0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the frames column into a list of frames\n",
    "df[\"persuasion_technique_list\"] = df[\"persuasion_technique\"].str.split(\",\")\n",
    "\n",
    "# create for each frame a new column with the frame as name and 1 if the frame is present in the article and 0 if not\n",
    "for frame in df[\"persuasion_technique_list\"].explode().unique():\n",
    "    df[frame] = df[\"persuasion_technique_list\"].apply(lambda x: 1 if frame in x else 0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"sentence\"]\n",
    "y = df.drop(columns=[\"article_id\", \"sentence_id\", \"sentence\", \"persuasion_technique\", \"persuasion_technique_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Michael Swadling: I guess her only chance is i...\n",
       "1    There is a chance; as unfortunately there are ...\n",
       "2    Michael Swadling: The EU withdrawal act is in ...\n",
       "3    I often use the example of an iPhone to people...\n",
       "4    Michael Swadling: The EU makes a profit on its...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>False_Dilemma-No_Choice</th>\n",
       "      <th>Loaded_Language</th>\n",
       "      <th>Name_Calling-Labeling</th>\n",
       "      <th>Conversation_Killer</th>\n",
       "      <th>Red_Herring</th>\n",
       "      <th>Obfuscation-Vagueness-Confusion</th>\n",
       "      <th>Exaggeration-Minimisation</th>\n",
       "      <th>Repetition</th>\n",
       "      <th>Slogans</th>\n",
       "      <th>Flag_Waving</th>\n",
       "      <th>Doubt</th>\n",
       "      <th>Whataboutism</th>\n",
       "      <th>Appeal_to_Fear-Prejudice</th>\n",
       "      <th>Causal_Oversimplification</th>\n",
       "      <th>Appeal_to_Hypocrisy</th>\n",
       "      <th>Appeal_to_Popularity</th>\n",
       "      <th>Appeal_to_Authority</th>\n",
       "      <th>Straw_Man</th>\n",
       "      <th>Guilt_by_Association</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   False_Dilemma-No_Choice  Loaded_Language  Name_Calling-Labeling  \\\n",
       "0                        1                1                      0   \n",
       "1                        1                1                      1   \n",
       "2                        0                0                      0   \n",
       "3                        0                0                      0   \n",
       "4                        0                0                      0   \n",
       "\n",
       "   Conversation_Killer  Red_Herring  Obfuscation-Vagueness-Confusion  \\\n",
       "0                    0            0                                0   \n",
       "1                    0            0                                0   \n",
       "2                    1            0                                0   \n",
       "3                    1            1                                0   \n",
       "4                    0            0                                1   \n",
       "\n",
       "   Exaggeration-Minimisation  Repetition  Slogans  Flag_Waving  Doubt  \\\n",
       "0                          0           0        0            0      0   \n",
       "1                          0           0        0            0      0   \n",
       "2                          0           0        0            0      0   \n",
       "3                          0           0        0            0      0   \n",
       "4                          0           0        0            0      0   \n",
       "\n",
       "   Whataboutism  Appeal_to_Fear-Prejudice  Causal_Oversimplification  \\\n",
       "0             0                         0                          0   \n",
       "1             0                         0                          0   \n",
       "2             0                         0                          0   \n",
       "3             0                         0                          0   \n",
       "4             0                         0                          0   \n",
       "\n",
       "   Appeal_to_Hypocrisy  Appeal_to_Popularity  Appeal_to_Authority  Straw_Man  \\\n",
       "0                    0                     0                    0          0   \n",
       "1                    0                     0                    0          0   \n",
       "2                    0                     0                    0          0   \n",
       "3                    0                     0                    0          0   \n",
       "4                    0                     0                    0          0   \n",
       "\n",
       "   Guilt_by_Association  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1120, 1120)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\elias\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] Die angegebene Prozedur wurde nicht gefunden\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\elias\\AppData\\Local\\Temp\\tmpt9i9kftq\\config.json as plain json\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "from allennlp_models.structured_prediction.models import srl_bert\n",
    "\n",
    "# Load the SRL predictor\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def extract_srl_components(article, predictor):\n",
    "    \"\"\"\n",
    "    Extract SRL components for an article.\n",
    "    \"\"\"\n",
    "    srl = predictor.predict(sentence=article)\n",
    "    \n",
    "    extracted_data = []\n",
    "    for verb_entry in srl['verbs']:\n",
    "        predicate = verb_entry['verb']\n",
    "        tags = verb_entry['tags']\n",
    "        \n",
    "        arg0_indices = [i for i, tag in enumerate(tags) if tag in ['B-ARG0', 'I-ARG0']]\n",
    "        arg1_indices = [i for i, tag in enumerate(tags) if tag in ['B-ARG1', 'I-ARG1']]\n",
    "        \n",
    "        arg0 = [srl['words'][i] for i in arg0_indices] if arg0_indices else []\n",
    "        arg1 = [srl['words'][i] for i in arg1_indices] if arg1_indices else []\n",
    "        \n",
    "        extracted_data.append({\n",
    "            'predicate': [predicate],\n",
    "            'ARG0': arg0,\n",
    "            'ARG1': arg1\n",
    "        })\n",
    "        \n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'predicate': ['turned'], 'ARG0': [], 'ARG1': ['The', 'red', 'horse']},\n",
       " {'predicate': ['fought'],\n",
       "  'ARG0': ['The', 'red', 'horse'],\n",
       "  'ARG1': ['the', 'fly']}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_srl_components(\"The red horse simply turned around and fought off the fly with its tail.\", predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_srl = X.apply(lambda x: extract_srl_components(x, predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [{'predicate': ['guess'], 'ARG0': ['I'], 'ARG1...\n",
       "1       [{'predicate': ['is'], 'ARG0': [], 'ARG1': ['a...\n",
       "2       [{'predicate': ['is'], 'ARG0': [], 'ARG1': ['T...\n",
       "3       [{'predicate': ['use'], 'ARG0': ['I'], 'ARG1':...\n",
       "4       [{'predicate': ['makes'], 'ARG0': ['The', 'EU'...\n",
       "                              ...                        \n",
       "1115    [{'predicate': ['do'], 'ARG0': [], 'ARG1': []}...\n",
       "1116    [{'predicate': ['are'], 'ARG0': [], 'ARG1': ['...\n",
       "1117    [{'predicate': ['added'], 'ARG0': ['Trump', 'J...\n",
       "1118    [{'predicate': ['seen'], 'ARG0': [], 'ARG1': [...\n",
       "1119    [{'predicate': ['came'], 'ARG0': [], 'ARG1': [...\n",
       "Name: sentence, Length: 1120, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_srl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_word_embedding(word):\n",
    "    \"\"\"\n",
    "    Get the BERT embedding for a given word.\n",
    "    \n",
    "    Args:\n",
    "    - word (str): The input word.\n",
    "    \n",
    "    Returns:\n",
    "    - torch.Tensor: The BERT embedding for the word.\n",
    "    \"\"\"\n",
    "    # Tokenize the word and get the corresponding IDs\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    # Convert token IDs to a torch tensor and add batch dimension\n",
    "    token_tensor = torch.tensor([token_ids])\n",
    "    \n",
    "    # Forward pass through the BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(token_tensor)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "    \n",
    "    # If the word was split into multiple tokens, average their embeddings\n",
    "    embedding = embeddings.mean(dim=1)\n",
    "    \n",
    "    return embedding.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_articles = len(X_srl)\n",
    "embedding_dim = 768\n",
    "\n",
    "max_srls = 10\n",
    "max_words = 10\n",
    "\n",
    "# Pre-allocate tensors\n",
    "predicates_tensor = torch.zeros((max_articles, max_srls, embedding_dim))\n",
    "arg0_tensor = torch.zeros((max_articles, max_srls, max_words, embedding_dim))\n",
    "arg1_tensor = torch.zeros((max_articles, max_srls, max_words, embedding_dim))\n",
    "\n",
    "for i, article_srls in enumerate(X_srl[:100]):\n",
    "    for j, srl_dict in enumerate(article_srls[:max_srls]):\n",
    "        # Handle predicate\n",
    "        predicates_tensor[i, j] = get_word_embedding(srl_dict['predicate'][0])\n",
    "        \n",
    "        # Handle ARG0\n",
    "        if srl_dict['ARG0']:\n",
    "            arg0_embeddings = torch.stack([get_word_embedding(word) for word in srl_dict['ARG0'][:max_words]])\n",
    "            arg0_tensor[i, j, :arg0_embeddings.shape[0]] = arg0_embeddings\n",
    "        \n",
    "        # Handle ARG1\n",
    "        if srl_dict['ARG1']:\n",
    "            arg1_embeddings = torch.stack([get_word_embedding(word) for word in srl_dict['ARG1'][:max_words]])\n",
    "            arg1_tensor[i, j, :arg1_embeddings.shape[0]] = arg1_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1120, 10, 768]),\n",
       " torch.Size([1120, 10, 10, 768]),\n",
       " torch.Size([1120, 10, 10, 768]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicates_tensor.shape, arg0_tensor.shape, arg1_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SRLAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=384, out_features=192, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=192, out_features=384, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=384, out_features=768, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the Autoencoder for SRL embeddings\n",
    "class SRLAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(SRLAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(encoding_dim, encoding_dim//2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim//2, encoding_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(encoding_dim, input_dim),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the SRL autoencoder\n",
    "input_dim = 768  # Embedding dimension\n",
    "encoding_dim = 384  # Reduced dimension after encoding\n",
    "srl_autoencoder = SRLAutoencoder(input_dim, encoding_dim)\n",
    "\n",
    "srl_autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointClassifierModel(nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_labels=19, hidden_dropout_prob=0.1, encoding_dim=384):\n",
    "        super(JointClassifierModel, self).__init__()\n",
    "        \n",
    "        # RoBERTa Model for sentence embeddings (assuming it will be loaded separately)\n",
    "        self.roberta = None\n",
    "        \n",
    "        # SRL Autoencoder\n",
    "        self.srl_autoencoder = SRLAutoencoder(input_dim=hidden_size, encoding_dim=encoding_dim)\n",
    "        \n",
    "        print(hidden_size, encoding_dim)\n",
    "        # Classifier Head\n",
    "        self.classifier_head_dim = hidden_size + encoding_dim*4  # Adding encoding_dim for predicate, arg0, and arg1\n",
    "        self.classifier = nn.Linear(self.classifier_head_dim, num_labels)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "        \n",
    "        # Softmax layer (if you're using it for multi-class classification)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        # Loss for multi-label classification\n",
    "        self.loss_fct = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, predicates_tensor, arg0_tensor, arg1_tensor, labels=None):\n",
    "        # Assuming the RoBERTa model is loaded and available\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        sentence_embeddings = outputs[0][:, 0, :]  # Extracting the CLS token embeddings\n",
    "        \n",
    "        # Averaging SRL embeddings over the word dimension\n",
    "        avg_predicates = predicates_tensor.mean(dim=1)\n",
    "        avg_arg0 = arg0_tensor.mean(dim=2)\n",
    "        avg_arg1 = arg1_tensor.mean(dim=2)\n",
    "        \n",
    "        # Pass SRL embeddings through the autoencoder to get encoded representations\n",
    "        encoded_predicates = self.srl_autoencoder(avg_predicates)\n",
    "        encoded_arg0 = self.srl_autoencoder(avg_arg0)\n",
    "        encoded_arg1 = self.srl_autoencoder(avg_arg1)\n",
    "        \n",
    "        encoded_arg0 = encoded_arg0.mean(dim=1)\n",
    "        encoded_arg1 = encoded_arg1.mean(dim=1)\n",
    "\n",
    "        # Concatenate encoded SRL representations with sentence embeddings\n",
    "        joint_representation = self.dropout(torch.cat([sentence_embeddings, encoded_predicates, encoded_arg0, encoded_arg1], dim=1))\n",
    "        \n",
    "        # Pass through classifier\n",
    "        logits = self.classifier(joint_representation)\n",
    "        \n",
    "        # Compute Loss\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(logits, labels.float())\n",
    "            return loss, logits\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from transformers import RobertaModel\n",
    "\n",
    "def train_joint_model(train_dataloader, validation_dataloader=None, epochs=3, learning_rate=3e-4):\n",
    "    # Initializing the model\n",
    "    model = JointClassifierModel(hidden_size=768, num_labels=len(y), hidden_dropout_prob=0.1)\n",
    "    model.roberta = RobertaModel.from_pretrained('roberta-base')  # Load the RoBERTa model\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            # Assuming batch is a tuple (input_ids, attention_mask, predicates_tensor, arg0_tensor, arg1_tensor, labels)\n",
    "            input_ids, attention_mask, predicates_tensor, arg0_tensor, arg1_tensor, labels = batch\n",
    "            input_ids, attention_mask, predicates_tensor, arg0_tensor, arg1_tensor, labels = \\\n",
    "                input_ids.to(device), attention_mask.to(device), predicates_tensor.to(device), \\\n",
    "                arg0_tensor.to(device), arg1_tensor.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            loss, logits = model(input_ids, attention_mask, predicates_tensor, arg0_tensor, arg1_tensor, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {total_loss/len(train_dataloader)}\")\n",
    "        \n",
    "        # Validation loop (optional)\n",
    "        if validation_dataloader:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch in validation_dataloader:\n",
    "                    input_ids, attention_mask, predicates_tensor, arg0_tensor, arg1_tensor, labels = batch\n",
    "                    input_ids, attention_mask, predicates_tensor, arg0_tensor, arg1_tensor, labels = \\\n",
    "                        input_ids.to(device), attention_mask.to(device), predicates_tensor.to(device), \\\n",
    "                        arg0_tensor.to(device), arg1_tensor.to(device), labels.to(device)\n",
    "                    \n",
    "                    loss, _ = model(input_ids, attention_mask, predicates_tensor, arg0_tensor, arg1_tensor, labels)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss/len(validation_dataloader)}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "class PersuasionTechniqueDataset(Dataset):\n",
    "    def __init__(self, sentences, predicates, arg0, arg1, labels, tokenizer):\n",
    "        self.sentences = sentences.tolist() if isinstance(sentences, pd.Series) else sentences\n",
    "        self.predicates = predicates\n",
    "        self.arg0 = arg0\n",
    "        self.arg1 = arg1\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the sentence\n",
    "        encoded_sentence = self.tokenizer.encode_plus(\n",
    "            self.sentences[idx], add_special_tokens=True, max_length=256,\n",
    "            padding='max_length', return_tensors='pt', truncation=True\n",
    "        )\n",
    "        input_ids = encoded_sentence['input_ids'].squeeze()\n",
    "        attention_mask = encoded_sentence['attention_mask'].squeeze()\n",
    "\n",
    "        # Get SRL tensors\n",
    "        predicate_tensor = self.predicates[idx]\n",
    "        arg0_tensor = self.arg0[idx]\n",
    "        arg1_tensor = self.arg1[idx]\n",
    "\n",
    "        # Get labels\n",
    "        label = torch.tensor(self.labels.iloc[idx])\n",
    "\n",
    "        return input_ids, attention_mask, predicate_tensor, arg0_tensor, arg1_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've loaded or defined your data in variables: sentences, predicates_tensor, arg0_tensor, arg1_tensor, and labels\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "dataset = PersuasionTechniqueDataset(X, predicates_tensor, arg0_tensor, arg1_tensor, y, tokenizer)\n",
    "train_loader = DataLoader(dataset, shuffle=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 384\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\elias\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\modeling_utils.py\", line 461, in load_state_dict\n",
      "    return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\serialization.py\", line 815, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\serialization.py\", line 1043, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\serialization.py\", line 975, in persistent_load\n",
      "    obj = cast(Storage, torch.UntypedStorage(nbytes))\n",
      "RuntimeError: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 9437184 bytes.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\elias\\AppData\\Local\\Temp\\ipykernel_11440\\1934149134.py\", line 2, in <module>\n",
      "    trained_model = train_joint_model(train_loader, epochs=3, learning_rate=3e-4)\n",
      "  File \"C:\\Users\\elias\\AppData\\Local\\Temp\\ipykernel_11440\\2126430760.py\", line 8, in train_joint_model\n",
      "    model.roberta = RobertaModel.from_pretrained('roberta-base')  # Load the RoBERTa model\n",
      "  File \"C:\\Users\\elias\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\modeling_utils.py\", line 2132, in from_pretrained\n",
      "    state_dict = load_state_dict(resolved_archive_file)\n",
      "  File \"C:\\Users\\elias\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\modeling_utils.py\", line 465, in load_state_dict\n",
      "    if f.read().startswith(\"version\"):\n",
      "MemoryError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 969, in format_record\n",
      "    _format_traceback_lines(\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 215, in _format_traceback_lines\n",
      "    line = stack_line.render(pygmented=has_colors).rstrip('\\n') + '\\n'\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\stack_data\\core.py\", line 360, in render\n",
      "    start_line, lines = self.frame_info._pygmented_scope_lines\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\stack_data\\core.py\", line 780, in _pygmented_scope_lines\n",
      "    lines = _pygmented_with_ranges(formatter, code, ranges)\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\stack_data\\utils.py\", line 165, in _pygmented_with_ranges\n",
      "    return pygments.highlight(code, lexer, formatter).splitlines()\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\pygments\\__init__.py\", line 82, in highlight\n",
      "    return format(lex(code, lexer), formatter, outfile)\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\pygments\\__init__.py\", line 64, in format\n",
      "    formatter.format(tokens, realoutfile)\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\pygments\\formatters\\terminal256.py\", line 250, in format\n",
      "    return Formatter.format(self, tokensource, outfile)\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\pygments\\formatter.py\", line 124, in format\n",
      "    return self.format_unencoded(tokensource, outfile)\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\pygments\\formatters\\terminal256.py\", line 256, in format_unencoded\n",
      "    for ttype, value in tokensource:\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\stack_data\\utils.py\", line 158, in get_tokens\n",
      "    for ttype, value in super().get_tokens(text):\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\pygments\\lexer.py\", line 250, in streamer\n",
      "    for _, t, v in self.get_tokens_unprocessed(text):\n",
      "  File \"c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\pygments\\lexer.py\", line 693, in get_tokens_unprocessed\n",
      "    m = rexmatch(text, pos)\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "# 3. Call the Training Function\n",
    "trained_model = train_joint_model(train_loader, epochs=3, learning_rate=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
