{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (4.33.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (4.3.2)\n",
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/ba/7c/b971f2485155917ecdcebb210e021e36a6b65457394590be01cc61515310/tensorflow-2.13.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached tensorflow-2.13.0-cp310-cp310-win_amd64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (0.17.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from gensim) (6.4.0)\n",
      "Collecting tensorflow-intel==2.13.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-intel==2.13.0 from https://files.pythonhosted.org/packages/40/fa/98115f6fe4d92e1962f549917be2dc8e369853b7e404191996fedaaf4dd6/tensorflow_intel-2.13.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached tensorflow_intel-2.13.0-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/e2/c4/6f8dae1530d57a6122fd5b72c750187484acbe612f630cb2179e4bcb12c1/h5py-3.9.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached h5py-3.9.0-cp310-cp310-win_amd64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.6)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.24.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.15.0)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/dd/c5/22351665cd11c84dc404386ed18177702891d311d8e6421ab8a2665c4a58/grpcio-1.58.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached grpcio-1.58.0-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for keras<2.14,>=2.13.1 from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata\n",
      "  Using cached keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.38.4)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9d/44/5a992cb9d7bf8aaae73bc5adaf721ad08731c9d00c1c17999a8691404b0c/google_auth-2.23.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_auth-2.23.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata\n",
      "  Using cached Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.3.7)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
      "Using cached tensorflow-2.13.0-cp310-cp310-win_amd64.whl (1.9 kB)\n",
      "Using cached tensorflow_intel-2.13.0-cp310-cp310-win_amd64.whl (276.5 MB)\n",
      "Using cached grpcio-1.58.0-cp310-cp310-win_amd64.whl (4.3 MB)\n",
      "Using cached h5py-3.9.0-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Using cached google_auth-2.23.0-py2.py3-none-any.whl (181 kB)\n",
      "Using cached Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Installing collected packages: rsa, pyasn1-modules, opt-einsum, markdown, keras, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 gast-0.4.0 google-auth-2.23.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.58.0 h5py-3.9.0 keras-2.13.1 markdown-3.4.4 opt-einsum-3.3.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-intel-2.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers pandas numpy matplotlib scikit-learn nltk gensim tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-19 19:35 matplotlib.pyplot DEBUG    Loaded backend module://matplotlib_inline.backend_inline version unknown.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# reload packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from semaxis import CoreUtil\n",
    "from semaxis import SemAxis\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',\n",
    "                    datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-19 19:35 urllib3.connectionpool DEBUG    Resetting dropped connection: huggingface.co\n",
      "09-19 19:35 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bert model\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"../models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-19 19:35 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sa = SemAxis(model, axes_str=CoreUtil.load_wordnet_antonyms_axes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04913395,  0.0227672 , -0.05149904,  0.02035379, -0.07861039,\n",
       "        0.08900583, -0.09128648,  0.00758144,  0.03420597, -0.07418875],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.compute_document_mean_with_tf(\n",
    "    [\"I like to eat apples and oranges.\"], min_freq=1\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# get list of file path from data\\data\\en\\dev-articles-subtask-2\\*\n",
    "articles = os.listdir(f\"../data/data/en/dev-articles-subtask-2\")\n",
    "\n",
    "for article in articles:\n",
    "    with open(\n",
    "        f\"../data/data/en/dev-articles-subtask-2/{article}\", \"r\", encoding=\"utf-8\"\n",
    "    ) as f:\n",
    "        obj = []\n",
    "\n",
    "        article_id = article.split(\".\")[0].replace(\"article\", \"\")\n",
    "\n",
    "        obj.append(article_id)\n",
    "\n",
    "        # read line 3 to n\n",
    "        lines = f.readlines()[2:]\n",
    "        obj.append(\"\".join(lines))\n",
    "\n",
    "        data.append(obj)\n",
    "\n",
    "# create pandas dataframe\n",
    "df_articles = pd.DataFrame(data, columns=[\"article_id\", \"article\"])\n",
    "\n",
    "# article_id to int\n",
    "df_articles[\"article_id\"] = df_articles[\"article_id\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "COLUMNS = [str(c) for c in sorted(sa.axes.keys()) if len(c) == 2]\n",
    "\n",
    "with open(\"big_table_by_average.tsv\", \"w\") as fo_a:\n",
    "    fo_a.write(\"{}\\t{}\\n\".format(\"article_id\", \"\\t\".join(COLUMNS)))\n",
    "\n",
    "    for index, row in df_articles[0:2].iterrows():\n",
    "\n",
    "        if index % 100 == 0:\n",
    "            print(index)\n",
    "\n",
    "        article_id = row[\"article_id\"]\n",
    "        text = row[\"article\"]\n",
    "        mean = sa.compute_document_mean_with_tf([text], min_freq=1)[0]\n",
    "        fo_a.write(\"{}\\t{}\\n\".format(article_id, \"\\t\".join([str(v) for v in mean])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping\n",
    "\n",
    "Cannot bootstrap as I need a opposite type of data. So e.g. analyze positive and see how the negative looks like. But I have no info about the type of data. So I cannot bootstrap.\n",
    "\n",
    "> Skip for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-19 19:39 __main__     INFO     average\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "mode = \"average\"\n",
    "logger.info(mode)\n",
    "try:\n",
    "    os.mkdir(mode)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapping(df, mode, COLUMNS):\n",
    "    for aspect in df['aspect'].unique():\n",
    "        for sentiment in ['positive', 'negative']:\n",
    "            df_r_count = df.query('aspect == @aspect and sentiment == @sentiment').shape[0]\n",
    "            with open(\"{}/bootstrap_{}_{}.tsv\".format(mode, aspect.replace(\"/\", \"-\"), sentiment), \"w\") as fo:\n",
    "                fo.write(\"{}\\n\".format(\"\\t\".join(COLUMNS)))\n",
    "#                 A = df.query('aspect == @aspect').drop(columns=[c for c in df.columns if '(' not in c]).values\n",
    "                A = df.drop(columns=[c for c in df.columns if '(' not in c]).values\n",
    "\n",
    "                for i in range(N):\n",
    "                    if i % 500 == 0:\n",
    "                        logger.info(\"{}-{}-{}\".format(aspect, sentiment, i))\n",
    "                    fmean = np.mean(A[np.random.choice(A.shape[0], df_r_count, replace=False), :], axis=0)\n",
    "                    fo.write(\"{}\\n\".format(\"\\t\".join([str(v) for v in (fmean)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"big_table_by_{}.tsv\".format(mode), sep=\"\\t\").dropna()\n",
    "COLUMNS = [c for c in df.columns if '(' in c]\n",
    "len(COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'aspect'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'aspect'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\elias\\OneDrive\\Dokumente\\Git\\MasterThesis\\frameaxis\\frameaxis.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/elias/OneDrive/Dokumente/Git/MasterThesis/frameaxis/frameaxis.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m bootstrapping(df, mode, COLUMNS)\n",
      "\u001b[1;32mc:\\Users\\elias\\OneDrive\\Dokumente\\Git\\MasterThesis\\frameaxis\\frameaxis.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/elias/OneDrive/Dokumente/Git/MasterThesis/frameaxis/frameaxis.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbootstrapping\u001b[39m(df, mode, COLUMNS):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/elias/OneDrive/Dokumente/Git/MasterThesis/frameaxis/frameaxis.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mfor\u001b[39;00m aspect \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39;49m\u001b[39maspect\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39munique():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/elias/OneDrive/Dokumente/Git/MasterThesis/frameaxis/frameaxis.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39mfor\u001b[39;00m sentiment \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mpositive\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnegative\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/elias/OneDrive/Dokumente/Git/MasterThesis/frameaxis/frameaxis.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m             df_r_count \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mquery(\u001b[39m'\u001b[39m\u001b[39maspect == @aspect and sentiment == @sentiment\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\core\\frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3896\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3897\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3898\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\elias\\anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'aspect'"
     ]
    }
   ],
   "source": [
    "bootstrapping(df, mode, COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
