{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Waumhn_ldoGu"
   },
   "source": [
    "## Classifier - Try 4\n",
    "\n",
    "Classify articles frames using aggregated SRL and sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21919,
     "status": "ok",
     "timestamp": 1696623996213,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "LM7e9jI4doGx",
    "outputId": "602ef0fa-e5fd-4626-be99-3fdef780fd4c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "  os.chdir('drive/MyDrive/Git/MasterThesis/data')\n",
    "else:\n",
    "  os.chdir('../../data/')\n",
    "\n",
    "labels_path = \"data/en/train-labels-subtask-2.txt\"\n",
    "articles_path = \"data/en/train-articles-subtask-2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1296
    },
    "executionInfo": {
     "elapsed": 3772,
     "status": "ok",
     "timestamp": 1696624002536,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "DG_Xix7gdoGy",
    "outputId": "d6fad26e-e6f7-4c20-f4bb-c7b01d51eb33",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>frames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>832959523</td>\n",
       "      <td>Morality,Security_and_defense,Policy_prescript...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>833039623</td>\n",
       "      <td>Political,Crime_and_punishment,External_regula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>833032367</td>\n",
       "      <td>Political,Crime_and_punishment,Fairness_and_eq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>814777937</td>\n",
       "      <td>Political,Morality,Fairness_and_equality,Exter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>821744708</td>\n",
       "      <td>Policy_prescription_and_evaluation,Political,L...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                             frames\n",
       "0   832959523  Morality,Security_and_defense,Policy_prescript...\n",
       "1   833039623  Political,Crime_and_punishment,External_regula...\n",
       "2   833032367  Political,Crime_and_punishment,Fairness_and_eq...\n",
       "3   814777937  Political,Morality,Fairness_and_equality,Exter...\n",
       "4   821744708  Policy_prescription_and_evaluation,Political,L..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dev-labels-subtask-2.txt file\n",
    "labels_df = pd.read_csv(labels_path, sep=\"\\t\")\n",
    "\n",
    "# Rename the columns for easier processing\n",
    "labels_df.columns = [\"article_id\", \"frames\"]\n",
    "\n",
    "\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 359378,
     "status": "ok",
     "timestamp": 1696624361911,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "wYeJBUyAdoGz",
    "outputId": "7bd17fad-1fda-4be9-8c66-a85f51e5f517",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>frames</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>832959523</td>\n",
       "      <td>Morality,Security_and_defense,Policy_prescript...</td>\n",
       "      <td>How Theresa May Botched\\n\\nThose were the time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>833039623</td>\n",
       "      <td>Political,Crime_and_punishment,External_regula...</td>\n",
       "      <td>Robert Mueller III Rests His Case—Dems NEVER W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>833032367</td>\n",
       "      <td>Political,Crime_and_punishment,Fairness_and_eq...</td>\n",
       "      <td>Robert Mueller Not Recommending Any More Indic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>814777937</td>\n",
       "      <td>Political,Morality,Fairness_and_equality,Exter...</td>\n",
       "      <td>The Far Right Is Trying to Co-opt the Yellow V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>821744708</td>\n",
       "      <td>Policy_prescription_and_evaluation,Political,L...</td>\n",
       "      <td>‘Special place in hell’ for those who promoted...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                             frames  \\\n",
       "0   832959523  Morality,Security_and_defense,Policy_prescript...   \n",
       "1   833039623  Political,Crime_and_punishment,External_regula...   \n",
       "2   833032367  Political,Crime_and_punishment,Fairness_and_eq...   \n",
       "3   814777937  Political,Morality,Fairness_and_equality,Exter...   \n",
       "4   821744708  Policy_prescription_and_evaluation,Political,L...   \n",
       "\n",
       "                                             content  \n",
       "0  How Theresa May Botched\\n\\nThose were the time...  \n",
       "1  Robert Mueller III Rests His Case—Dems NEVER W...  \n",
       "2  Robert Mueller Not Recommending Any More Indic...  \n",
       "3  The Far Right Is Trying to Co-opt the Yellow V...  \n",
       "4  ‘Special place in hell’ for those who promoted...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A function to read the article text given its ID\n",
    "def get_article_content(article_id):\n",
    "    try:\n",
    "        with open(f\"{articles_path}/article{article_id}.txt\", \"r\") as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "df = labels_df\n",
    "\n",
    "# Apply the function to get the article content\n",
    "df[\"content\"] = df[\"article_id\"].apply(get_article_content)\n",
    "\n",
    "# Drop rows where content could not be found\n",
    "df.dropna(subset=[\"content\"], inplace=True)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1696624361912,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "fU66l4twdoGz",
    "outputId": "48f06d3d-e677-4d16-dadb-aae312d25342",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>frames</th>\n",
       "      <th>content</th>\n",
       "      <th>frames_list</th>\n",
       "      <th>Morality</th>\n",
       "      <th>Security_and_defense</th>\n",
       "      <th>Policy_prescription_and_evaluation</th>\n",
       "      <th>Legality_Constitutionality_and_jurisprudence</th>\n",
       "      <th>Economic</th>\n",
       "      <th>Political</th>\n",
       "      <th>Crime_and_punishment</th>\n",
       "      <th>External_regulation_and_reputation</th>\n",
       "      <th>Public_opinion</th>\n",
       "      <th>Fairness_and_equality</th>\n",
       "      <th>Capacity_and_resources</th>\n",
       "      <th>Quality_of_life</th>\n",
       "      <th>Cultural_identity</th>\n",
       "      <th>Health_and_safety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>832959523</td>\n",
       "      <td>Morality,Security_and_defense,Policy_prescript...</td>\n",
       "      <td>How Theresa May Botched\\n\\nThose were the time...</td>\n",
       "      <td>[Morality, Security_and_defense, Policy_prescr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>833039623</td>\n",
       "      <td>Political,Crime_and_punishment,External_regula...</td>\n",
       "      <td>Robert Mueller III Rests His Case—Dems NEVER W...</td>\n",
       "      <td>[Political, Crime_and_punishment, External_reg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>833032367</td>\n",
       "      <td>Political,Crime_and_punishment,Fairness_and_eq...</td>\n",
       "      <td>Robert Mueller Not Recommending Any More Indic...</td>\n",
       "      <td>[Political, Crime_and_punishment, Fairness_and...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>814777937</td>\n",
       "      <td>Political,Morality,Fairness_and_equality,Exter...</td>\n",
       "      <td>The Far Right Is Trying to Co-opt the Yellow V...</td>\n",
       "      <td>[Political, Morality, Fairness_and_equality, E...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>821744708</td>\n",
       "      <td>Policy_prescription_and_evaluation,Political,L...</td>\n",
       "      <td>‘Special place in hell’ for those who promoted...</td>\n",
       "      <td>[Policy_prescription_and_evaluation, Political...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                             frames  \\\n",
       "0   832959523  Morality,Security_and_defense,Policy_prescript...   \n",
       "1   833039623  Political,Crime_and_punishment,External_regula...   \n",
       "2   833032367  Political,Crime_and_punishment,Fairness_and_eq...   \n",
       "3   814777937  Political,Morality,Fairness_and_equality,Exter...   \n",
       "4   821744708  Policy_prescription_and_evaluation,Political,L...   \n",
       "\n",
       "                                             content  \\\n",
       "0  How Theresa May Botched\\n\\nThose were the time...   \n",
       "1  Robert Mueller III Rests His Case—Dems NEVER W...   \n",
       "2  Robert Mueller Not Recommending Any More Indic...   \n",
       "3  The Far Right Is Trying to Co-opt the Yellow V...   \n",
       "4  ‘Special place in hell’ for those who promoted...   \n",
       "\n",
       "                                         frames_list  Morality  \\\n",
       "0  [Morality, Security_and_defense, Policy_prescr...         1   \n",
       "1  [Political, Crime_and_punishment, External_reg...         0   \n",
       "2  [Political, Crime_and_punishment, Fairness_and...         0   \n",
       "3  [Political, Morality, Fairness_and_equality, E...         1   \n",
       "4  [Policy_prescription_and_evaluation, Political...         0   \n",
       "\n",
       "   Security_and_defense  Policy_prescription_and_evaluation  \\\n",
       "0                     1                                   1   \n",
       "1                     0                                   1   \n",
       "2                     0                                   0   \n",
       "3                     1                                   0   \n",
       "4                     0                                   1   \n",
       "\n",
       "   Legality_Constitutionality_and_jurisprudence  Economic  Political  \\\n",
       "0                                             1         1          0   \n",
       "1                                             1         0          1   \n",
       "2                                             1         0          1   \n",
       "3                                             0         1          1   \n",
       "4                                             1         0          1   \n",
       "\n",
       "   Crime_and_punishment  External_regulation_and_reputation  Public_opinion  \\\n",
       "0                     0                                   0               0   \n",
       "1                     1                                   1               1   \n",
       "2                     1                                   1               0   \n",
       "3                     0                                   1               1   \n",
       "4                     0                                   1               0   \n",
       "\n",
       "   Fairness_and_equality  Capacity_and_resources  Quality_of_life  \\\n",
       "0                      0                       0                0   \n",
       "1                      0                       0                0   \n",
       "2                      1                       0                0   \n",
       "3                      1                       0                0   \n",
       "4                      0                       0                0   \n",
       "\n",
       "   Cultural_identity  Health_and_safety  \n",
       "0                  0                  0  \n",
       "1                  0                  0  \n",
       "2                  0                  0  \n",
       "3                  0                  0  \n",
       "4                  0                  0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the frames column into a list of frames\n",
    "df[\"frames_list\"] = df[\"frames\"].str.split(\",\")\n",
    "\n",
    "# create for each frame a new column with the frame as name and 1 if the frame is present in the article and 0 if not\n",
    "for frame in df[\"frames_list\"].explode().unique():\n",
    "    df[frame] = df[\"frames_list\"].apply(lambda x: 1 if frame in x else 0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1696624361912,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "FdAZUIg6doG0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df[\"content\"]\n",
    "y = df.drop(columns=[\"article_id\", \"frames\", \"frames_list\", \"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1696624361912,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "HlL31uTzdoG0",
    "outputId": "e732ffad-597e-42c6-f687-62693490fa18",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    How Theresa May Botched\\n\\nThose were the time...\n",
       "1    Robert Mueller III Rests His Case—Dems NEVER W...\n",
       "2    Robert Mueller Not Recommending Any More Indic...\n",
       "3    The Far Right Is Trying to Co-opt the Yellow V...\n",
       "4    ‘Special place in hell’ for those who promoted...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1696624361912,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "cDBgYDTidoG0",
    "outputId": "9c0094db-e682-4e25-bb33-76005f2e7ed5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Morality</th>\n",
       "      <th>Security_and_defense</th>\n",
       "      <th>Policy_prescription_and_evaluation</th>\n",
       "      <th>Legality_Constitutionality_and_jurisprudence</th>\n",
       "      <th>Economic</th>\n",
       "      <th>Political</th>\n",
       "      <th>Crime_and_punishment</th>\n",
       "      <th>External_regulation_and_reputation</th>\n",
       "      <th>Public_opinion</th>\n",
       "      <th>Fairness_and_equality</th>\n",
       "      <th>Capacity_and_resources</th>\n",
       "      <th>Quality_of_life</th>\n",
       "      <th>Cultural_identity</th>\n",
       "      <th>Health_and_safety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Morality  Security_and_defense  Policy_prescription_and_evaluation  \\\n",
       "0         1                     1                                   1   \n",
       "1         0                     0                                   1   \n",
       "2         0                     0                                   0   \n",
       "3         1                     1                                   0   \n",
       "4         0                     0                                   1   \n",
       "\n",
       "   Legality_Constitutionality_and_jurisprudence  Economic  Political  \\\n",
       "0                                             1         1          0   \n",
       "1                                             1         0          1   \n",
       "2                                             1         0          1   \n",
       "3                                             0         1          1   \n",
       "4                                             1         0          1   \n",
       "\n",
       "   Crime_and_punishment  External_regulation_and_reputation  Public_opinion  \\\n",
       "0                     0                                   0               0   \n",
       "1                     1                                   1               1   \n",
       "2                     1                                   1               0   \n",
       "3                     0                                   1               1   \n",
       "4                     0                                   1               0   \n",
       "\n",
       "   Fairness_and_equality  Capacity_and_resources  Quality_of_life  \\\n",
       "0                      0                       0                0   \n",
       "1                      0                       0                0   \n",
       "2                      1                       0                0   \n",
       "3                      1                       0                0   \n",
       "4                      0                       0                0   \n",
       "\n",
       "   Cultural_identity  Health_and_safety  \n",
       "0                  0                  0  \n",
       "1                  0                  0  \n",
       "2                  0                  0  \n",
       "3                  0                  0  \n",
       "4                  0                  0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1696624371765,
     "user": {
      "displayName": "Elias Anderlohr",
      "userId": "15301978580987406749"
     },
     "user_tz": -120
    },
    "id": "QhcR4ySddoG1",
    "outputId": "1ab79474-cc3b-490e-da77-ae0877c58266",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(432, 432)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.to_csv(\"../notebooks/classifier/y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='transformers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract SRL Embeddings from articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycuda\n",
      "  Downloading pycuda-2022.2.2.tar.gz (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting appdirs>=1.4.0\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting pytools>=2011.2\n",
      "  Downloading pytools-2023.1.1-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from pytools>=2011.2->pycuda) (2.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.9/dist-packages (from pytools>=2011.2->pycuda) (4.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from mako->pycuda) (2.1.2)\n",
      "Building wheels for collected packages: pycuda\n",
      "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycuda: filename=pycuda-2022.2.2-cp39-cp39-linux_x86_64.whl size=661947 sha256=5e7ce158e0a2518dbd61b9e71cb83a4769535288051ed0ddcfa82acd3cc6de07\n",
      "  Stored in directory: /root/.cache/pip/wheels/c8/24/b2/c54ef2a7a3dcb9daa9e403c036360b7e47a8200c94413d0c77\n",
      "Successfully built pycuda\n",
      "Installing collected packages: appdirs, pytools, mako, pycuda\n",
      "Successfully installed appdirs-1.4.4 mako-1.2.4 pycuda-2022.2.2 pytools-2023.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting allennlp\n",
      "  Downloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.2/730.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting allennlp-models\n",
      "  Downloading allennlp_models-2.10.1-py3-none-any.whl (464 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.5/464.5 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.9/dist-packages (from allennlp) (2.28.2)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (3.19.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.12.0)\n",
      "Collecting filelock<3.8,>=3.3\n",
      "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Collecting spacy<3.4,>=2.1.0\n",
      "  Downloading spacy-3.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting more-itertools>=8.12.0\n",
      "  Downloading more_itertools-10.1.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers<4.21,>=4.1\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.9/dist-packages (from allennlp) (4.64.1)\n",
      "Collecting base58>=2.1.1\n",
      "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.9.2)\n",
      "Requirement already satisfied: sentencepiece>=0.1.96 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.1.97)\n",
      "Collecting wandb<0.13.0,>=0.10.0\n",
      "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cached-path<1.2.0,>=1.1.3\n",
      "  Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.9/dist-packages (from allennlp) (7.2.1)\n",
      "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.4.2)\n",
      "Collecting tensorboardX>=1.2\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision<0.14.0,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.13.1+cu116)\n",
      "Requirement already satisfied: torch<1.13.0,>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.12.1+cu116)\n",
      "Requirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.9/dist-packages (from allennlp) (3.7)\n",
      "Collecting termcolor==1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fairscale==0.4.6\n",
      "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (5.8.1)\n",
      "Requirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from allennlp) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.23.4)\n",
      "Collecting lmdb>=1.2.1\n",
      "  Downloading lmdb-1.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (298 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.5/298.5 kB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from allennlp) (1.1.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.9/dist-packages (from allennlp) (0.3.5.1)\n",
      "Collecting jsonnet>=0.10.0\n",
      "  Downloading jsonnet-0.20.0.tar.gz (594 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (from allennlp-models) (2.4.0)\n",
      "Collecting word2number>=1.1\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py-rouge==1.1\n",
      "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting conllu==4.4.2\n",
      "  Downloading conllu-4.4.2-py2.py3-none-any.whl (15 kB)\n",
      "Collecting rich<13.0,>=12.1\n",
      "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-storage<3.0,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.12.0-py2.py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.5/120.5 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: boto3<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (1.24.90)\n",
      "Collecting huggingface-hub>=0.0.16\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.0.16->allennlp) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.0.16->allennlp) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.0.16->allennlp) (4.4.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk>=3.6.5->allennlp) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk>=3.6.5->allennlp) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk>=3.6.5->allennlp) (1.2.0)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (2.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (1.1.0)\n",
      "Collecting attrs>=19.2.0\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (2.0.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.9/dist-packages (from pytest>=6.2.5->allennlp) (1.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.28->allennlp) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.28->allennlp) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.28->allennlp) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.28->allennlp) (1.26.14)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.1->allennlp) (3.1.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.12)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (6.3.0)\n",
      "Collecting thinc<8.1.0,>=8.0.14\n",
      "  Downloading thinc-8.0.17-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (668 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m668.8/668.8 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.9)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp39-cp39-manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (66.1.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.1)\n",
      "Collecting protobuf<4.0.0,>=3.12.0\n",
      "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (9.2.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<4.21,>=4.1->allennlp) (0.12.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.9.4)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.14.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (3.1.30)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.3.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.4.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.14.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.0.11)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (3.8.3)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (0.70.13)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (1.5.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (10.0.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (2023.1.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (3.2.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets->allennlp-models) (0.18.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy->allennlp-models) (0.2.6)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.90 in /usr/local/lib/python3.9/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.27.90)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->allennlp-models) (1.8.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (4.0.10)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.12.0-py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-resumable-media>=2.6.0\n",
      "  Downloading google_resumable_media-2.6.0-py2.py3-none-any.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3.0dev,>=2.23.3\n",
      "  Downloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.14.0)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->allennlp-models) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->allennlp-models) (2.8.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (5.0.0)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.9/230.9 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.7.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.4.8)\n",
      "Building wheels for collected packages: fairscale, termcolor, jsonnet, word2number, sacremoses\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307220 sha256=0f9343a161591676a152a8b4e1b712ff7d881e5d40831a8e523cbcb8b84c000e\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/7b/f9/6c7a350821ca240450550801e17996aee846b71caaccda7a32\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4833 sha256=76d6d48c9d3868b5c6dd40e74a57af7113e222550d59ec2c42c32ba9017489e6\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/00/51/e04e70a050b271a6aac779726204a324ada2b39b99334175c3\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jsonnet: filename=jsonnet-0.20.0-cp39-cp39-linux_x86_64.whl size=6619627 sha256=ddbe5f018d92d38599fe1fab96815c403bc7e7a5f7128114931f53096d765063\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/1a/fe/4d7df1823604150f77a6877f88fc6236d7c56d92a4d15e8b8c\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5566 sha256=68176f149787bf3d6f56807afcc1078f85cb3737e9fd7961a7d08d38220732d7\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/1d/b2/768b65901d249c6eb2a8d9c30392506555a4e94055ba4e0aa0\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895242 sha256=a5f29cde89c4f8f0bd192510fdfd93eb5f3bb8e69fcf0a5494331e372c38afe9\n",
      "  Stored in directory: /root/.cache/pip/wheels/42/79/78/5ad3b042cb2d97c294535162cdbaf9b167e3b186eae55ab72d\n",
      "Successfully built fairscale termcolor jsonnet word2number sacremoses\n",
      "Installing collected packages: word2number, termcolor, py-rouge, lmdb, jsonnet, commonmark, sacremoses, rich, pydantic, protobuf, more-itertools, google-crc32c, ftfy, filelock, conllu, base58, attrs, thinc, tensorboardX, huggingface-hub, googleapis-common-protos, google-resumable-media, google-auth, fairscale, wandb, transformers, spacy, google-api-core, google-cloud-core, google-cloud-storage, cached-path, allennlp, allennlp-models\n",
      "  Attempting uninstall: termcolor\n",
      "    Found existing installation: termcolor 2.2.0\n",
      "    Uninstalling termcolor-2.2.0:\n",
      "      Successfully uninstalled termcolor-2.2.0\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 13.2.0\n",
      "    Uninstalling rich-13.2.0:\n",
      "      Successfully uninstalled rich-13.2.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.9.2\n",
      "    Uninstalling pydantic-1.9.2:\n",
      "      Successfully uninstalled pydantic-1.9.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.9.0\n",
      "    Uninstalling filelock-3.9.0:\n",
      "      Successfully uninstalled filelock-3.9.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 18.2.0\n",
      "    Uninstalling attrs-18.2.0:\n",
      "      Successfully uninstalled attrs-18.2.0\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.1.7\n",
      "    Uninstalling thinc-8.1.7:\n",
      "      Successfully uninstalled thinc-8.1.7\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.12.0\n",
      "    Uninstalling huggingface-hub-0.12.0:\n",
      "      Successfully uninstalled huggingface-hub-0.12.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.16.0\n",
      "    Uninstalling google-auth-2.16.0:\n",
      "      Successfully uninstalled google-auth-2.16.0\n",
      "  Attempting uninstall: wandb\n",
      "    Found existing installation: wandb 0.13.4\n",
      "    Uninstalling wandb-0.13.4:\n",
      "      Successfully uninstalled wandb-0.13.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.21.3\n",
      "    Uninstalling transformers-4.21.3:\n",
      "      Successfully uninstalled transformers-4.21.3\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.4.1\n",
      "    Uninstalling spacy-3.4.1:\n",
      "      Successfully uninstalled spacy-3.4.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed allennlp-2.10.1 allennlp-models-2.10.1 attrs-23.1.0 base58-2.1.1 cached-path-1.1.6 commonmark-0.9.1 conllu-4.4.2 fairscale-0.4.6 filelock-3.7.1 ftfy-6.1.1 google-api-core-2.12.0 google-auth-2.23.3 google-cloud-core-2.3.3 google-cloud-storage-2.12.0 google-crc32c-1.5.0 google-resumable-media-2.6.0 googleapis-common-protos-1.61.0 huggingface-hub-0.10.1 jsonnet-0.20.0 lmdb-1.4.1 more-itertools-10.1.0 protobuf-3.20.3 py-rouge-1.1 pydantic-1.8.2 rich-12.6.0 sacremoses-0.0.53 spacy-3.3.3 tensorboardX-2.6.2.2 termcolor-1.1.0 thinc-8.0.17 transformers-4.20.1 wandb-0.12.21 word2number-1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pycuda\n",
    "!pip install allennlp allennlp-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 device(s) found.\n",
      "0 NVIDIA RTX A4000\n"
     ]
    }
   ],
   "source": [
    "import pycuda\n",
    "from pycuda import compiler\n",
    "import pycuda.driver as drv\n",
    "\n",
    "drv.init()\n",
    "print(\"%d device(s) found.\" % drv.Device.count())\n",
    "           \n",
    "for ordinal in range(drv.Device.count()):\n",
    "    dev = drv.Device(ordinal)\n",
    "    print (ordinal, dev.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "from allennlp_models.structured_prediction.models import srl_bert\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_extract_srl_components(sentences, predictor):\n",
    "    # Prepare the batched input for the predictor\n",
    "    batched_input = [{'sentence': sentence} for sentence in sentences]\n",
    "    batched_srl = predictor.predict_batch_json(batched_input)\n",
    "    \n",
    "    # Extract SRL components from the batched predictions\n",
    "    results = []\n",
    "    for srl in batched_srl:\n",
    "        best_extracted_data = None\n",
    "        second_best_extracted_data = None\n",
    "        for verb_entry in srl['verbs']:\n",
    "            tags = verb_entry['tags']\n",
    "            arg0_indices = [i for i, tag in enumerate(tags) if tag in ['B-ARG0', 'I-ARG0']]\n",
    "            arg1_indices = [i for i, tag in enumerate(tags) if tag in ['B-ARG1', 'I-ARG1']]\n",
    "\n",
    "            if arg0_indices and arg1_indices:\n",
    "                best_extracted_data = {\n",
    "                    'predicate': verb_entry['verb'],\n",
    "                    'ARG0': ' '.join([srl['words'][i] for i in arg0_indices]),\n",
    "                    'ARG1': ' '.join([srl['words'][i] for i in arg1_indices])\n",
    "                }\n",
    "                break\n",
    "            elif (arg0_indices or arg1_indices) and not second_best_extracted_data:\n",
    "                second_best_extracted_data = {\n",
    "                    'predicate': verb_entry['verb'],\n",
    "                    'ARG0': ' '.join([srl['words'][i] for i in arg0_indices]) if arg0_indices else '',\n",
    "                    'ARG1': ' '.join([srl['words'][i] for i in arg1_indices]) if arg1_indices else ''\n",
    "                }\n",
    "\n",
    "        if best_extracted_data:\n",
    "            results.append(best_extracted_data)\n",
    "        elif second_best_extracted_data:\n",
    "            results.append(second_best_extracted_data)\n",
    "            \n",
    "    return results\n",
    "\n",
    "def optimized_extract_srl(X, predictor, batch_size=32):\n",
    "    total_articles = len(X)\n",
    "    processed_articles = 0\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for article in X:\n",
    "        sentences = sent_tokenize(article)\n",
    "        article_srls = []\n",
    "\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batched_sentences = sentences[i:i+batch_size]\n",
    "            article_srls.extend(batched_extract_srl_components(batched_sentences, predictor))\n",
    "\n",
    "        all_results.append(article_srls)\n",
    "        processed_articles += 1\n",
    "        print(f\"Processed article {processed_articles}/{total_articles}\")\n",
    "\n",
    "    return pd.Series(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_X_srl(X, recalculate=False, pickle_path=\"../notebooks/classifier/X_srl.pkl\"):\n",
    "    \"\"\"\n",
    "    Returns the X_srl either by loading from a pickled file or recalculating.\n",
    "    \"\"\"\n",
    "    if recalculate or not os.path.exists(pickle_path):\n",
    "        print(\"Recalculate SRL\")\n",
    "        # Load predictor\n",
    "        predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\", cuda_device=0)\n",
    "        X_srl = optimized_extract_srl(X, predictor)\n",
    "        with open(pickle_path, 'wb') as f:\n",
    "            pickle.dump(X_srl, f)\n",
    "    else:\n",
    "        print(\"Load SRL from Pickle\")\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            X_srl = pickle.load(f)\n",
    "    return X_srl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "(16721772544, 16891248640)\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def free_gpu():\n",
    "    print(torch.cuda.mem_get_info())\n",
    "    print(torch.cuda.memory_summary())\n",
    "    \n",
    "free_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/distributed/distributed_c10d.py:181: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def list_gpu_tensors():\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                if obj.is_cuda:\n",
    "                    obj = obj.cpu()\n",
    "                    obj = obj.to(\"cpu\")\n",
    "                    print(type(obj), obj.size())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "list_gpu_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "class ArticleDataset(Dataset):\n",
    "    def __init__(self, X, X_srl, tokenizer, labels=None, max_sentences_per_article=32, max_sentence_length=32, max_arg_length=16):\n",
    "        self.X = X\n",
    "        self.X_srl = X_srl\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sentences_per_article = max_sentences_per_article\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.max_arg_length = max_arg_length\n",
    "        nltk.download('punkt')  # Download the Punkt tokenizer model for sentence splitting\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def _truncate_or_pad(self, lst, target_length, pad_value=0):\n",
    "        \"\"\"\n",
    "        Truncate or pad the input list to match the target length.\n",
    "        \"\"\"\n",
    "        if len(lst) > target_length:\n",
    "            return lst[:target_length]\n",
    "        else:\n",
    "            return lst + [pad_value] * (target_length - len(lst))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        article = self.X.iloc[idx]\n",
    "        srl = self.X_srl.iloc[idx]\n",
    "\n",
    "        # Split the article into sentences\n",
    "        sentences = nltk.sent_tokenize(article)\n",
    "        sentences = sentences[:self.max_sentences_per_article]  # Limit the number of sentences\n",
    "\n",
    "        # Tokenize and pad/truncate the sentences\n",
    "        sentence_ids = [self.tokenizer.encode(sentence, add_special_tokens=True, max_length=self.max_sentence_length, truncation=True, padding='max_length') for sentence in sentences]\n",
    "        while len(sentence_ids) < self.max_sentences_per_article:\n",
    "            sentence_ids.append([0] * self.max_sentence_length)\n",
    "\n",
    "        # Tokenize and pad/truncate the SRL items\n",
    "        predicate_ids = [self.tokenizer.encode(predicate, add_special_tokens=True, max_length=self.max_arg_length, truncation=True, padding='max_length') for predicate in [item['predicate'] for item in srl]]\n",
    "        arg0_ids = [self.tokenizer.encode(arg0, add_special_tokens=True, max_length=self.max_arg_length, truncation=True, padding='max_length') for arg0 in [item.get('arg0', '') for item in srl]]\n",
    "        arg1_ids = [self.tokenizer.encode(arg1, add_special_tokens=True, max_length=self.max_arg_length, truncation=True, padding='max_length') for arg1 in [item.get('arg1', '') for item in srl]]\n",
    "        \n",
    "        predicate_ids = predicate_ids[:self.max_sentences_per_article]\n",
    "        arg0_ids = arg0_ids[:self.max_sentences_per_article]\n",
    "        arg1_ids = arg1_ids[:self.max_sentences_per_article]  \n",
    "        \n",
    "        while len(predicate_ids) < self.max_sentences_per_article:\n",
    "            predicate_ids.append([0] * self.max_arg_length)\n",
    "        while len(arg0_ids) < self.max_sentences_per_article:\n",
    "            arg0_ids.append([0] * self.max_arg_length)\n",
    "        while len(arg1_ids) < self.max_sentences_per_article:\n",
    "            arg1_ids.append([0] * self.max_arg_length)\n",
    "\n",
    "        data = {\n",
    "            'sentence_ids': torch.tensor(sentence_ids, dtype=torch.long),\n",
    "            'predicate_ids': torch.tensor(predicate_ids, dtype=torch.long),\n",
    "            'arg0_ids': torch.tensor(arg0_ids, dtype=torch.long),\n",
    "            'arg1_ids': torch.tensor(arg1_ids, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            data['labels'] = self.labels.iloc[idx]\n",
    "        \n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863fccde3cba414aab4dd6bf3052ebf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9a8f96f09d415ebf6dc1b78b0b9d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee3eefdf39a48f0b10c5352c334a828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load SRL from Pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATION DONE\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Extract individual lists from the batch\n",
    "    sentence_ids = [item['sentence_ids'] for item in batch]\n",
    "    predicate_ids = [item['predicate_ids'] for item in batch]\n",
    "    arg0_ids = [item['arg0_ids'] for item in batch]\n",
    "    arg1_ids = [item['arg1_ids'] for item in batch]\n",
    "    \n",
    "    # Pad each list\n",
    "    sentence_ids = torch.nn.utils.rnn.pad_sequence(sentence_ids, batch_first=True, padding_value=0)\n",
    "    predicate_ids = torch.nn.utils.rnn.pad_sequence(predicate_ids, batch_first=True, padding_value=0)\n",
    "    arg0_ids = torch.nn.utils.rnn.pad_sequence(arg0_ids, batch_first=True, padding_value=0)\n",
    "    arg1_ids = torch.nn.utils.rnn.pad_sequence(arg1_ids, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Conditionally extract and add labels\n",
    "    output_dict = {\n",
    "        'sentence_ids': sentence_ids,\n",
    "        'predicate_ids': predicate_ids,\n",
    "        'arg0_ids': arg0_ids,\n",
    "        'arg1_ids': arg1_ids\n",
    "    }\n",
    "    \n",
    "    if 'labels' in batch[0]:\n",
    "        labels = [item['labels'] for item in batch]\n",
    "        output_dict['labels'] = torch.Tensor(labels)\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def get_datasets_dataloaders(X, y, tokenizer, recalculate_srl=False, pickle_path=\"../notebooks/classifier/X_srl.pkl\", batch_size=4):\n",
    "    # Get X_srl\n",
    "    X_srl = get_X_srl(X, recalculate=recalculate_srl, pickle_path=pickle_path)\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = ArticleDataset(X, X_srl, tokenizer, y)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    print(\"CREATION DONE\")\n",
    "    return train_dataset, test_dataset, train_dataloader, test_dataloader\n",
    "\n",
    "train_dataset, test_dataset, train_dataloader, test_dataloader = get_datasets_dataloaders(X, y, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_dataloader(article, tokenizer, batch_size=4):\n",
    "    X = pd.Series([article])\n",
    "    y = None  # No labels for this single article\n",
    "    \n",
    "    predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\", cuda_device=0)\n",
    "    # Directly use the optimized_extract_srl function since we don't need to cache for single articles\n",
    "    X_srl = optimized_extract_srl(X, predictor)\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = ArticleDataset(X, X_srl, tokenizer, y)\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e2b38250174bf496838e82629da5c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f090e92860b44438bc19378c960632ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383ad0b02ee14fae95662a8c4a2b61d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Spacy models 'en_core_web_sm' not found.  Downloading and installing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 92.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-sm==3.3.0) (3.3.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.10.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (23.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.23.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.28.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (66.1.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (6.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.14)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Processed article 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7ffa693c2e20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_article_dataloader(\"This is a test sentence\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14647689216, 16891248640)\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |     838 MB |     917 MB |     917 MB |\n",
      "|       from large pool |       0 B  |     836 MB |     836 MB |     836 MB |\n",
      "|       from small pool |       0 B  |       2 MB |      80 MB |      80 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |     838 MB |     917 MB |     917 MB |\n",
      "|       from large pool |       0 B  |     836 MB |     836 MB |     836 MB |\n",
      "|       from small pool |       0 B  |       2 MB |      80 MB |      80 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     924 MB |     924 MB |     924 MB |       0 B  |\n",
      "|       from large pool |     920 MB |     920 MB |     920 MB |       0 B  |\n",
      "|       from small pool |       4 MB |       4 MB |       4 MB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |  127364 KB |    1022 MB |    1022 MB |\n",
      "|       from large pool |       0 B  |  126976 KB |     940 MB |     940 MB |\n",
      "|       from small pool |       0 B  |    2124 KB |      82 MB |      82 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |     405    |     751    |     751    |\n",
      "|       from large pool |       0    |     150    |     150    |     150    |\n",
      "|       from small pool |       0    |     255    |     601    |     601    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |     405    |     751    |     751    |\n",
      "|       from large pool |       0    |     150    |     150    |     150    |\n",
      "|       from small pool |       0    |     255    |     601    |     601    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      41    |      41    |      41    |       0    |\n",
      "|       from large pool |      39    |      39    |      39    |       0    |\n",
      "|       from small pool |       2    |       2    |       2    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |      40    |     241    |     241    |\n",
      "|       from large pool |       0    |      38    |      74    |      74    |\n",
      "|       from small pool |       0    |       5    |     167    |     167    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "free_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, bert_model_name):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "    def forward(self, x, max_items):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # List to collect embeddings for each batch\n",
    "        batch_embeddings = []\n",
    "\n",
    "        # Loop through each item in the batch\n",
    "        for i in range(batch_size):\n",
    "            sequence_embeddings = []\n",
    "\n",
    "            # Loop through each sequence in the item and obtain embeddings\n",
    "            for j in range(max_items):\n",
    "                sequence = x[i][j].unsqueeze(0)  # Adding an extra dimension for BERT\n",
    "                embeddings = self.bert_model(sequence).last_hidden_state.squeeze(0)  # Removing the extra dimension after obtaining embeddings\n",
    "                sequence_embeddings.append(embeddings)\n",
    "            \n",
    "            # Stack embeddings for each sequence in the item\n",
    "            item_embeddings = torch.stack(sequence_embeddings)\n",
    "            batch_embeddings.append(item_embeddings)\n",
    "        \n",
    "        # Stack embeddings for each item in the batch\n",
    "        reshaped_embeddings = torch.stack(batch_embeddings)\n",
    "        \n",
    "        return reshaped_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceAttentionLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, heads=8):\n",
    "        super(SentenceAttentionLayer, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = embedding_dim // heads\n",
    "\n",
    "        # Q, K, V weight matrices\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embedding_dim)\n",
    "        \n",
    "    def forward(self, values, keys=None, queries=None):\n",
    "        if keys is None:\n",
    "            keys = values\n",
    "        if queries is None:\n",
    "            queries = values\n",
    "\n",
    "        N = queries.shape[0]  # batch size\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        # Split the embedding_dim into self.heads pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) \n",
    "        attention = attention / (self.embedding_dim ** (1/2))\n",
    "        attention = F.softmax(attention, dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.heads*self.head_dim)\n",
    "        \n",
    "        # Combine heads\n",
    "        out = self.fc_out(out)\n",
    "        \n",
    "        # Aggregate to single vector per instance\n",
    "        out = out.sum(dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregationLayer(nn.Module):\n",
    "    def __init__(self, max_sentences_per_article=32, max_sentence_length=32, max_srl_items=None, \n",
    "                 max_arg_length=16, bert_model_name=\"bert-base-uncased\"):\n",
    "        super(AggregationLayer, self).__init__()\n",
    "        self.embedding_layer = EmbeddingLayer(bert_model_name)\n",
    "        \n",
    "        embedding_dim = 768  # For bert-base-uncased\n",
    "        self.attention = SentenceAttentionLayer(embedding_dim=embedding_dim)\n",
    "        \n",
    "        # Store the values as attributes\n",
    "        self.max_sentences_per_article = max_sentences_per_article\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.max_srl_items = max_srl_items if max_srl_items is not None else max_sentences_per_article\n",
    "        self.max_arg_length = max_arg_length\n",
    "\n",
    "    def aggregate_word_embeddings(self, embeddings):\n",
    "        \"\"\"\n",
    "        Apply the attention mechanism to the word embeddings to get the sentence embeddings.\n",
    "        \"\"\"\n",
    "        N, num_sentences, num_words, embedding_dim = embeddings.shape\n",
    "        embeddings_reshaped = embeddings.reshape(N*num_sentences, num_words, embedding_dim)\n",
    "        \n",
    "        sentence_embeddings = self.attention(embeddings_reshaped)\n",
    "    \n",
    "        # Reshape back to [N, num_sentences, embedding_dim]\n",
    "        sentence_embeddings = sentence_embeddings.reshape(N, num_sentences, embedding_dim)\n",
    "\n",
    "        # Aggregate across the num_sentences dimension to get shape [N, 768]\n",
    "        aggregated_embeddings = sentence_embeddings.sum(dim=1) # TODO: Try mean\n",
    "        \n",
    "        return aggregated_embeddings\n",
    "    \n",
    "    def forward(self, sentence_ids, predicate_ids, arg0_ids, arg1_ids):\n",
    "        # Get embeddings\n",
    "        sentence_embeddings = self.embedding_layer(sentence_ids, self.max_sentences_per_article)\n",
    "        predicate_embeddings = self.embedding_layer(predicate_ids, self.max_srl_items)\n",
    "        arg0_embeddings = self.embedding_layer(arg0_ids, self.max_srl_items)\n",
    "        arg1_embeddings = self.embedding_layer(arg1_ids, self.max_srl_items)\n",
    "        \n",
    "        # Aggregate word embeddings into sentence embeddings\n",
    "        sentence_embeddings = self.aggregate_word_embeddings(sentence_embeddings)\n",
    "        predicate_embeddings = self.aggregate_word_embeddings(predicate_embeddings)\n",
    "        arg0_embeddings = self.aggregate_word_embeddings(arg0_embeddings)\n",
    "        arg1_embeddings = self.aggregate_word_embeddings(arg1_embeddings)\n",
    "        \n",
    "        # Get aggregated embeddings (if you want a second level of attention on the sentence embeddings)\n",
    "        #sentence_aggregated = self.attention(sentence_embeddings)\n",
    "        #predicate_aggregated = self.attention(predicate_embeddings)\n",
    "        #arg0_aggregated = self.attention(arg0_embeddings)\n",
    "        #arg1_aggregated = self.attention(arg1_embeddings)\n",
    "        \n",
    "        return sentence_embeddings, predicate_embeddings, arg0_embeddings, arg1_embeddings #sentence_aggregated, predicate_aggregated, arg0_aggregated, arg1_aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): FRISSEncoder(\n",
       "    (fc_shared): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    (fc_latent_p): Linear(in_features=768, out_features=15, bias=True)\n",
       "    (fc_latent_a0): Linear(in_features=768, out_features=15, bias=True)\n",
       "    (fc_latent_a1): Linear(in_features=768, out_features=15, bias=True)\n",
       "  )\n",
       "  (decoder): FRISSDecoder()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.distributions as dist\n",
    "\n",
    "class FRISSEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(FRISSEncoder, self).__init__()\n",
    "        # Shared feed-forward layer\n",
    "        self.fc_shared = nn.Linear(input_dim, hidden_dim)\n",
    "        # View-specific feed-forward layers\n",
    "        self.fc_latent_p = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_latent_a0 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_latent_a1 = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def gumbel_softmax(self, logits, temperature=1.0):\n",
    "        gumbel_noise = dist.Gumbel(0, 1).sample(logits.size()).to(logits.device)\n",
    "        y = logits + gumbel_noise\n",
    "        return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "    def forward(self, x, view):\n",
    "        x = F.relu(self.fc_shared(x))\n",
    "        if view == \"p\":\n",
    "            latent = self.fc_latent_p(x)\n",
    "        elif view == \"a0\":\n",
    "            latent = self.fc_latent_a0(x)\n",
    "        elif view == \"a1\":\n",
    "            latent = self.fc_latent_a1(x)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid view provided!\")\n",
    "        \n",
    "        # Apply Gumbel-Softmax on the latent representation\n",
    "        latent = self.gumbel_softmax(latent)\n",
    "        return latent\n",
    "\n",
    "class FRISSDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(FRISSDecoder, self).__init__()\n",
    "        # Using the dictionary terms (descriptors) to reconstruct\n",
    "        self.dictionary = nn.Parameter(torch.randn(latent_dim, output_dim))\n",
    "\n",
    "    def forward(self, latent):\n",
    "        # Here we're mimicking the linear combination using matrix multiplication.\n",
    "        # The Gumbel-Softmax sampled latent weights the dictionary terms for reconstruction.\n",
    "        reconstructed = torch.matmul(latent, self.dictionary)\n",
    "        return reconstructed\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, output_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = FRISSEncoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = FRISSDecoder(latent_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, view):\n",
    "        latent = self.encoder(x, view)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed\n",
    "\n",
    "# Define the model\n",
    "input_dim = 1536  # Size of the concatenated embeddings\n",
    "hidden_dim = 768  # An arbitrary size for the hidden layer\n",
    "latent_dim = 15   # Number of dictionary terms or frames\n",
    "output_dim = 768  # Size of the original SRL span embedding (e.g., for the predicate, ARG0, or ARG1)\n",
    "\n",
    "AutoEncoder(input_dim, hidden_dim, latent_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FRISS(nn.Module):\n",
    "    def __init__(self, input_dim=1536, hidden_dim=768, latent_dim=14, output_dim=768, num_classes=14, embedding_dim=768):\n",
    "        super(FRISS, self).__init__()\n",
    "        \n",
    "        self.aggregation_layer = AggregationLayer()\n",
    "        \n",
    "        # The unsupervised AutoEncoder part\n",
    "        self.autoencoder = AutoEncoder(input_dim, hidden_dim, latent_dim, output_dim)\n",
    "        \n",
    "        # Span-based Classifier\n",
    "        self.span_classifier = nn.Linear(latent_dim, num_classes)\n",
    "        \n",
    "        # Sentence-based Classifier\n",
    "        self.sentence_encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, sentence_ids, predicate_ids, arg0_ids, arg1_ids):\n",
    "        sentence_aggregated, predicate_aggregated, arg0_aggregated, arg1_aggregated = self.aggregation_layer(sentence_ids, predicate_ids, arg0_ids, arg1_ids)\n",
    "        \n",
    "        # Concatenate aggregated sentence embeddings with the SRL embeddings\n",
    "        x_p = torch.cat((sentence_aggregated, predicate_aggregated), 1)\n",
    "        x_a0 = torch.cat((sentence_aggregated, arg0_aggregated), 1)\n",
    "        x_a1 = torch.cat((sentence_aggregated, arg1_aggregated), 1)\n",
    "        \n",
    "        # Pass through AutoEncoder\n",
    "        latent_p = self.autoencoder.encoder(x_p, view=\"p\")\n",
    "        latent_a0 = self.autoencoder.encoder(x_a0, view=\"a0\")\n",
    "        latent_a1 = self.autoencoder.encoder(x_a1, view=\"a1\")\n",
    "        \n",
    "        # Span-based Classification\n",
    "        aggregated_latent = torch.mean(torch.stack([latent_p, latent_a0, latent_a1]), 0)\n",
    "        y_hat_span = self.span_classifier(aggregated_latent)\n",
    "        \n",
    "        # Sentence-based Classification\n",
    "        y_hat_sentence = self.sentence_encoder(sentence_aggregated)\n",
    "        \n",
    "        return y_hat_span, y_hat_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(15343484928, 17063280640)\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |     838 MB |     917 MB |     917 MB |\n",
      "|       from large pool |       0 B  |     836 MB |     836 MB |     836 MB |\n",
      "|       from small pool |       0 B  |       2 MB |      80 MB |      80 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |     838 MB |     917 MB |     917 MB |\n",
      "|       from large pool |       0 B  |     836 MB |     836 MB |     836 MB |\n",
      "|       from small pool |       0 B  |       2 MB |      80 MB |      80 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     924 MB |     924 MB |     924 MB |       0 B  |\n",
      "|       from large pool |     920 MB |     920 MB |     920 MB |       0 B  |\n",
      "|       from small pool |       4 MB |       4 MB |       4 MB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |  127364 KB |    1022 MB |    1022 MB |\n",
      "|       from large pool |       0 B  |  126976 KB |     940 MB |     940 MB |\n",
      "|       from small pool |       0 B  |    2124 KB |      82 MB |      82 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |     405    |     751    |     751    |\n",
      "|       from large pool |       0    |     150    |     150    |     150    |\n",
      "|       from small pool |       0    |     255    |     601    |     601    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |     405    |     751    |     751    |\n",
      "|       from large pool |       0    |     150    |     150    |     150    |\n",
      "|       from small pool |       0    |     255    |     601    |     601    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      41    |      41    |      41    |       0    |\n",
      "|       from large pool |      39    |      39    |      39    |       0    |\n",
      "|       from small pool |       2    |       2    |       2    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |      40    |     241    |     241    |\n",
      "|       from large pool |       0    |      38    |      74    |      74    |\n",
      "|       from small pool |       0    |       5    |     167    |     167    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'model' in locals():\n",
    "    model.to(\"cpu\")\n",
    "    model.cpu()\n",
    "    print()\n",
    "    free_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1-Score (micro-averaged) and Average Precision Score are chosen as primary metrics for evaluating the multi-label classification task due to the following reasons:\n",
    "\n",
    "1. **F1-Score (Micro)**:\n",
    "    - The micro-averaged F1-score computes global counts of true positives, false negatives, and false positives. \n",
    "    - It provides a balance between precision (the number of correct positive results divided by the number of all positive results) and recall (the number of correct positive results divided by the number of positive results that should have been returned).\n",
    "    - Given the imbalance in the label distribution observed in the dataset, the micro-averaged F1-score is robust against this imbalance, making it a suitable metric for optimization.\n",
    "\n",
    "2. **Average Precision Score**:\n",
    "    - This metric summarizes the precision-recall curve, giving a single value that represents the average of precision values at different recall levels.\n",
    "    - It's especially valuable when class imbalances exist, as it gives more weight to the positive class (the rarer class in an imbalanced dataset).\n",
    "\n",
    "Using these metrics will ensure that the model is optimized for a balanced performance across all labels, even if some labels are rarer than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, average_precision_score\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, optimizer, alpha=0.5, num_epochs=10, device='cuda', save_path='../notebooks/classifier/'):\n",
    "    loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Metrics dictionary\n",
    "    metrics = {\n",
    "        'f1': [],\n",
    "        'avg_precision': []\n",
    "    }\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_progress = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=\"Batches\", leave=False)\n",
    "\n",
    "        for batch_idx, batch in batch_progress:\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move data and labels to device\n",
    "            sentence_ids = batch['sentence_ids'].to(device)\n",
    "            predicate_ids = batch['predicate_ids'].to(device)\n",
    "            arg0_ids = batch['arg0_ids'].to(device)\n",
    "            arg1_ids = batch['arg1_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits_span, logits_sentence = model(sentence_ids, predicate_ids, arg0_ids, arg1_ids)\n",
    "\n",
    "            # Compute the supervised loss\n",
    "            loss_span = loss_function(logits_span, labels.float())\n",
    "            loss_sentence = loss_function(logits_sentence, labels.float())\n",
    "            supervised_loss = loss_span + loss_sentence\n",
    "\n",
    "            # Combine the supervised and unsupervised losses\n",
    "            total_loss = alpha * supervised_loss\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Clear GPU cache\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            batch_progress.set_description(f\"Epoch {epoch+1} Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {total_loss/len(train_dataloader)}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        all_preds_span = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                # Move data and labels to device\n",
    "                sentence_ids = batch['sentence_ids'].to(device)\n",
    "                predicate_ids = batch['predicate_ids'].to(device)\n",
    "                arg0_ids = batch['arg0_ids'].to(device)\n",
    "                arg1_ids = batch['arg1_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                logits_span, _ = model(sentence_ids, predicate_ids, arg0_ids, arg1_ids)\n",
    "                preds_span = (torch.sigmoid(logits_span) > 0.5).float()\n",
    "\n",
    "                all_preds_span.append(preds_span.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        all_preds_span = np.vstack(all_preds_span)\n",
    "        all_labels = np.vstack(all_labels)\n",
    "\n",
    "        # Compute metrics\n",
    "        f1 = f1_score(all_labels, all_preds_span, average='micro')\n",
    "        avg_precision = average_precision_score(all_labels, all_preds_span)\n",
    "\n",
    "        metrics['f1'].append(f1)\n",
    "        metrics['avg_precision'].append(avg_precision)\n",
    "\n",
    "        print(f\"Validation Metrics - F1 Score (Micro): {f1:.4f}, Average Precision: {avg_precision:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    model_save_path = os.path.join(save_path, 'model1.pth')\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    # Save metrics to JSON file\n",
    "    with open('../notebooks/classifier/metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c2f6d486dd4ca4abac509774e49627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Training Loss: 0.012187257409095764\n",
      "Validation Metrics - Accuracy: 0.0000, F1 Score: 0.4192, Precision: 0.4023, Recall: 0.4375, Average Precision: 0.2627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Training Loss: 0.00617617554962635\n",
      "Validation Metrics - Accuracy: 0.0000, F1 Score: 0.4509, Precision: 0.5019, Recall: 0.4094, Average Precision: 0.2627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Training Loss: 0.008196800947189331\n",
      "Validation Metrics - Accuracy: 0.0000, F1 Score: 0.3725, Precision: 0.5287, Recall: 0.2875, Average Precision: 0.2627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Training Loss: 0.005116731394082308\n",
      "Validation Metrics - Accuracy: 0.0000, F1 Score: 0.3725, Precision: 0.5287, Recall: 0.2875, Average Precision: 0.2627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Training Loss: 0.006629019044339657\n",
      "Validation Metrics - Accuracy: 0.0000, F1 Score: 0.3725, Precision: 0.5287, Recall: 0.2875, Average Precision: 0.2627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Training Loss: 0.004905941896140575\n",
      "Validation Metrics - Accuracy: 0.0000, F1 Score: 0.3725, Precision: 0.5287, Recall: 0.2875, Average Precision: 0.2627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Training Loss: 0.005613383837044239\n",
      "Validation Metrics - Accuracy: 0.0000, F1 Score: 0.3725, Precision: 0.5287, Recall: 0.2875, Average Precision: 0.2627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15, Training Loss: 0.005487698595970869\n",
      "Validation Metrics - Accuracy: 0.0000, F1 Score: 0.3725, Precision: 0.5287, Recall: 0.2875, Average Precision: 0.2627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15, Training Loss: 0.007822412066161633\n",
      "Validation Metrics - Accuracy: 0.0000, F1 Score: 0.3725, Precision: 0.5287, Recall: 0.2875, Average Precision: 0.2627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681a576c1c1f4cc48110922b2c966344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import json\n",
    "\n",
    "# Initialize the FRISS model\n",
    "model = FRISS()\n",
    "\n",
    "# Assuming you're using a GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function (criterion)\n",
    "# You can choose a suitable loss function. For multi-class classification, CrossEntropyLoss is commonly used.\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer\n",
    "# Using the Adam optimizer as it is widely used and works well for many tasks.\n",
    "# You can adjust the learning rate and other parameters based on your needs.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# now train\n",
    "\n",
    "# Define your parameters\n",
    "alpha_value = 0.5\n",
    "num_epochs_value = 15\n",
    "device_value = 'cuda'\n",
    "\n",
    "# Call the train function\n",
    "metrics = train(model, train_dataloader, test_dataloader, optimizer, alpha=alpha_value, num_epochs=num_epochs_value, device=device_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_path(model_class, path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads the weights into an instance of the model class from the given path.\n",
    "    \n",
    "    Args:\n",
    "    - model_class (torch.nn.Module): The class of the model (uninitialized).\n",
    "    - path (str): Path to the saved weights.\n",
    "    - device (str): Device to load the model on ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - model (torch.nn.Module): Model with weights loaded.\n",
    "    \"\"\"\n",
    "    model = model_class().to(device)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model_from_path(FRISS, '../notebooks/classifier/new_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, y_columns, device='cuda'):\n",
    "    \"\"\"\n",
    "    Make predictions with the given model and dataloader.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): The model to make predictions with.\n",
    "    - dataloader (DataLoader): DataLoader for the dataset to predict on.\n",
    "    - y_columns (pandas.Index): Column names from the y dataframe which correspond to labels.\n",
    "    - device (str): Device to make predictions on ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "    - predicted_labels (list of lists): List containing the predicted labels for each instance.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds_span = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move data to device\n",
    "            sentence_ids = batch['sentence_ids'].to(device)\n",
    "            predicate_ids = batch['predicate_ids'].to(device)\n",
    "            arg0_ids = batch['arg0_ids'].to(device)\n",
    "            arg1_ids = batch['arg1_ids'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits_span, _ = model(sentence_ids, predicate_ids, arg0_ids, arg1_ids)\n",
    "            preds_span = (torch.sigmoid(logits_span) > 0.5).float()\n",
    "\n",
    "            all_preds_span.append(preds_span.cpu().numpy())\n",
    "                \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    predictions = np.vstack(all_preds_span)\n",
    "    \n",
    "    # Convert boolean predictions to labels\n",
    "    predicted_labels = []\n",
    "    for pred in predictions:\n",
    "        labels = list(y_columns[pred.astype(bool)])\n",
    "        predicted_labels.append(labels)\n",
    "    \n",
    "    return predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# article813452859\n",
    "article = \"\"\"EU Profits From Trading With UK While London Loses Money – Political Campaigner\n",
    "\n",
    "With the Parliamentary vote on British Prime Minister Theresa May’s Brexit plan set to be held next month; President of the European Commission Jean Claude Juncker has criticised the UK’s preparations for their departure from the EU.\n",
    "But is there any chance that May's deal will make it through parliament and if it fails, how could this ongoing political deadlock finally come to an end?\n",
    "Sputnik spoke with political campaigner Michael Swadling for more…\n",
    "Sputnik: Does Theresa May have any chance of getting her deal through Parliament on the 14th January?\n",
    "Michael Swadling: I guess her only chance is if Labour decides that they want to dishonour democracy and effectively keep us in the EU.\n",
    "© AP Photo / Pablo Martinez Monsivais UK 'In Need of Leadership', May's Brexit Deal Unwelcome to Trump - US Ambassador\n",
    "There is a chance; as unfortunately there are many MPs who don't respect the vote and may just turn on it, but short of that I don't see any way the Conservatives would vote for it, and the majority is slender as it is, as the DUP is bitterly against it, and I can't see the Lib Dems voting for it, so it will only be if there are enough, what I can describe as remoaner MPs, that the deal won't be dead in the water.\n",
    "Sputnik: What could be a solution to the political chaos if the Prime Minister's deal is not approved?\n",
    "Michael Swadling: The EU withdrawal act is in place; we'll leave and revert to WTO terms and that works, that's fine.\n",
    "I often use the example of an iPhone to people; that's a piece of technology which is manufactured in China, uses American technology and these are two countries we deal with on WTO terms, this isn't a fantasy, stuck in a port somewhere, there isn't a massive tariff, this is the world that really exists today.\n",
    "When we exit the EU on WTO terms; that will be fine for whatever trading we do with the EU, just as well as it does for our trade in China.READ MORE: UK Finance Chief Bashed for Failing to Unlock Money for No-Deal Brexit — Reports\n",
    "Sputnik: Do you think that the EU needs the UK more than the UK needs the EU?\n",
    "Michael Swadling: The EU makes a profit on its trade with the UK; the UK makes a loss on its trade with the EU.\n",
    "They have a financial incentive to ensure that good trading relations continue far more than we do.\n",
    "© REUTERS / Toby Melville UK Trade Minister Says '50-50' Chance Brexit Will Not Happen – Reports\n",
    "The lifeblood and cash flow that keeps manufacturing in Europe going, comes from the city of London.\n",
    "If someone in a city in Germany wants to do a deal with someone in Japan; the financial services of that are probably going through the city of London, they're not going through Frankfurt and Paris.\n",
    "Views and opinions, expressed in the article are those of Michael Swadling and do not necessarily reflect those of Sputnik\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "test_article = get_article_dataloader(article, tokenizer)\n",
    "predict(model, test_article, y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "free_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05c4f9fa65704ea5ba7f80e879e08510": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f54181f857a4110bf9976a56c97de97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fbfe002ccd541f9b6ab62ed4eebef35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1048f064e69b46d497b65cb9b88d0142": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc871ed94e764fe884ffdafca1445bfe",
       "IPY_MODEL_b4e7e77911e5408b84d89ccaff134708",
       "IPY_MODEL_c322e99ad9504207b7cfb9ae2ca9d6b1"
      ],
      "layout": "IPY_MODEL_dd92db5892be4357a6446146d9e9a0dd"
     }
    },
    "145c8616bab245358c8052beac5bd2bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20d6b378a69744e4a327d929c391b475": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "364a4257f8a641e18b7bde39e63a618d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "409a73804fc34a63a619a42b9468be46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "475020323c07410e87d366a6f553aafb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_145c8616bab245358c8052beac5bd2bc",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca901348dcd1428ebecc55d659fa40d1",
      "value": 28
     }
    },
    "4ddbb3fa3b924a9e9650f51204580f8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a5458551f974a94b61ff658ae59fa70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b48b8b79d6a4ff1b0655a6bfcf7a422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c06d626b5ee4fe8bdb9be7fb879aae0",
      "placeholder": "​",
      "style": "IPY_MODEL_779202125255413cb719a43fe5d14ce7",
      "value": "Downloading (…)solve/main/vocab.txt: 100%"
     }
    },
    "5d40c6e034fd4b278969dd928cc07dff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f4b55ead6884f3790a3acfa43232fc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87907508c4eb43f3a27858f1d397ca99",
      "placeholder": "​",
      "style": "IPY_MODEL_d21ed2e527464a0ca0cb2b3b4fad928a",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "650f8e0f82e849f58a47507291b15500": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbb672669b9f4c36873048cfa1d2daf6",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cf42b39e3be94c90b3a63d05783481a3",
      "value": 231508
     }
    },
    "67ea6145091b440da6b4c5d5f8695aa9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ac495a816994aaeb8926c6c97d103ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ddbb3fa3b924a9e9650f51204580f8d",
      "placeholder": "​",
      "style": "IPY_MODEL_a115ed03534a4c64a17fa55d93a0f93a",
      "value": " 570/570 [00:00&lt;00:00, 47.0kB/s]"
     }
    },
    "71add6bf108545af8db6aeb392793759": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5f4b55ead6884f3790a3acfa43232fc9",
       "IPY_MODEL_475020323c07410e87d366a6f553aafb",
       "IPY_MODEL_e2454ec2f5904e0bacde56ae7d4089a9"
      ],
      "layout": "IPY_MODEL_0f54181f857a4110bf9976a56c97de97"
     }
    },
    "779202125255413cb719a43fe5d14ce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87907508c4eb43f3a27858f1d397ca99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c06d626b5ee4fe8bdb9be7fb879aae0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e13dccd922946ab873cc287fcca5baf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93fa55d130db463c8ddffc947f2e99cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a6d16261d5b4693b09acf83dcb38920": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0265a21ae504485ab59207228c1f6d6",
      "placeholder": "​",
      "style": "IPY_MODEL_8e13dccd922946ab873cc287fcca5baf",
      "value": " 232k/232k [00:00&lt;00:00, 9.51MB/s]"
     }
    },
    "a0265a21ae504485ab59207228c1f6d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a115ed03534a4c64a17fa55d93a0f93a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a74bacc051494d1ea0f4cbd656505cfe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4e7e77911e5408b84d89ccaff134708": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67ea6145091b440da6b4c5d5f8695aa9",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bef37b47c8ee49778464c05adcffa30d",
      "value": 466062
     }
    },
    "b83493088cd24629a04ec612aa522ed4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c222d2a5e4664f9b9aefed7f093d4a4e",
       "IPY_MODEL_fa94f90b735f418fb2b502f7877b5306",
       "IPY_MODEL_6ac495a816994aaeb8926c6c97d103ae"
      ],
      "layout": "IPY_MODEL_409a73804fc34a63a619a42b9468be46"
     }
    },
    "bef37b47c8ee49778464c05adcffa30d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c222d2a5e4664f9b9aefed7f093d4a4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_364a4257f8a641e18b7bde39e63a618d",
      "placeholder": "​",
      "style": "IPY_MODEL_93fa55d130db463c8ddffc947f2e99cd",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "c322e99ad9504207b7cfb9ae2ca9d6b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d40c6e034fd4b278969dd928cc07dff",
      "placeholder": "​",
      "style": "IPY_MODEL_f6b3c67d61114db5810bd78339a218d9",
      "value": " 466k/466k [00:00&lt;00:00, 1.88MB/s]"
     }
    },
    "ca901348dcd1428ebecc55d659fa40d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cf42b39e3be94c90b3a63d05783481a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d21ed2e527464a0ca0cb2b3b4fad928a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d568d9b894694b6fb432456031cee230": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbb672669b9f4c36873048cfa1d2daf6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd92db5892be4357a6446146d9e9a0dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2454ec2f5904e0bacde56ae7d4089a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20d6b378a69744e4a327d929c391b475",
      "placeholder": "​",
      "style": "IPY_MODEL_05c4f9fa65704ea5ba7f80e879e08510",
      "value": " 28.0/28.0 [00:00&lt;00:00, 1.97kB/s]"
     }
    },
    "e9ee9acc5e414c65ad10b8864cc07b07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f19c187b1e8c4bbe9fda366f24f2b79e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5b48b8b79d6a4ff1b0655a6bfcf7a422",
       "IPY_MODEL_650f8e0f82e849f58a47507291b15500",
       "IPY_MODEL_9a6d16261d5b4693b09acf83dcb38920"
      ],
      "layout": "IPY_MODEL_d568d9b894694b6fb432456031cee230"
     }
    },
    "f6b3c67d61114db5810bd78339a218d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa94f90b735f418fb2b502f7877b5306": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a74bacc051494d1ea0f4cbd656505cfe",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0fbfe002ccd541f9b6ab62ed4eebef35",
      "value": 570
     }
    },
    "fc871ed94e764fe884ffdafca1445bfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a5458551f974a94b61ff658ae59fa70",
      "placeholder": "​",
      "style": "IPY_MODEL_e9ee9acc5e414c65ad10b8864cc07b07",
      "value": "Downloading (…)/main/tokenizer.json: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
